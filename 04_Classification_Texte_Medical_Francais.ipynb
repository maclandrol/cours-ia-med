{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpDb4Esm2cjb"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/maclandrol/cours-ia-med/blob/master/04_Classification_Texte_Medical_Francais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPJDV3As2cjc"
   },
   "source": [
    "# 04. Classification de Texte Médical Français\n",
    "\n",
    "**Enseignant:** Emmanuel Noutahi, PhD\n",
    "\n",
    "---\n",
    "\n",
    "**Objectif:** Apprendre à classifier des textes médicaux en français en utilisant des approches de prompting et d'apprentissage automatique.\n",
    "\n",
    "**Dataset:** FrenchMedMCQA - Questions à choix multiples de pharmacie\n",
    "\n",
    "**Approches:**\n",
    "- Prompting avec ChatGPT\n",
    "- Classification automatique avec CamemBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnmkLFM52cjc"
   },
   "source": [
    "## Installation et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wb3EXQqZ2cjc",
    "outputId": "a9327752-a194-4644-c8f2-7e9144d54be5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "!pip install \"datasets<4.0.0\" transformers torch pandas scikit-learn matplotlib seaborn -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Dispositif: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JMrZ8mV2cjd"
   },
   "source": [
    "## 1. Accès aux Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFQM69qf2cjd",
    "outputId": "2a552123-381f-4f94-a33d-a803fa960e6b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Chargement du dataset FrenchMedMCQA\ndataset = load_dataset(\"qanastek/frenchmedmcqa\")\nprint(f\"Dataset chargé: {len(dataset['train'])} exemples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qSq37HA2cjd"
   },
   "source": [
    "## 2. Transformation en DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0Js5Mmi2cjd",
    "outputId": "9114f28b-625b-44ca-9f22-9eaf4b8cf102",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# Conversion en DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "print(f\"DataFrame: {df.shape}\")\n",
    "print(f\"Colonnes: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lkv4I6i2cjd"
   },
   "source": "## 3. Analyse des Données"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZT_2Jfp2cjd",
    "outputId": "c1839e86-ca2f-460f-a0da-e19475c4f5bd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Statistiques de base\nprint(f\"Nombre de questions: {len(df)}\")\nprint(f\"Longueur moyenne des questions: {df['question'].str.len().mean():.0f} caractères\")\n\n# Distribution des réponses correctes\nanswer_counts = df['number_correct_answers'].value_counts().sort_index()\nprint(\"\\\\nNombre de bonnes réponses par question:\")\nfor nb, count in answer_counts.items():\n    print(f\"  {nb} réponse(s): {count} questions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epu9tk732cjd",
    "outputId": "c0f218c1-98c2-4335-b776-327aa175b4a0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Exemple de question\nexemple = df.iloc[0]\nprint(\"Exemple de question:\")\nprint(f\"Q: {exemple['question']}\")\nprint(f\"A: {exemple['answer_a']}\")\nprint(f\"B: {exemple['answer_b']}\")\nprint(f\"C: {exemple['answer_c']}\")\nprint(f\"D: {exemple['answer_d']}\")\nprint(f\"E: {exemple['answer_e']}\")\nprint(f\"Réponses correctes: {exemple['correct_answers']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxD_GQ-q2cjd"
   },
   "source": [
    "## 4. Prompting avec ChatGPT\n",
    "\n",
    "### Le Rôle du Prompting\n",
    "\n",
    "Le prompting consiste à formuler des instructions claires pour guider un modèle de langue vers la réponse souhaitée. Pour les questions médicales, un bon prompt doit:\n",
    "- Définir le rôle d'expert médical\n",
    "- Préciser le format de réponse attendu\n",
    "- Fournir le contexte nécessaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtfQOrie2cjd"
   },
   "source": [
    "### Prompt Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jooq8pD92cjd",
    "outputId": "1e30ad7e-61e8-4fb2-e8a3-7f46d07fdf7b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Génération d'un prompt simple\ndef generer_prompt_simple(question, options):\n    options_text = \"\\n\".join([f\"{chr(65+i)}. {option}\" for i, option in enumerate(options)])\n    \n    prompt = f\"\"\"Tu es un expert médical français. Pour cette question d'examen de pharmacie, indique toutes les réponses correctes.\n\nQuestion: {question}\n\nOptions:\n{options_text}\n\nRéponses correctes (lettres uniquement):\"\"\"\n    \n    return prompt\n\n# Test avec notre exemple (toutes les 5 options A, B, C, D, E)\noptions_exemple = [exemple['answer_'+pos] for pos in [\"a\", \"b\", \"c\", \"d\", \"e\"]]\nprompt_simple = generer_prompt_simple(exemple['question'], options_exemple)\nprint(\"PROMPT SIMPLE À COPIER DANS CHATGPT:\")\nprint(\"=\" * 50)\nprint(prompt_simple)\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "016j1NiD2cje"
   },
   "source": [
    "### Prompt Few-Shot (Apprentissage en Contexte)\n",
    "\n",
    "L'apprentissage en contexte (in-context learning) consiste à fournir quelques exemples dans le prompt pour guider le modèle. Cette technique améliore souvent les performances sans nécessiter d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kv5JNZh72cje",
    "outputId": "60b7b13e-1512-4576-b331-b83bcbce3de7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "def generer_prompt_few_shot(question, options):\n    options_text = \"\\n\".join([f\"{chr(65+i)}. {option}\" for i, option in enumerate(options)])\n    \n    prompt = f\"\"\"Tu es un expert médical français. Voici des exemples de questions d'examen de pharmacie avec leurs réponses correctes:\n\nExemple 1:\nQuestion: Quels sont les effets indésirables de l'aspirine?\nOptions:\nA. Troubles digestifs\nB. Amélioration de l'humeur\nC. Risque hémorragique\nD. Augmentation de l'appétit\nE. Réactions allergiques\nRéponses correctes: A, C, E\n\nExemple 2:\nQuestion: Le paracétamol est contre-indiqué en cas de:\nOptions:\nA. Insuffisance hépatique sévère\nB. Hypertension artérielle\nC. Diabète\nD. Grossesse\nE. Insuffisance rénale chronique\nRéponses correctes: A\n\nMaintenant, réponds à cette nouvelle question:\n\nQuestion: {question}\n\nOptions:\n{options_text}\n\nRéponses correctes (lettres uniquement):\"\"\"\n    \n    return prompt\n\n# Test few-shot avec toutes les 5 options\nprompt_few_shot = generer_prompt_few_shot(exemple['question'], options_exemple)\nprint(\"PROMPT FEW-SHOT À COPIER DANS CHATGPT:\")\nprint(\"=\" * 50)\nprint(prompt_few_shot)\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3x9oyY72cje"
   },
   "source": [
    "## 5. Classification Automatique\n",
    "\n",
    "### Préparation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qp3nwKQL2cje",
    "outputId": "864abb89-2fa1-4e69-8005-38df3ac8c15f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Préparation des données pour classification multi-label\n# Prédire pour chaque question quelles options sont correctes\n\ndef preparer_donnees_classification(df, max_samples=500):\n    df_sample = df.head(max_samples).copy()\n\n    texts = []\n    labels = []\n\n    for _, row in df_sample.iterrows():\n        question = row['question']\n        options = [row['answer_a'], row['answer_b'], row['answer_c'], row['answer_d'], row['answer_e']]\n        correct_answers = row['correct_answers']\n\n        # Créer le texte avec toutes les options\n        options_text = \" [SEP] \".join([f\"{chr(65+i)}: {option}\" for i, option in enumerate(options)])\n        text = f\"{question} [SEP] {options_text}\"\n        texts.append(text)\n\n        # Label multi-label: [0,1,0,1,1] pour options B, D, E correctes par exemple\n        label = [1 if i in correct_answers else 0 for i in range(5)]\n        labels.append(label)\n\n    return texts, labels\n\ntexts, labels = preparer_donnees_classification(df, max_samples=500)\nprint(f\"Données préparées: {len(texts)} exemples\")\nprint(f\"Distribution des labels:\")\nlabels_array = np.array(labels)\nfor i, option in enumerate(['A', 'B', 'C', 'D', 'E']):\n    count = labels_array[:, i].sum()\n    print(f\"  Option {option}: {count} ({count/len(labels)*100:.1f}%)\")\n\nprint(f\"Moyenne d'options correctes par question: {labels_array.sum(axis=1).mean():.2f}\")"
  },
  {
   "cell_type": "code",
   "source": "# Division des données en train/validation/test\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=== DIVISION DES DONNÉES ===\")\n\n# Définition des noms d'options pour visualisation\noption_names = ['A', 'B', 'C', 'D', 'E']\n\n# Stratification basée sur le nombre total de bonnes réponses par question\nstratify_labels = [sum(label) for label in labels]\n\n# Division initiale train/temp (80/20)\ntexts_train, texts_temp, labels_train, labels_temp = train_test_split(\n    texts, labels, test_size=0.2, random_state=42, stratify=stratify_labels\n)\n\n# Stratification pour la division temp\nstratify_temp = [sum(label) for label in labels_temp]\n\n# Division temp en validation/test (10/10 du total)\ntexts_val, texts_test, labels_val, labels_test = train_test_split(\n    texts_temp, labels_temp, test_size=0.5, random_state=42, stratify=stratify_temp\n)\n\nprint(f\"Données d'entraînement: {len(texts_train)} exemples\")\nprint(f\"Données de validation: {len(texts_val)} exemples\")\nprint(f\"Données de test: {len(texts_test)} exemples\")\n\n# Vérification de la distribution stratifiée\ntrain_avg_labels = np.array(labels_train).sum(axis=1).mean()\nval_avg_labels = np.array(labels_val).sum(axis=1).mean()\ntest_avg_labels = np.array(labels_test).sum(axis=1).mean()\n\nprint(f\"\\nVérification stratification:\")\nprint(f\"Train - moyenne labels/question: {train_avg_labels:.2f}\")\nprint(f\"Validation - moyenne labels/question: {val_avg_labels:.2f}\")\nprint(f\"Test - moyenne labels/question: {test_avg_labels:.2f}\")\n\n# Visualisation de la distribution\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nsplits = [\n    ('Train', np.array(labels_train)),\n    ('Validation', np.array(labels_val)),\n    ('Test', np.array(labels_test))\n]\n\nfor idx, (split_name, split_labels) in enumerate(splits):\n    option_counts = split_labels.sum(axis=0)\n    axes[idx].bar(option_names, option_counts, alpha=0.7, color=f'C{idx}', edgecolor='black')\n    axes[idx].set_title(f'Distribution - {split_name}\\n({len(split_labels)} exemples)')\n    axes[idx].set_xlabel('Options de Réponse')\n    axes[idx].set_ylabel('Nombre d\\'Occurrences')\n    axes[idx].grid(True, alpha=0.3)\n\n    # Ajout des valeurs sur les barres\n    for i, count in enumerate(option_counts):\n        axes[idx].text(i, count + 0.5, str(count), ha='center', va='bottom', fontweight='bold')\n\nplt.suptitle('Distribution des Labels par Split', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "id": "2TEQ-kd44KRk",
    "outputId": "e420b247-ea9d-40a0-90e9-1b9413480698",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrFJo1Br2cje"
   },
   "source": [
    "### Entraînement du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTTIX48W2cje",
    "outputId": "8d56c4b9-7925-4788-d765-fcc2e1060260",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567,
     "referenced_widgets": [
      "4de2f85ec0cd452caaf2f30a5377b684",
      "e7c2b7d4ce544a9387d228507e254413",
      "17795141faf14f99a1c9977120d8e8af",
      "45df180b660b4f7f899ee91bdbee21cd",
      "61cec92c231e4b18a0d91dc04a7f298b",
      "d9df3f173abf4c81aaf3a42679b2d059",
      "afc403e665f04cbca7eb5041dcdf50fd",
      "ca47af3595914495bff7fe3f57c655bf",
      "dbb1f6845d4b4c48853854815efcb2ee",
      "66231ac4ceb448a7b9c4e6d1c4484e4b",
      "cc5b89f35e714616af488961413cdd5b"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# Configuration et chargement du modèle CamemBERT\n",
    "print(\"=== CONFIGURATION CAMEMBERT POUR MÉDICAL ===\")\n",
    "\n",
    "# Choix du modèle pour le français médical\n",
    "model_name = \"camembert-base\"  # Alternative: \"flaubert/flaubert_base_cased\"\n",
    "\n",
    "# Chargement du tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Tokenizer chargé: {model_name}\")\n",
    "print(f\"Taille du vocabulaire: {len(tokenizer)}\")\n",
    "\n",
    "# Configuration pour classification multi-label\n",
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=5,  # 5 options (A, B, C, D, E)\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label={0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4: \"E\"},\n",
    "    label2id={\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4}\n",
    ")\n",
    "\n",
    "# Chargement du modèle\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"Modèle chargé avec {model.num_parameters()} paramètres\")\n",
    "print(f\"Architecture: {type(model).__name__}\")\n",
    "\n",
    "# Test de tokenisation sur exemple médical\n",
    "medical_example = texts[0]\n",
    "tokens = tokenizer.tokenize(medical_example)\n",
    "input_ids = tokenizer.encode(medical_example, truncation=True, max_length=512)\n",
    "\n",
    "print(f\"\\nTest de tokenisation:\")\n",
    "print(f\"Texte original: {len(medical_example)} caractères\")\n",
    "print(f\"Tokens générés: {len(tokens)}\")\n",
    "print(f\"Input IDs: {len(input_ids)}\")\n",
    "print(f\"Premiers tokens: {tokens[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2w4SDDXg2cje",
    "outputId": "01f9625d-df10-473a-97e9-300848ee7ee9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Création d'un dataset PyTorch pour l'entraînement\nimport torch\nfrom torch.utils.data import Dataset\n\nclass FrenchMedicalDataset(Dataset):\n    \"\"\"\n    Dataset personnalisé pour questions médicales françaises multi-label\n    \"\"\"\n\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        # Tokenisation\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.FloatTensor(label)  # Labels déjà sous forme de liste [0,1,0,1,1]\n        }\n\n# Création des datasets\ntrain_dataset = FrenchMedicalDataset(texts_train, labels_train, tokenizer)\nval_dataset = FrenchMedicalDataset(texts_val, labels_val, tokenizer)\ntest_dataset = FrenchMedicalDataset(texts_test, labels_test, tokenizer)\n\nprint(f\"Datasets créés:\")\nprint(f\"  Train: {len(train_dataset)} exemples\")\nprint(f\"  Validation: {len(val_dataset)} exemples\")\nprint(f\"  Test: {len(test_dataset)} exemples\")\n\n# Test du dataset\nsample = train_dataset[0]\nprint(f\"\\nÉchantillon du dataset:\")\nprint(f\"  Input IDs shape: {sample['input_ids'].shape}\")\nprint(f\"  Attention mask shape: {sample['attention_mask'].shape}\")\nprint(f\"  Labels shape: {sample['labels'].shape}\")\nprint(f\"  Labels: {sample['labels'].tolist()}\")\n\n# Analyse de la longueur des séquences tokenisées\nsequence_lengths = []\nfor text in texts_train[:100]:  # Échantillon pour analyse\n    tokens = tokenizer.encode(text, truncation=False)\n    sequence_lengths.append(len(tokens))\n\nprint(f\"\\nAnalyse des longueurs de séquences:\")\nprint(f\"  Longueur moyenne: {np.mean(sequence_lengths):.1f} tokens\")\nprint(f\"  Longueur médiane: {np.median(sequence_lengths):.1f} tokens\")\nprint(f\"  Longueur maximale: {max(sequence_lengths)} tokens\")\nprint(f\"  % séquences > 512 tokens: {sum(1 for l in sequence_lengths if l > 512)/len(sequence_lengths)*100:.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyD19Q4k2cje",
    "outputId": "90919e94-2a2d-4942-bc4b-a304a8467f51",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration de l'entraînement optimisée pour Colab\n",
    "print(\"=== CONFIGURATION D'ENTRAÎNEMENT ===\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./medical_french_classifier',\n",
    "    num_train_epochs=3,  # Réduction pour Colab\n",
    "    per_device_train_batch_size=8,  # Optimisé pour mémoire GPU\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=0,  # Évite les problèmes de multiprocessing\n",
    "    fp16=torch.cuda.is_available(),  # Optimisation mémoire GPU\n",
    "    gradient_checkpointing=True,  # Économie mémoire\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcule les métriques d'évaluation pour classification multi-label\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Conversion des probabilités en prédictions binaires\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = (probs > 0.5).int().numpy()\n",
    "    y_true = labels.astype(int)\n",
    "\n",
    "    # Métriques globales\n",
    "    exact_match = accuracy_score(y_true, y_pred)\n",
    "    # Métriques par classe (micro et macro)\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='micro', zero_division=0\n",
    "    )\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'exact_match': exact_match,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_micro': precision_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialisation du trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"Configuration d'entraînement:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size (train): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Optimisations mémoire: FP16={training_args.fp16}, Gradient checkpointing={training_args.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4nNWP-y2cje"
   },
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "print(\"Début de l'entraînement...\")\n",
    "trainer.train()\n",
    "print(\"Entraînement terminé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAV2RChV2cje"
   },
   "source": [
    "## 6. Évaluation et Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnDAc6Uj2cje",
    "outputId": "55c9f136-fab1-4609-b769-c1d01867931c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    }
   },
   "outputs": [],
   "source": [
    "# Évaluation sur le jeu de test\n",
    "results = trainer.evaluate()\n",
    "print(\"Résultats d'évaluation:\")\n",
    "for metric, value in results.items():\n",
    "    if metric.startswith('eval_'):\n",
    "        metric_name = metric.replace('eval_', '')\n",
    "        print(f\"{metric_name}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRUQLJoG2cje",
    "outputId": "e7150156-b836-4b35-f316-adb0fcdb5887",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    }
   },
   "outputs": [],
   "source": "# Test sur un nouvel exemple\ndef predire_reponse(question_text, model, tokenizer):\n    \"\"\"\n    Prédit quelles options sont correctes pour une question donnée\n    \"\"\"\n    inputs = tokenizer(question_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        probabilities = torch.sigmoid(outputs.logits)  # Sigmoid pour multi-label\n        predictions = (probabilities > 0.5).int()  # Seuil à 0.5\n    \n    return predictions[0].tolist(), probabilities[0].tolist()\n\n# Test sur l'exemple avec toutes les 5 options\ntest_question = exemple['question']\ntest_options = [exemple['answer_a'], exemple['answer_b'], exemple['answer_c'], exemple['answer_d'], exemple['answer_e']]\ncorrect_answers = exemple['correct_answers']\n\n# Créer le texte formaté comme dans l'entraînement\noptions_text = \" [SEP] \".join([f\"{chr(65+i)}: {option}\" for i, option in enumerate(test_options)])\nformatted_text = f\"{test_question} [SEP] {options_text}\"\n\nprint(f\"Question: {test_question[:100]}...\")\nprint(f\"Réponses attendues: {[chr(65+i) for i in correct_answers]}\")\n\npredictions, confidences = predire_reponse(formatted_text, model, tokenizer)\n\nprint(\"\\\\nPrédictions du modèle:\")\nfor i in range(5):\n    option_letter = chr(65+i)\n    predicted = \"CORRECT\" if predictions[i] == 1 else \"INCORRECT\"\n    expected = \"✓\" if i in correct_answers else \"✗\"\n    confidence = confidences[i]\n    print(f\"{option_letter}. {predicted} (conf: {confidence:.3f}) [{expected}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IZwo-rK2cje",
    "outputId": "d84b8e9a-f19d-49b6-c632-f5b6b80a9817",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "outputs": [],
   "source": "# Évaluation détaillée sur le jeu de test\nfrom sklearn.metrics import multilabel_confusion_matrix, classification_report\n\n# Prédictions sur le jeu de test\ntest_predictions = []\ntest_probabilities = []\ntest_true_labels = []\n\nmodel.eval()\nfor i in range(len(test_dataset)):\n    sample = test_dataset[i]\n    inputs = {\n        'input_ids': sample['input_ids'].unsqueeze(0),\n        'attention_mask': sample['attention_mask'].unsqueeze(0)\n    }\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        probs = torch.sigmoid(outputs.logits)\n        preds = (probs > 0.5).int()\n    \n    test_predictions.append(preds[0].tolist())\n    test_probabilities.append(probs[0].tolist())\n    test_true_labels.append(sample['labels'].tolist())\n\ntest_pred_binary = np.array(test_predictions)\ntest_probs = np.array(test_probabilities)\ntest_labels = np.array(test_true_labels)\n\n# Calcul des métriques\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nexact_match = accuracy_score(test_labels, test_pred_binary)\nprecision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n    test_labels, test_pred_binary, average='micro', zero_division=0\n)\nprecision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n    test_labels, test_pred_binary, average='macro', zero_division=0\n)\n\n# Métriques par classe\nprecision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n    test_labels, test_pred_binary, average=None, zero_division=0\n)\n\nprint(\"=== RÉSULTATS D'ÉVALUATION ===\")\nprint(f\"Exact Match (toutes options correctes): {exact_match:.3f}\")\nprint(f\"F1-Score Micro: {f1_micro:.3f}\")\nprint(f\"F1-Score Macro: {f1_macro:.3f}\")\nprint(f\"Précision Micro: {precision_micro:.3f}\")\nprint(f\"Rappel Micro: {recall_micro:.3f}\")\n\nprint(f\"\\nMétriques par option:\")\nfor i, option in enumerate(option_names):\n    print(f\"Option {option} - Précision: {precision_per_class[i]:.3f}, Rappel: {recall_per_class[i]:.3f}, F1: {f1_per_class[i]:.3f}\")\n\n# Visualisation des performances\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\nfig.suptitle('Analyse des Performances - Classification Multi-label', fontsize=16, fontweight='bold')\n\n# 1. Métriques par classe\nx = np.arange(len(option_names))\nwidth = 0.25\n\naxes[0, 0].bar(x - width, precision_per_class, width, label='Precision', alpha=0.7)\naxes[0, 0].bar(x, recall_per_class, width, label='Recall', alpha=0.7)\naxes[0, 0].bar(x + width, f1_per_class, width, label='F1-Score', alpha=0.7)\n\naxes[0, 0].set_xlabel('Options de Réponse')\naxes[0, 0].set_ylabel('Score')\naxes[0, 0].set_title('Métriques par Option')\naxes[0, 0].set_xticks(x)\naxes[0, 0].set_xticklabels(option_names)\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Matrice de confusion pour l'option A (exemple)\nconf_matrices = multilabel_confusion_matrix(test_labels, test_pred_binary)\ncm_normalized = conf_matrices[0] / conf_matrices[0].sum(axis=1, keepdims=True)\n\nsns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',\n            xticklabels=['Prédit: Non', 'Prédit: Oui'],\n            yticklabels=['Réel: Non', 'Réel: Oui'],\n            ax=axes[0, 1])\naxes[0, 1].set_title('Matrice de Confusion - Option A')\n\n# 3. Distribution des scores de confiance\naxes[1, 0].hist(test_probs.flatten(), bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\naxes[1, 0].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Seuil de décision')\naxes[1, 0].set_xlabel('Probabilité')\naxes[1, 0].set_ylabel('Fréquence')\naxes[1, 0].set_title('Distribution des Probabilités Prédites')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. Métriques globales\nglobal_metrics = {\n    'Exact Match': exact_match,\n    'F1 Micro': f1_micro,\n    'F1 Macro': f1_macro,\n}\n\nmetric_names = list(global_metrics.keys())\nmetric_values = list(global_metrics.values())\n\nbars = axes[1, 1].bar(metric_names, metric_values, alpha=0.7, color='salmon', edgecolor='black')\naxes[1, 1].set_ylabel('Score')\naxes[1, 1].set_title('Métriques Globales')\naxes[1, 1].set_ylim(0, 1)\naxes[1, 1].grid(True, alpha=0.3)\nplt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n\n# Ajout des valeurs sur les barres\nfor bar, value in zip(bars, metric_values):\n    height = bar.get_height()\n    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80sZCp6z2cje"
   },
   "source": [
    "## Résumé\n",
    "\n",
    "### Approches Testées\n",
    "\n",
    "1. **Prompting Simple**: Instructions directes pour ChatGPT\n",
    "2. **Few-Shot Learning**: Utilisation d'exemples dans le prompt pour guider le modèle\n",
    "3. **Classification Automatique**: Entraînement d'un modèle CamemBERT spécialisé\n",
    "\n",
    "### Applications Pratiques\n",
    "\n",
    "- **Prompting**: Idéal pour des cas d'usage ponctuels ou des domaines très spécialisés\n",
    "- **Classification**: Recommandée pour des volumes importants et des besoins de rapidité\n",
    "- **Hybride**: Combiner prompting pour la validation et classification pour le traitement en masse\n",
    "\n",
    "Le prochain notebook explorera l'analyse d'images médicales avec TorchXRayVision."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}