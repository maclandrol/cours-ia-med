{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/maclandrol/cours-ia-med/blob/master/04_Classification_Texte_Medical_Francais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Classification de Texte Médical Français\n",
    "\n",
    "**Enseignant:** Emmanuel Noutahi, PhD\n",
    "\n",
    "---\n",
    "\n",
    "**Objectif:** Apprendre à classifier des textes médicaux en français en utilisant des approches de prompting et d'apprentissage automatique.\n",
    "\n",
    "**Dataset:** FrenchMedMCQA - Questions à choix multiples de pharmacie\n",
    "\n",
    "**Approches:**\n",
    "- Prompting avec ChatGPT\n",
    "- Classification automatique avec CamemBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"datasets<4.0.0\" transformers torch pandas scikit-learn matplotlib seaborn -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Dispositif: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accès aux Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset FrenchMedMCQA\n",
    "dataset = load_dataset(\"qanastek/FrenchMedMCQA\", trust_remote_code=True)\n",
    "print(f\"Dataset chargé: {len(dataset['train'])} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformation en DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "print(f\"DataFrame: {df.shape}\")\n",
    "print(f\"Colonnes: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de base\n",
    "print(f\"Nombre de questions: {len(df)}\")\n",
    "print(f\"Longueur moyenne des questions: {df['question'].str.len().mean():.0f} caractères\")\n",
    "\n",
    "# Distribution des réponses correctes\n",
    "answer_counts = df['correct_answers'].apply(len).value_counts().sort_index()\n",
    "print(\"\\nNombre de bonnes réponses par question:\")\n",
    "for nb, count in answer_counts.items():\n",
    "    print(f\"  {nb} réponse(s): {count} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de question\n",
    "exemple = df.iloc[0]\n",
    "print(\"Exemple de question:\")\n",
    "print(f\"Q: {exemple['question']}\")\n",
    "print(f\"Options: {exemple['options']}\")\n",
    "print(f\"Réponses correctes: {exemple['correct_answers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompting avec ChatGPT\n",
    "\n",
    "### Le Rôle du Prompting\n",
    "\n",
    "Le prompting consiste à formuler des instructions claires pour guider un modèle de langue vers la réponse souhaitée. Pour les questions médicales, un bon prompt doit:\n",
    "- Définir le rôle d'expert médical\n",
    "- Préciser le format de réponse attendu\n",
    "- Fournir le contexte nécessaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération d'un prompt simple\n",
    "def generer_prompt_simple(question, options):\n",
    "    options_text = \"\\n\".join([f\"{chr(65+i)}. {option}\" for i, option in enumerate(options)])\n",
    "    \n",
    "    prompt = f\"\"\"Tu es un expert médical français. Pour cette question d'examen de pharmacie, indique toutes les réponses correctes.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "{options_text}\n",
    "\n",
    "Réponses correctes (lettres uniquement):\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test avec notre exemple\n",
    "prompt_simple = generer_prompt_simple(exemple['question'], exemple['options'])\n",
    "print(\"PROMPT SIMPLE À COPIER DANS CHATGPT:\")\n",
    "print(\"=\" * 50)\n",
    "print(prompt_simple)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Few-Shot (Apprentissage en Contexte)\n",
    "\n",
    "L'apprentissage en contexte (in-context learning) consiste à fournir quelques exemples dans le prompt pour guider le modèle. Cette technique améliore souvent les performances sans nécessiter d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_prompt_few_shot(question, options):\n",
    "    options_text = \"\\n\".join([f\"{chr(65+i)}. {option}\" for i, option in enumerate(options)])\n",
    "    \n",
    "    prompt = f\"\"\"Tu es un expert médical français. Voici des exemples de questions d'examen de pharmacie avec leurs réponses correctes:\n",
    "\n",
    "Exemple 1:\n",
    "Question: Quels sont les effets indésirables de l'aspirine?\n",
    "Options:\n",
    "A. Troubles digestifs\n",
    "B. Amélioration de l'humeur\n",
    "C. Risque hémorragique\n",
    "D. Augmentation de l'appétit\n",
    "Réponses correctes: A, C\n",
    "\n",
    "Exemple 2:\n",
    "Question: Le paracétamol est contre-indiqué en cas de:\n",
    "Options:\n",
    "A. Insuffisance hépatique sévère\n",
    "B. Hypertension artérielle\n",
    "C. Diabète\n",
    "D. Grossesse\n",
    "Réponses correctes: A\n",
    "\n",
    "Maintenant, réponds à cette nouvelle question:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "{options_text}\n",
    "\n",
    "Réponses correctes (lettres uniquement):\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test few-shot\n",
    "prompt_few_shot = generer_prompt_few_shot(exemple['question'], exemple['options'])\n",
    "print(\"PROMPT FEW-SHOT À COPIER DANS CHATGPT:\")\n",
    "print(\"=\" * 50)\n",
    "print(prompt_few_shot)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification Automatique\n",
    "\n",
    "### Préparation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation pour la classification binaire (simplifiée)\n",
    "# Prédire si chaque option est correcte ou non\n",
    "\n",
    "def preparer_donnees_classification(df, max_samples=1000):\n",
    "    df_sample = df.head(max_samples).copy()\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df_sample.iterrows():\n",
    "        question = row['question']\n",
    "        options = row['options']\n",
    "        correct_answers = row['correct_answers']\n",
    "        \n",
    "        # Pour chaque option, créer un exemple\n",
    "        for i, option in enumerate(options):\n",
    "            text = f\"{question} [SEP] {option}\"\n",
    "            texts.append(text)\n",
    "            \n",
    "            # Label: 1 si cette option est correcte, 0 sinon\n",
    "            label = 1 if chr(65+i) in correct_answers else 0\n",
    "            labels.append(label)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "texts, labels = preparer_donnees_classification(df, max_samples=500)\n",
    "print(f\"Données préparées: {len(texts)} exemples\")\n",
    "print(f\"Labels positifs: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division train/test\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(texts_train)} exemples\")\n",
    "print(f\"Test: {len(texts_test)} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle CamemBERT\n",
    "model_name = \"camembert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "print(f\"Modèle chargé: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "train_encodings = tokenize_function(texts_train)\n",
    "test_encodings = tokenize_function(texts_test)\n",
    "\n",
    "print(\"Tokenisation terminée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset PyTorch\n",
    "class MedicalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MedicalDataset(train_encodings, labels_train)\n",
    "test_dataset = MedicalDataset(test_encodings, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Fonction de métriques\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "print(\"Début de l'entraînement...\")\n",
    "trainer.train()\n",
    "print(\"Entraînement terminé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation et Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation sur le jeu de test\n",
    "results = trainer.evaluate()\n",
    "print(\"Résultats d'évaluation:\")\n",
    "for metric, value in results.items():\n",
    "    if metric.startswith('eval_'):\n",
    "        metric_name = metric.replace('eval_', '')\n",
    "        print(f\"{metric_name}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions détaillées\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = labels_test\n",
    "\n",
    "print(\"Rapport de classification:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Incorrect', 'Correct']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur un nouvel exemple\n",
    "def predire_reponse(question, option, model, tokenizer):\n",
    "    text = f\"{question} [SEP] {option}\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        prediction = torch.argmax(probabilities, dim=-1)\n",
    "    \n",
    "    return prediction.item(), probabilities[0][1].item()\n",
    "\n",
    "# Test sur l'exemple\n",
    "test_question = exemple['question']\n",
    "test_options = exemple['options']\n",
    "correct_answers = exemple['correct_answers']\n",
    "\n",
    "print(f\"Question: {test_question[:100]}...\")\n",
    "print(f\"Réponses attendues: {correct_answers}\")\n",
    "print(\"\\nPrédictions du modèle:\")\n",
    "\n",
    "for i, option in enumerate(test_options):\n",
    "    pred, confidence = predire_reponse(test_question, option, model, tokenizer)\n",
    "    status = \"CORRECT\" if pred == 1 else \"INCORRECT\"\n",
    "    expected = \"✓\" if chr(65+i) in correct_answers else \"✗\"\n",
    "    print(f\"{chr(65+i)}. {status} (conf: {confidence:.3f}) [{expected}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé\n",
    "\n",
    "### Approches Testées\n",
    "\n",
    "1. **Prompting Simple**: Instructions directes pour ChatGPT\n",
    "2. **Few-Shot Learning**: Utilisation d'exemples dans le prompt pour guider le modèle\n",
    "3. **Classification Automatique**: Entraînement d'un modèle CamemBERT spécialisé\n",
    "\n",
    "### Applications Pratiques\n",
    "\n",
    "- **Prompting**: Idéal pour des cas d'usage ponctuels ou des domaines très spécialisés\n",
    "- **Classification**: Recommandée pour des volumes importants et des besoins de rapidité\n",
    "- **Hybride**: Combiner prompting pour la validation et classification pour le traitement en masse\n",
    "\n",
    "Le prochain notebook explorera l'analyse d'images médicales avec TorchXRayVision."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
