{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/maclandrol/aa48a4bdba09ddae1f5158d715bc5671/10_TorchXRayVision_Dataset_Personnalise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "**Enseignant:** Emmanuel Noutahi, PhD",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_dataset_header"
      },
      "source": [
        "# Tutorial 6 : Int√©gration de Datasets Personnalis√©s\n",
        "# Tutorial 6: Custom Dataset Integration\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Contexte M√©dical / Medical Context\n",
        "\n",
        "### Pour les √âtudiants en M√©decine / For Medical Students\n",
        "L'**int√©gration de donn√©es personnalis√©es** est essentielle pour :\n",
        "- **Recherche m√©dicale** : Analyser vos propres donn√©es cliniques\n",
        "- **√âtudes sp√©cialis√©es** : Focus sur pathologies sp√©cifiques\n",
        "- **Validation locale** : Adapter l'IA √† votre population\n",
        "- **Projets √©tudiants** : Mener des recherches originales\n",
        "\n",
        "### Pour les Praticiens / For Practitioners\n",
        "- **Chirurgiens** : Validation sur cas chirurgicaux sp√©cifiques\n",
        "- **M√©decins G√©n√©ralistes** : Adaptation aux sp√©cificit√©s locales\n",
        "- **Enseignants** : Cr√©ation de cas p√©dagogiques personnalis√©s\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Objectifs d'Apprentissage / Learning Objectives\n",
        "\n",
        "√Ä la fin de ce tutoriel, vous serez capable de :\n",
        "1. **Charger facilement** vos propres images radiologiques\n",
        "2. **Organiser** un dataset personnalis√© pour l'analyse\n",
        "3. **Appliquer** les mod√®les TorchXRayVision sur vos donn√©es\n",
        "4. **Analyser en lot** plusieurs images simultan√©ment\n",
        "5. **G√©n√©rer** des rapports comparatifs pour votre dataset\n",
        "6. **Exporter** les r√©sultats pour usage clinique ou recherche\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Fonctionnalit√©s Colab Faciles / Easy Colab Features\n",
        "\n",
        "Ce tutoriel est con√ßu pour **Google Colab** avec :\n",
        "- **Glisser-d√©poser** d'images multiples\n",
        "- **Organisation automatique** des donn√©es\n",
        "- **Traitement par lot** simplifi√©\n",
        "- **Visualisations interactives**\n",
        "- **Export automatique** des r√©sultats\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Pr√©requis / Prerequisites\n",
        "\n",
        "Ce tutoriel fait suite aux **Tutoriels 1-4**. Vous devriez ma√Ætriser :\n",
        "- Utilisation de base de TorchXRayVision\n",
        "- Classification et d√©tection de pathologies\n",
        "- Interpr√©tation des r√©sultats\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Installation et Configuration / Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Installation des biblioth√®ques n√©cessaires / Install required libraries\n",
        "!pip install torchxrayvision\n",
        "!pip install torch torchvision\n",
        "!pip install matplotlib seaborn\n",
        "!pip install numpy pandas\n",
        "!pip install scikit-image opencv-python\n",
        "!pip install tqdm  # Barre de progression\n",
        "!pip install zipfile36  # Pour g√©rer les archives\n",
        "\n",
        "print(\"‚úÖ Installation termin√©e / Installation completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libraries"
      },
      "outputs": [],
      "source": [
        "# Import des biblioth√®ques / Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchxrayvision as xrv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "import io\n",
        "import json\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration de l'affichage / Display configuration\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(\"üìö Biblioth√®ques import√©es avec succ√®s / Libraries imported successfully\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üè• TorchXRayVision version: {xrv.__version__}\")\n",
        "\n",
        "# V√©rification du GPU / GPU check\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üíª Device utilis√© / Device used: {device}\")\n",
        "\n",
        "# Cr√©er dossier de travail pour les donn√©es\n",
        "os.makedirs('custom_dataset', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "print(\"üìÅ Dossiers de travail cr√©√©s / Working directories created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_loading"
      },
      "source": [
        "## ü§ñ Chargement des Mod√®les / Loading Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_models"
      },
      "outputs": [],
      "source": [
        "# Chargement des mod√®les TorchXRayVision / Load TorchXRayVision models\n",
        "print(\"üîÑ Chargement des mod√®les pour analyse de dataset personnalis√©...\")\n",
        "print(\"üîÑ Loading models for custom dataset analysis...\")\n",
        "\n",
        "models = {}\n",
        "model_info = {}\n",
        "\n",
        "# 1. Mod√®le principal pour classification\n",
        "try:\n",
        "    models['densenet'] = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n",
        "    models['densenet'].to(device)\n",
        "    models['densenet'].eval()\n",
        "    model_info['densenet'] = {\n",
        "        'name': 'DenseNet121-All',\n",
        "        'pathologies': models['densenet'].pathologies,\n",
        "        'description': 'Mod√®le g√©n√©ral pour toutes pathologies'\n",
        "    }\n",
        "    print(\"‚úÖ DenseNet121-All charg√© / loaded\")\nexcept Exception as e:\n",
        "    print(f\"‚ùå Erreur DenseNet: {e}\")\n",
        "\n",
        "# 2. Mod√®le CheXpert\n",
        "try:\n",
        "    models['chexpert'] = xrv.models.DenseNet(weights=\"densenet121-res224-chexpert\")\n",
        "    models['chexpert'].to(device)\n",
        "    models['chexpert'].eval()\n",
        "    model_info['chexpert'] = {\n",
        "        'name': 'CheXpert',\n",
        "        'pathologies': models['chexpert'].pathologies,\n",
        "        'description': 'Sp√©cialis√© pour donn√©es CheXpert'\n",
        "    }\n",
        "    print(\"‚úÖ CheXpert charg√© / loaded\")\nexcept Exception as e:\n",
        "    print(f\"‚ùå Erreur CheXpert: {e}\")\n",
        "\n",
        "# 3. Mod√®le NIH\n",
        "try:\n",
        "    models['nih'] = xrv.models.DenseNet(weights=\"densenet121-res224-nih\")\n",
        "    models['nih'].to(device)\n",
        "    models['nih'].eval()\n",
        "    model_info['nih'] = {\n",
        "        'name': 'NIH',\n",
        "        'pathologies': models['nih'].pathologies,\n",
        "        'description': 'Entra√Æn√© sur dataset NIH'\n",
        "    }\n",
        "    print(\"‚úÖ NIH charg√© / loaded\")\nexcept Exception as e:\n",
        "    print(f\"‚ùå Erreur NIH: {e}\")\n",
        "\n",
        "if not models:\n",
        "    raise Exception(\"Aucun mod√®le n'a pu √™tre charg√©\")\n",
        "\n",
        "# S√©lectionner le mod√®le principal\n",
        "main_model_key = list(models.keys())[0]\n",
        "main_model = models[main_model_key]\n",
        "\n",
        "print(f\"\\nüéØ Mod√®les disponibles: {list(models.keys())}\")\n",
        "print(f\"üè• Mod√®le principal: {model_info[main_model_key]['name']}\")\n",
        "print(f\"üìä Pathologies d√©tectables: {len(main_model.pathologies)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_upload"
      },
      "source": [
        "## üì§ Chargement de votre Dataset / Upload Your Dataset\n",
        "\n",
        "### üéØ M√©thodes de Chargement Faciles / Easy Upload Methods\n",
        "\n",
        "Choisissez votre m√©thode pr√©f√©r√©e :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "method1_individual"
      },
      "source": [
        "### M√©thode 1 : Images Individuelles / Individual Images\n",
        "Id√©al pour 1-10 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_individual_images"
      },
      "outputs": [],
      "source": [
        "# Chargement d'images individuelles / Upload individual images\n",
        "print(\"üì§ M√âTHODE 1: CHARGEMENT D'IMAGES INDIVIDUELLES\")\n",
        "print(\"üì§ METHOD 1: INDIVIDUAL IMAGE UPLOAD\")\n",
        "print(\"-\" * 60)\n",
        "print(\"üí° Conseil: Cette m√©thode est id√©ale pour 1-10 images\")\n",
        "print(\"üí° Tip: This method is ideal for 1-10 images\")\n",
        "print(\"\")\n",
        "print(\"üî∏ Formats support√©s / Supported formats: .jpg, .jpeg, .png, .tiff\")\n",
        "print(\"üî∏ Vous pouvez s√©lectionner plusieurs images en m√™me temps\")\n",
        "print(\"üî∏ You can select multiple images at once\")\n",
        "print(\"\")\n",
        "print(\"üëÜ Cliquez 'Choisir des fichiers' ci-dessous:\")\n",
        "\n",
        "# Interface de chargement\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "individual_images = []\n",
        "individual_filenames = []\n",
        "\n",
        "if uploaded_files:\n",
        "    print(f\"\\nüìÅ {len(uploaded_files)} fichier(s) charg√©(s) / file(s) uploaded\")\n",
        "    \n",
        "    for filename, file_content in uploaded_files.items():\n",
        "        try:\n",
        "            # V√©rifier l'extension\n",
        "            if not filename.lower().endswith(('.jpg', '.jpeg', '.png', '.tiff', '.tif')):\n",
        "                print(f\"‚ö†Ô∏è {filename}: Format non support√©\")\n",
        "                continue\n",
        "            \n",
        "            # Charger l'image\n",
        "            image = Image.open(io.BytesIO(file_content))\n",
        "            \n",
        "            # Convertir en niveaux de gris si n√©cessaire\n",
        "            if image.mode != 'L':\n",
        "                image = image.convert('L')\n",
        "            \n",
        "            # Convertir en array numpy\n",
        "            img_array = np.array(image)\n",
        "            \n",
        "            individual_images.append(img_array)\n",
        "            individual_filenames.append(filename)\n",
        "            \n",
        "            print(f\"‚úÖ {filename}: {img_array.shape} - Charg√©e avec succ√®s\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur avec {filename}: {e}\")\n",
        "    \n",
        "    print(f\"\\nüéâ {len(individual_images)} image(s) pr√™te(s) pour l'analyse!\")\n",
        "    \n",
        "    # Aper√ßu rapide des images charg√©es\n",
        "    if len(individual_images) > 0:\n",
        "        n_preview = min(4, len(individual_images))\n",
        "        fig, axes = plt.subplots(1, n_preview, figsize=(4*n_preview, 4))\n",
        "        \n",
        "        if n_preview == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for i in range(n_preview):\n",
        "            axes[i].imshow(individual_images[i], cmap='gray')\n",
        "            axes[i].set_title(f'{individual_filenames[i][:20]}...\\n{individual_images[i].shape}')\n",
        "            axes[i].axis('off')\n",
        "        \n",
        "        plt.suptitle('Aper√ßu des Images Charg√©es / Preview of Uploaded Images')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\nelse:\n",
        "    print(\"‚ÑπÔ∏è Aucune image charg√©e / No images uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "method2_zip"
      },
      "source": [
        "### M√©thode 2 : Archive ZIP / ZIP Archive\n",
        "Id√©al pour beaucoup d'images (10+)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_zip_archive"
      },
      "outputs": [],
      "source": [
        "# Chargement d'archive ZIP / Upload ZIP archive\n",
        "print(\"üì¶ M√âTHODE 2: CHARGEMENT D'ARCHIVE ZIP\")\n",
        "print(\"üì¶ METHOD 2: ZIP ARCHIVE UPLOAD\")\n",
        "print(\"-\" * 60)\n",
        "print(\"üí° Conseil: Cette m√©thode est id√©ale pour 10+ images\")\n",
        "print(\"üí° Tip: This method is ideal for 10+ images\")\n",
        "print(\"\")\n",
        "print(\"üî∏ Cr√©ez un fichier .zip contenant vos images de radiographies\")\n",
        "print(\"üî∏ Create a .zip file containing your X-ray images\")\n",
        "print(\"üî∏ Les images peuvent √™tre dans des sous-dossiers\")\n",
        "print(\"üî∏ Images can be in subdirectories\")\n",
        "print(\"\")\n",
        "print(\"üëÜ Cliquez 'Choisir des fichiers' pour s√©lectionner votre fichier .zip:\")\n",
        "\n",
        "# Interface de chargement ZIP\n",
        "uploaded_zip = files.upload()\n",
        "\n",
        "zip_images = []\n",
        "zip_filenames = []\n",
        "\n",
        "if uploaded_zip:\n",
        "    zip_filename = list(uploaded_zip.keys())[0]\n",
        "    \n",
        "    if zip_filename.lower().endswith('.zip'):\n",
        "        print(f\"üì¶ Extraction de {zip_filename}...\")\n",
        "        \n",
        "        try:\n",
        "            # Cr√©er le fichier ZIP temporaire\n",
        "            with open('temp_dataset.zip', 'wb') as f:\n",
        "                f.write(uploaded_zip[zip_filename])\n",
        "            \n",
        "            # Extraire le ZIP\n",
        "            with zipfile.ZipFile('temp_dataset.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall('custom_dataset')\n",
        "            \n",
        "            # Trouver toutes les images dans l'extraction\n",
        "            image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.tiff', '*.tif']\n",
        "            image_files = []\n",
        "            \n",
        "            for ext in image_extensions:\n",
        "                # Chercher r√©cursivement\n",
        "                image_files.extend(glob.glob(os.path.join('custom_dataset', '**', ext), recursive=True))\n",
        "                image_files.extend(glob.glob(os.path.join('custom_dataset', '**', ext.upper()), recursive=True))\n",
        "            \n",
        "            print(f\"üîç {len(image_files)} image(s) trouv√©e(s) dans l'archive\")\n",
        "            \n",
        "            # Charger toutes les images\n",
        "            for img_path in tqdm(image_files, desc=\"Chargement des images\"):\n",
        "                try:\n",
        "                    image = Image.open(img_path)\n",
        "                    \n",
        "                    # Convertir en niveaux de gris\n",
        "                    if image.mode != 'L':\n",
        "                        image = image.convert('L')\n",
        "                    \n",
        "                    img_array = np.array(image)\n",
        "                    \n",
        "                    zip_images.append(img_array)\n",
        "                    zip_filenames.append(os.path.basename(img_path))\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Erreur avec {img_path}: {e}\")\n",
        "            \n",
        "            print(f\"\\nüéâ {len(zip_images)} image(s) charg√©e(s) depuis l'archive!\")\n",
        "            \n",
        "            # Statistiques du dataset\n",
        "            if len(zip_images) > 0:\n",
        "                sizes = [img.shape for img in zip_images]\n",
        "                print(f\"üìä Tailles d'images d√©tect√©es: {set(sizes)}\")\n",
        "                \n",
        "                # Aper√ßu d'un √©chantillon al√©atoire\n",
        "                n_sample = min(6, len(zip_images))\n",
        "                sample_indices = np.random.choice(len(zip_images), n_sample, replace=False)\n",
        "                \n",
        "                fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "                axes = axes.flatten()\n",
        "                \n",
        "                for i, idx in enumerate(sample_indices):\n",
        "                    axes[i].imshow(zip_images[idx], cmap='gray')\n",
        "                    axes[i].set_title(f'{zip_filenames[idx][:20]}...\\n{zip_images[idx].shape}')\n",
        "                    axes[i].axis('off')\n",
        "                \n",
        "                plt.suptitle(f'√âchantillon du Dataset (6/{len(zip_images)} images)')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "            \n",
        "            # Nettoyer le fichier temporaire\n",
        "            os.remove('temp_dataset.zip')\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur lors de l'extraction: {e}\")\n",
        "    \n",
        "    else:\n",
        "        print(\"‚ùå Le fichier charg√© n'est pas une archive ZIP\")\n",
        "\nelse:\n",
        "    print(\"‚ÑπÔ∏è Aucune archive charg√©e / No archive uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "method3_sample"
      },
      "source": [
        "### M√©thode 3 : Dataset d'Exemple / Sample Dataset\n",
        "Pour tester sans vos propres donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_sample_dataset"
      },
      "outputs": [],
      "source": [
        "# Cr√©ation d'un dataset d'exemple pour test / Create sample dataset for testing\n",
        "print(\"üß™ M√âTHODE 3: DATASET D'EXEMPLE POUR TEST\")\n",
        "print(\"üß™ METHOD 3: SAMPLE DATASET FOR TESTING\")\n",
        "print(\"-\" * 60)\n",
        "print(\"üí° Utilise cette option si vous n'avez pas encore vos propres images\")\n",
        "print(\"üí° Use this option if you don't have your own images yet\")\n",
        "print(\"\")\n",
        "\n",
        "def create_sample_dataset(n_images=8):\n",
        "    \"\"\"\n",
        "    Cr√©er un dataset d'exemple avec diff√©rents types de pathologies simul√©es\n",
        "    Create sample dataset with different simulated pathology types\n",
        "    \"\"\"\n",
        "    sample_images = []\n",
        "    sample_filenames = []\n",
        "    sample_descriptions = []\n",
        "    \n",
        "    # Types de cas simul√©s\n",
        "    case_types = [\n",
        "        (\"normal\", \"Radiographie normale\"),\n",
        "        (\"cardiomegaly\", \"Cardiom√©galie simul√©e\"),\n",
        "        (\"pneumonia\", \"Pneumonie simul√©e\"),\n",
        "        (\"nodule\", \"Nodule pulmonaire simul√©\"),\n",
        "        (\"pneumothorax\", \"Pneumothorax simul√©\"),\n",
        "        (\"infiltration\", \"Infiltration pulmonaire simul√©e\"),\n",
        "        (\"atelectasis\", \"At√©lectasie simul√©e\"),\n",
        "        (\"edema\", \"≈íd√®me pulmonaire simul√©\")\n",
        "    ]\n",
        "    \n",
        "    np.random.seed(42)  # Pour la reproductibilit√©\n",
        "    \n",
        "    for i in range(n_images):\n",
        "        case_type, description = case_types[i % len(case_types)]\n",
        "        \n",
        "        # Image de base (thorax normal)\n",
        "        img = np.random.rand(224, 224) * 0.3 + 0.4\n",
        "        \n",
        "        # Structures anatomiques de base\n",
        "        # Poumons\n",
        "        img[50:180, 30:100] *= 0.7   # Poumon gauche\n",
        "        img[50:180, 124:194] *= 0.7  # Poumon droit\n",
        "        \n",
        "        # C≈ìur\n",
        "        img[120:180, 90:134] *= 1.2\n",
        "        \n",
        "        # Ajouter pathologies sp√©cifiques\n",
        "        if case_type == \"cardiomegaly\":\n",
        "            # C≈ìur √©largi\n",
        "            img[110:190, 80:144] *= 1.4\n",
        "            \n",
        "        elif case_type == \"pneumonia\":\n",
        "            # Consolidation dans poumon droit\n",
        "            img[80:140, 140:180] *= 1.6\n",
        "            \n",
        "        elif case_type == \"nodule\":\n",
        "            # Nodule rond dans poumon gauche\n",
        "            center_y, center_x = 100, 70\n",
        "            y, x = np.ogrid[:224, :224]\n",
        "            mask = (x - center_x)**2 + (y - center_y)**2 <= 8**2\n",
        "            img[mask] *= 2.0\n",
        "            \n",
        "        elif case_type == \"pneumothorax\":\n",
        "            # Zone hyperlucide (air) dans poumon droit\n",
        "            img[60:120, 150:190] *= 0.3\n",
        "            \n",
        "        elif case_type == \"infiltration\":\n",
        "            # Infiltrats diffus\n",
        "            img[70:150, 40:90] *= 1.3    # Poumon gauche\n",
        "            img[90:160, 130:170] *= 1.2  # Poumon droit\n",
        "            \n",
        "        elif case_type == \"atelectasis\":\n",
        "            # Collapsus partiel poumon gauche\n",
        "            img[80:130, 35:85] *= 1.7\n",
        "            \n",
        "        elif case_type == \"edema\":\n",
        "            # ≈íd√®me diffus bilat√©ral\n",
        "            img[60:170, 35:95] *= 1.4    # Gauche\n",
        "            img[60:170, 129:189] *= 1.4  # Droit\n",
        "            img[100:170, 85:139] *= 1.3  # Base cardiaque\n",
        "        \n",
        "        # Normaliser l'image\n",
        "        img = np.clip(img, 0, 1)\n",
        "        \n",
        "        # Appliquer un l√©ger flou pour un aspect plus r√©aliste\n",
        "        img = cv2.GaussianBlur(img, (3, 3), 0)\n",
        "        \n",
        "        sample_images.append(img)\n",
        "        sample_filenames.append(f\"sample_{i+1:02d}_{case_type}.png\")\n",
        "        sample_descriptions.append(f\"Cas {i+1}: {description}\")\n",
        "    \n",
        "    return sample_images, sample_filenames, sample_descriptions\n",
        "\n",
        "# Cr√©er le dataset d'exemple\n",
        "choice = input(\"Voulez-vous cr√©er un dataset d'exemple ? (y/n): \").lower()\n",
        "\n",
        "sample_images = []\n",
        "sample_filenames = []\n",
        "sample_descriptions = []\n",
        "\n",
        "if choice in ['y', 'yes', 'oui', 'o']:\n",
        "    print(\"\\nüîÑ Cr√©ation du dataset d'exemple...\")\n",
        "    sample_images, sample_filenames, sample_descriptions = create_sample_dataset(8)\n",
        "    \n",
        "    print(f\"‚úÖ {len(sample_images)} images d'exemple cr√©√©es!\")\n",
        "    \n",
        "    # Affichage du dataset d'exemple\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, (img, filename, desc) in enumerate(zip(sample_images, sample_filenames, sample_descriptions)):\n",
        "        axes[i].imshow(img, cmap='gray')\n",
        "        axes[i].set_title(f'{filename}\\n{desc}', fontsize=10)\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.suptitle('Dataset d\\'Exemple Cr√©√© / Sample Dataset Created', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Dataset d'exemple non cr√©√© / Sample dataset not created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_consolidation"
      },
      "source": [
        "## üìã Consolidation du Dataset / Dataset Consolidation\n",
        "\n",
        "Regroupons toutes les images charg√©es en un seul dataset :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "consolidate_dataset"
      },
      "outputs": [],
      "source": [
        "# Consolidation de toutes les images en un dataset unifi√©\n",
        "print(\"üìã CONSOLIDATION DU DATASET\")\n",
        "print(\"üìã DATASET CONSOLIDATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Combiner toutes les sources d'images\n",
        "all_images = []\n",
        "all_filenames = []\n",
        "all_sources = []\n",
        "\n",
        "# Ajouter images individuelles\n",
        "if 'individual_images' in locals() and individual_images:\n",
        "    all_images.extend(individual_images)\n",
        "    all_filenames.extend(individual_filenames)\n",
        "    all_sources.extend(['Individual'] * len(individual_images))\n",
        "    print(f\"‚ûï {len(individual_images)} image(s) individuelles ajout√©es\")\n",
        "\n",
        "# Ajouter images du ZIP\n",
        "if 'zip_images' in locals() and zip_images:\n",
        "    all_images.extend(zip_images)\n",
        "    all_filenames.extend(zip_filenames)\n",
        "    all_sources.extend(['ZIP Archive'] * len(zip_images))\n",
        "    print(f\"‚ûï {len(zip_images)} image(s) du ZIP ajout√©es\")\n",
        "\n",
        "# Ajouter images d'exemple\n",
        "if 'sample_images' in locals() and sample_images:\n",
        "    all_images.extend(sample_images)\n",
        "    all_filenames.extend(sample_filenames)\n",
        "    all_sources.extend(['Sample'] * len(sample_images))\n",
        "    print(f\"‚ûï {len(sample_images)} image(s) d'exemple ajout√©es\")\n",
        "\n",
        "# V√©rifier si nous avons des images\n",
        "if not all_images:\n",
        "    print(\"‚ùå Aucune image disponible pour l'analyse!\")\n",
        "    print(\"üí° Veuillez charger des images en utilisant l'une des m√©thodes ci-dessus\")\n",
        "else:\n",
        "    print(f\"\\nüéâ DATASET CONSOLID√â: {len(all_images)} image(s) au total\")\n",
        "    \n",
        "    # Cr√©er un DataFrame pour organiser les m√©tadonn√©es\n",
        "    dataset_info = pd.DataFrame({\n",
        "        'ID': range(len(all_images)),\n",
        "        'Filename': all_filenames,\n",
        "        'Source': all_sources,\n",
        "        'Shape': [img.shape for img in all_images],\n",
        "        'Size_MB': [img.nbytes / 1024 / 1024 for img in all_images],\n",
        "        'Min_Pixel': [img.min() for img in all_images],\n",
        "        'Max_Pixel': [img.max() for img in all_images]\n",
        "    })\n",
        "    \n",
        "    print(\"\\nüìä INFORMATIONS DU DATASET:\")\n",
        "    print(dataset_info.to_string(index=False))\n",
        "    \n",
        "    # Statistiques g√©n√©rales\n",
        "    print(f\"\\nüìà STATISTIQUES:\")\n",
        "    print(f\"   ‚Ä¢ Nombre total d'images: {len(all_images)}\")\n",
        "    print(f\"   ‚Ä¢ Tailles d'images uniques: {len(set([img.shape for img in all_images]))}\")\n",
        "    print(f\"   ‚Ä¢ Taille totale du dataset: {sum(dataset_info['Size_MB']):.2f} MB\")\n",
        "    print(f\"   ‚Ä¢ Sources: {', '.join(set(all_sources))}\")\n",
        "    \n",
        "    # Affichage d'un r√©sum√© visuel\n",
        "    if len(all_images) > 0:\n",
        "        # Graphique des sources\n",
        "        source_counts = pd.Series(all_sources).value_counts()\n",
        "        \n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "        \n",
        "        # Graphique en secteurs des sources\n",
        "        axes[0].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
        "        axes[0].set_title('R√©partition par Source\\nSource Distribution')\n",
        "        \n",
        "        # Histogramme des tailles d'images\n",
        "        image_sizes = [img.shape[0] * img.shape[1] for img in all_images]\n",
        "        axes[1].hist(image_sizes, bins=min(10, len(set(image_sizes))), alpha=0.7, edgecolor='black')\n",
        "        axes[1].set_xlabel('Nombre de Pixels')\n",
        "        axes[1].set_ylabel('Nombre d\\'Images')\n",
        "        axes[1].set_title('Distribution des Tailles\\nSize Distribution')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Dataset consolid√© et pr√™t pour l'analyse!\")\n",
        "    print(\"‚úÖ Dataset consolidated and ready for analysis!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batch_preprocessing"
      },
      "source": [
        "## üîß Pr√©paration en Lot / Batch Preprocessing\n",
        "\n",
        "Pr√©parons toutes les images pour l'analyse avec TorchXRayVision :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_preprocessing_code"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset_batch(images, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Pr√©paration en lot de toutes les images du dataset\n",
        "    Batch preprocessing of all images in the dataset\n",
        "    \"\"\"\n",
        "    print(\"üîß PR√âPARATION EN LOT DU DATASET\")\n",
        "    print(\"üîß BATCH PREPROCESSING OF DATASET\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    processed_images = []\n",
        "    processed_tensors = []\n",
        "    preprocessing_info = []\n",
        "    \n",
        "    print(f\"üìè Redimensionnement vers: {target_size}\")\n",
        "    print(f\"üéØ Nombre d'images √† traiter: {len(images)}\")\n",
        "    print(\"\")\n",
        "    \n",
        "    for i, img in enumerate(tqdm(images, desc=\"Pr√©paration des images\")):\n",
        "        try:\n",
        "            # Copie de l'image originale\n",
        "            img_work = img.copy()\n",
        "            \n",
        "            # Informations originales\n",
        "            original_shape = img_work.shape\n",
        "            original_dtype = img_work.dtype\n",
        "            original_range = (img_work.min(), img_work.max())\n",
        "            \n",
        "            # Redimensionnement si n√©cessaire\n",
        "            if img_work.shape != target_size:\n",
        "                img_work = cv2.resize(img_work, target_size)\n",
        "            \n",
        "            # Normalisation des pixels entre 0 et 1\n",
        "            if img_work.max() > 1:\n",
        "                img_work = img_work.astype(np.float32) / 255.0\n",
        "            else:\n",
        "                img_work = img_work.astype(np.float32)\n",
        "            \n",
        "            # Normalisation Z-score pour TorchXRayVision\n",
        "            mean_val = np.mean(img_work)\n",
        "            std_val = np.std(img_work)\n",
        "            \n",
        "            if std_val > 0:\n",
        "                img_normalized = (img_work - mean_val) / std_val\n",
        "            else:\n",
        "                img_normalized = img_work - mean_val\n",
        "            \n",
        "            # Conversion en tensor PyTorch\n",
        "            img_tensor = torch.FloatTensor(img_normalized)\n",
        "            img_tensor = img_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]\n",
        "            img_tensor = img_tensor.to(device)\n",
        "            \n",
        "            # Stocker les r√©sultats\n",
        "            processed_images.append(img_work)\n",
        "            processed_tensors.append(img_tensor)\n",
        "            \n",
        "            # Informations de pr√©paration\n",
        "            prep_info = {\n",
        "                'original_shape': original_shape,\n",
        "                'original_dtype': str(original_dtype),\n",
        "                'original_range': original_range,\n",
        "                'processed_shape': img_work.shape,\n",
        "                'processed_range': (img_work.min(), img_work.max()),\n",
        "                'normalized_range': (img_normalized.min(), img_normalized.max()),\n",
        "                'mean': mean_val,\n",
        "                'std': std_val,\n",
        "                'tensor_shape': tuple(img_tensor.shape)\n",
        "            }\n",
        "            preprocessing_info.append(prep_info)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur avec l'image {i}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\n‚úÖ {len(processed_images)}/{len(images)} images pr√©par√©es avec succ√®s\")\n",
        "    \n",
        "    # Statistiques de pr√©paration\n",
        "    if preprocessing_info:\n",
        "        prep_df = pd.DataFrame(preprocessing_info)\n",
        "        \n",
        "        print(f\"\\nüìä STATISTIQUES DE PR√âPARATION:\")\n",
        "        print(f\"   ‚Ä¢ Forme finale: {target_size}\")\n",
        "        print(f\"   ‚Ä¢ Range normalis√© moyen: [{prep_df['normalized_range'].apply(lambda x: x[0]).mean():.3f}, {prep_df['normalized_range'].apply(lambda x: x[1]).mean():.3f}]\")\n",
        "        print(f\"   ‚Ä¢ Moyenne des moyennes: {prep_df['mean'].mean():.3f}\")\n",
        "        print(f\"   ‚Ä¢ Moyenne des √©carts-types: {prep_df['std'].mean():.3f}\")\n",
        "        \n",
        "        # Graphique des statistiques de normalisation\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        \n",
        "        # Distribution des moyennes\n",
        "        axes[0].hist(prep_df['mean'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        axes[0].set_xlabel('Moyenne des Pixels')\n",
        "        axes[0].set_ylabel('Nombre d\\'Images')\n",
        "        axes[0].set_title('Distribution des Moyennes\\nper Image')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Distribution des √©carts-types\n",
        "        axes[1].hist(prep_df['std'], bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "        axes[1].set_xlabel('√âcart-type des Pixels')\n",
        "        axes[1].set_ylabel('Nombre d\\'Images')\n",
        "        axes[1].set_title('Distribution des √âcarts-types\\nper Image')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Corr√©lation moyenne vs √©cart-type\n",
        "        axes[2].scatter(prep_df['mean'], prep_df['std'], alpha=0.7, s=60)\n",
        "        axes[2].set_xlabel('Moyenne des Pixels')\n",
        "        axes[2].set_ylabel('√âcart-type des Pixels')\n",
        "        axes[2].set_title('Corr√©lation Moyenne vs\\n√âcart-type')\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.suptitle('Statistiques de Normalisation du Dataset', fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    return processed_images, processed_tensors, preprocessing_info\n",
        "\n",
        "# Appliquer le pr√©processing si nous avons des images\n",
        "if 'all_images' in locals() and all_images:\n",
        "    processed_images, processed_tensors, prep_info = preprocess_dataset_batch(all_images)\n",
        "    \n",
        "    print(\"\\nüéØ Dataset pr√™t pour l'analyse avec TorchXRayVision!\")\n",
        "    print(\"üéØ Dataset ready for TorchXRayVision analysis!\")\n",
        "else:\n",
        "    print(\"‚ùå Aucune image √† pr√©parer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batch_analysis"
      },
      "source": [
        "## üîç Analyse en Lot / Batch Analysis\n",
        "\n",
        "Analysons maintenant toutes les images avec les mod√®les TorchXRayVision :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_analysis_code"
      },
      "outputs": [],
      "source": [
        "def analyze_dataset_batch(processed_tensors, filenames, models_dict, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Analyse en lot de tout le dataset avec tous les mod√®les disponibles\n",
        "    Batch analysis of entire dataset with all available models\n",
        "    \"\"\"\n",
        "    print(\"üîç ANALYSE EN LOT DU DATASET\")\n",
        "    print(\"üîç BATCH DATASET ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"üìä Images √† analyser: {len(processed_tensors)}\")\n",
        "    print(f\"ü§ñ Mod√®les disponibles: {list(models_dict.keys())}\")\n",
        "    print(f\"üéØ Seuil de d√©tection: {threshold}\")\n",
        "    print(\"\")\n",
        "    \n",
        "    # Stocker tous les r√©sultats\n",
        "    all_results = {}\n",
        "    analysis_summary = []\n",
        "    \n",
        "    # Analyser avec chaque mod√®le\n",
        "    for model_name, model in models_dict.items():\n",
        "        print(f\"üîÑ Analyse avec le mod√®le {model_name}...\")\n",
        "        \n",
        "        model_results = []\n",
        "        pathologies = model.pathologies\n",
        "        \n",
        "        # Analyser chaque image\n",
        "        for i, (tensor, filename) in enumerate(tqdm(zip(processed_tensors, filenames), \n",
        "                                                   desc=f\"Analyse {model_name}\",\n",
        "                                                   total=len(processed_tensors))):\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(tensor)\n",
        "                    probabilities = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
        "                \n",
        "                # Cr√©er le r√©sultat pour cette image\n",
        "                image_result = {\n",
        "                    'image_id': i,\n",
        "                    'filename': filename,\n",
        "                    'model': model_name\n",
        "                }\n",
        "                \n",
        "                # Ajouter les probabilit√©s pour chaque pathologie\n",
        "                for pathology, prob in zip(pathologies, probabilities):\n",
        "                    image_result[pathology] = prob\n",
        "                    image_result[f'{pathology}_detected'] = prob > threshold\n",
        "                \n",
        "                # Compter les d√©tections totales\n",
        "                image_result['total_detections'] = sum(prob > threshold for prob in probabilities)\n",
        "                image_result['max_probability'] = max(probabilities)\n",
        "                image_result['avg_probability'] = np.mean(probabilities)\n",
        "                \n",
        "                model_results.append(image_result)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Erreur avec {filename} sur {model_name}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        all_results[model_name] = model_results\n",
        "        \n",
        "        # R√©sum√© pour ce mod√®le\n",
        "        if model_results:\n",
        "            total_detections = sum(result['total_detections'] for result in model_results)\n",
        "            avg_detections_per_image = total_detections / len(model_results)\n",
        "            \n",
        "            summary = {\n",
        "                'model': model_name,\n",
        "                'images_analyzed': len(model_results),\n",
        "                'total_detections': total_detections,\n",
        "                'avg_detections_per_image': avg_detections_per_image,\n",
        "                'max_detections_single_image': max(result['total_detections'] for result in model_results),\n",
        "                'images_with_detections': sum(1 for result in model_results if result['total_detections'] > 0)\n",
        "            }\n",
        "            analysis_summary.append(summary)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Analyse termin√©e pour {len(processed_tensors)} images\")\n",
        "    \n",
        "    # Affichage du r√©sum√©\n",
        "    if analysis_summary:\n",
        "        summary_df = pd.DataFrame(analysis_summary)\n",
        "        \n",
        "        print(\"\\nüìä R√âSUM√â DE L'ANALYSE:\")\n",
        "        print(summary_df.to_string(index=False))\n",
        "        \n",
        "        # Graphiques de r√©sum√©\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "        \n",
        "        # 1. D√©tections totales par mod√®le\n",
        "        axes[0, 0].bar(summary_df['model'], summary_df['total_detections'], \n",
        "                      color=['skyblue', 'lightcoral', 'lightgreen'][:len(summary_df)])\n",
        "        axes[0, 0].set_title('D√©tections Totales par Mod√®le')\n",
        "        axes[0, 0].set_ylabel('Nombre de D√©tections')\n",
        "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # 2. Moyenne de d√©tections par image\n",
        "        axes[0, 1].bar(summary_df['model'], summary_df['avg_detections_per_image'],\n",
        "                      color=['orange', 'purple', 'brown'][:len(summary_df)])\n",
        "        axes[0, 1].set_title('Moyenne D√©tections/Image')\n",
        "        axes[0, 1].set_ylabel('D√©tections par Image')\n",
        "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # 3. Images avec vs sans d√©tections\n",
        "        models_names = summary_df['model'].tolist()\n",
        "        with_detections = summary_df['images_with_detections'].tolist()\n",
        "        without_detections = [summary_df['images_analyzed'].iloc[i] - with_detections[i] \n",
        "                            for i in range(len(with_detections))]\n",
        "        \n",
        "        x = np.arange(len(models_names))\n",
        "        width = 0.35\n",
        "        \n",
        "        axes[1, 0].bar(x, with_detections, width, label='Avec D√©tections', color='tomato')\n",
        "        axes[1, 0].bar(x, without_detections, width, bottom=with_detections, \n",
        "                      label='Sans D√©tections', color='lightblue')\n",
        "        axes[1, 0].set_title('Images avec/sans D√©tections')\n",
        "        axes[1, 0].set_ylabel('Nombre d\\'Images')\n",
        "        axes[1, 0].set_xticks(x)\n",
        "        axes[1, 0].set_xticklabels(models_names, rotation=45)\n",
        "        axes[1, 0].legend()\n",
        "        \n",
        "        # 4. Maximum de d√©tections sur une seule image\n",
        "        axes[1, 1].bar(summary_df['model'], summary_df['max_detections_single_image'],\n",
        "                      color=['gold', 'silver', 'bronze'][:len(summary_df)])\n",
        "        axes[1, 1].set_title('Maximum D√©tections\\n(Une Seule Image)')\n",
        "        axes[1, 1].set_ylabel('Nombre de D√©tections')\n",
        "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.suptitle('R√âSUM√â DE L\\'ANALYSE DU DATASET', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    return all_results, analysis_summary\n",
        "\n",
        "# Effectuer l'analyse si nous avons des tenseurs pr√©par√©s\n",
        "if 'processed_tensors' in locals() and processed_tensors:\n",
        "    batch_results, batch_summary = analyze_dataset_batch(processed_tensors, all_filenames, models)\n",
        "    \n",
        "    print(\"\\nüéâ Analyse en lot termin√©e avec succ√®s!\")\n",
        "    print(\"üéâ Batch analysis completed successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå Aucun tensor pr√©par√© pour l'analyse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "detailed_results"
      },
      "source": [
        "## üìä Analyse D√©taill√©e des R√©sultats / Detailed Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "detailed_analysis"
      },
      "outputs": [],
      "source": [
        "def create_detailed_analysis(batch_results, filenames, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Analyse d√©taill√©e avec visualisations pour l'ensemble du dataset\n",
        "    Detailed analysis with visualizations for the entire dataset\n",
        "    \"\"\"\n",
        "    print(\"üìä ANALYSE D√âTAILL√âE DES R√âSULTATS\")\n",
        "    print(\"üìä DETAILED RESULTS ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if not batch_results:\n",
        "        print(\"‚ùå Aucun r√©sultat disponible pour l'analyse\")\n",
        "        return\n",
        "    \n",
        "    # Prendre le premier mod√®le pour l'analyse principale\n",
        "    main_model = list(batch_results.keys())[0]\n",
        "    main_results = batch_results[main_model]\n",
        "    \n",
        "    print(f\"üéØ Analyse principale bas√©e sur le mod√®le: {main_model}\")\n",
        "    print(f\"üìà Nombre d'images analys√©es: {len(main_results)}\")\n",
        "    \n",
        "    # 1. Analyse des pathologies les plus fr√©quentes\n",
        "    pathologies = [col for col in main_results[0].keys() \n",
        "                  if col not in ['image_id', 'filename', 'model', 'total_detections', \n",
        "                               'max_probability', 'avg_probability'] \n",
        "                  and not col.endswith('_detected')]\n",
        "    \n",
        "    # Compter les d√©tections par pathologie\n",
        "    pathology_counts = {}\n",
        "    pathology_avg_probs = {}\n",
        "    \n",
        "    for pathology in pathologies:\n",
        "        detections = sum(1 for result in main_results if result.get(f'{pathology}_detected', False))\n",
        "        avg_prob = np.mean([result.get(pathology, 0) for result in main_results])\n",
        "        \n",
        "        pathology_counts[pathology] = detections\n",
        "        pathology_avg_probs[pathology] = avg_prob\n",
        "    \n",
        "    # Trier par fr√©quence\n",
        "    sorted_pathologies = sorted(pathology_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    print(f\"\\nüè• TOP 10 PATHOLOGIES D√âTECT√âES:\")\n",
        "    for i, (pathology, count) in enumerate(sorted_pathologies[:10]):\n",
        "        percentage = (count / len(main_results)) * 100\n",
        "        avg_prob = pathology_avg_probs[pathology]\n",
        "        print(f\"   {i+1:2d}. {pathology:25s}: {count:3d}/{len(main_results)} ({percentage:5.1f}%) - Prob moy: {avg_prob:.3f}\")\n",
        "    \n",
        "    # 2. Analyse des images les plus probl√©matiques\n",
        "    print(f\"\\nüö® IMAGES AVEC LE PLUS DE PATHOLOGIES:\")\n",
        "    sorted_images = sorted(main_results, key=lambda x: x['total_detections'], reverse=True)\n",
        "    \n",
        "    for i, result in enumerate(sorted_images[:5]):\n",
        "        detected_paths = [path for path in pathologies \n",
        "                         if result.get(f'{path}_detected', False)]\n",
        "        print(f\"   {i+1}. {result['filename']:30s}: {result['total_detections']} d√©tections\")\n",
        "        if detected_paths:\n",
        "            print(f\"      ‚Üí {', '.join(detected_paths[:3])}{'...' if len(detected_paths) > 3 else ''}\")\n",
        "    \n",
        "    # 3. Visualisations compl√®tes\n",
        "    create_comprehensive_visualizations(batch_results, pathologies, filenames, threshold)\n",
        "    \n",
        "    # 4. Comparaison inter-mod√®les si disponible\n",
        "    if len(batch_results) > 1:\n",
        "        create_model_comparison_analysis(batch_results, pathologies)\n",
        "    \n",
        "    # 5. Matrice de corr√©lation des pathologies\n",
        "    create_pathology_correlation_matrix(main_results, pathologies)\n",
        "    \n",
        "    return sorted_pathologies, sorted_images\n",
        "\n",
        "def create_comprehensive_visualizations(batch_results, pathologies, filenames, threshold):\n",
        "    \"\"\"\n",
        "    Cr√©er des visualisations compl√®tes pour l'analyse du dataset\n",
        "    Create comprehensive visualizations for dataset analysis\n",
        "    \"\"\"\n",
        "    main_model = list(batch_results.keys())[0]\n",
        "    results = batch_results[main_model]\n",
        "    \n",
        "    fig = plt.figure(figsize=(20, 16))\n",
        "    gs = fig.add_gridspec(4, 3, height_ratios=[1, 1, 1, 1], hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # 1. Heatmap des d√©tections par image et pathologie\n",
        "    ax1 = fig.add_subplot(gs[0, :])\n",
        "    \n",
        "    # Cr√©er matrice de d√©tection\n",
        "    top_pathologies = sorted(pathologies, \n",
        "                           key=lambda p: sum(1 for r in results if r.get(f'{p}_detected', False)), \n",
        "                           reverse=True)[:15]  # Top 15 pour la lisibilit√©\n",
        "    \n",
        "    detection_matrix = []\n",
        "    image_labels = []\n",
        "    \n",
        "    for result in results:\n",
        "        row = [1 if result.get(f'{path}_detected', False) else 0 for path in top_pathologies]\n",
        "        detection_matrix.append(row)\n",
        "        image_labels.append(result['filename'][:15] + '...' if len(result['filename']) > 15 else result['filename'])\n",
        "    \n",
        "    detection_matrix = np.array(detection_matrix).T  # Transpose pour avoir pathologies en lignes\n",
        "    \n",
        "    sns.heatmap(detection_matrix, \n",
        "                xticklabels=image_labels,\n",
        "                yticklabels=[p[:15] for p in top_pathologies],\n",
        "                cmap='Reds', cbar_kws={'label': 'D√©tection (0=Non, 1=Oui)'},\n",
        "                ax=ax1)\n",
        "    ax1.set_title(f'Matrice de D√©tection - {len(results)} Images vs Top 15 Pathologies', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Images du Dataset')\n",
        "    ax1.set_ylabel('Pathologies')\n",
        "    \n",
        "    # Rotation des labels pour lisibilit√©\n",
        "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
        "    \n",
        "    # 2. Distribution des d√©tections par image\n",
        "    ax2 = fig.add_subplot(gs[1, 0])\n",
        "    detections_per_image = [result['total_detections'] for result in results]\n",
        "    ax2.hist(detections_per_image, bins=range(max(detections_per_image)+2), \n",
        "            alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    ax2.set_xlabel('Nombre de D√©tections')\n",
        "    ax2.set_ylabel('Nombre d\\'Images')\n",
        "    ax2.set_title('Distribution des\\nD√©tections par Image')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Top pathologies (barres)\n",
        "    ax3 = fig.add_subplot(gs[1, 1])\n",
        "    top_10_paths = top_pathologies[:10]\n",
        "    counts = [sum(1 for r in results if r.get(f'{path}_detected', False)) for path in top_10_paths]\n",
        "    \n",
        "    bars = ax3.barh(range(len(top_10_paths)), counts, color='lightcoral', alpha=0.7)\n",
        "    ax3.set_yticks(range(len(top_10_paths)))\n",
        "    ax3.set_yticklabels([p[:12] for p in top_10_paths])\n",
        "    ax3.set_xlabel('Nombre de D√©tections')\n",
        "    ax3.set_title('Top 10 Pathologies\\nles Plus Fr√©quentes')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
        "        width = bar.get_width()\n",
        "        ax3.text(width + 0.1, bar.get_y() + bar.get_height()/2, \n",
        "                f'{count}', ha='left', va='center', fontweight='bold')\n",
        "    \n",
        "    # 4. Distribution des probabilit√©s moyennes\n",
        "    ax4 = fig.add_subplot(gs[1, 2])\n",
        "    avg_probs = [result['avg_probability'] for result in results]\n",
        "    ax4.hist(avg_probs, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "    ax4.axvline(x=threshold, color='red', linestyle='--', linewidth=2, label=f'Seuil ({threshold})')\n",
        "    ax4.set_xlabel('Probabilit√© Moyenne')\n",
        "    ax4.set_ylabel('Nombre d\\'Images')\n",
        "    ax4.set_title('Distribution des\\nProbabilit√©s Moyennes')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Corr√©lation entre d√©tections et probabilit√©s max\n",
        "    ax5 = fig.add_subplot(gs[2, 0])\n",
        "    max_probs = [result['max_probability'] for result in results]\n",
        "    ax5.scatter(detections_per_image, max_probs, alpha=0.6, s=50)\n",
        "    ax5.set_xlabel('Nombre de D√©tections')\n",
        "    ax5.set_ylabel('Probabilit√© Maximale')\n",
        "    ax5.set_title('D√©tections vs\\nProbabilit√© Max')\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Images par cat√©gorie de s√©v√©rit√©\n",
        "    ax6 = fig.add_subplot(gs[2, 1])\n",
        "    \n",
        "    # Cat√©goriser les images par s√©v√©rit√©\n",
        "    normal_images = sum(1 for r in results if r['total_detections'] == 0)\n",
        "    mild_images = sum(1 for r in results if 1 <= r['total_detections'] <= 2)\n",
        "    moderate_images = sum(1 for r in results if 3 <= r['total_detections'] <= 5)\n",
        "    severe_images = sum(1 for r in results if r['total_detections'] > 5)\n",
        "    \n",
        "    categories = ['Normal\\n(0)', 'L√©ger\\n(1-2)', 'Mod√©r√©\\n(3-5)', 'S√©v√®re\\n(6+)']\n",
        "    counts_sev = [normal_images, mild_images, moderate_images, severe_images]\n",
        "    colors_sev = ['green', 'yellow', 'orange', 'red']\n",
        "    \n",
        "    wedges, texts, autotexts = ax6.pie(counts_sev, labels=categories, colors=colors_sev,\n",
        "                                      autopct='%1.1f%%', startangle=90)\n",
        "    ax6.set_title('R√©partition par\\nNiveau de S√©v√©rit√©')\n",
        "    \n",
        "    # 7. Timeline/ordre des images\n",
        "    ax7 = fig.add_subplot(gs[2, 2])\n",
        "    image_indices = range(len(results))\n",
        "    ax7.plot(image_indices, detections_per_image, 'bo-', alpha=0.7, markersize=4)\n",
        "    ax7.set_xlabel('Index de l\\'Image')\n",
        "    ax7.set_ylabel('Nombre de D√©tections')\n",
        "    ax7.set_title('√âvolution des D√©tections\\nSelon l\\'Ordre')\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 8. Comparaison probabilit√©s vs d√©tections (scatter large)\n",
        "    ax8 = fig.add_subplot(gs[3, :])\n",
        "    \n",
        "    # Pour chaque pathologie, afficher prob moyenne vs fr√©quence de d√©tection\n",
        "    path_freq = []\n",
        "    path_avg_prob = []\n",
        "    path_names = []\n",
        "    \n",
        "    for pathology in top_pathologies[:20]:  # Top 20\n",
        "        freq = sum(1 for r in results if r.get(f'{pathology}_detected', False))\n",
        "        avg_prob = np.mean([r.get(pathology, 0) for r in results])\n",
        "        \n",
        "        path_freq.append(freq)\n",
        "        path_avg_prob.append(avg_prob)\n",
        "        path_names.append(pathology)\n",
        "    \n",
        "    scatter = ax8.scatter(path_freq, path_avg_prob, s=100, alpha=0.7, c=range(len(path_freq)), cmap='viridis')\n",
        "    \n",
        "    # Ajouter les noms des pathologies\n",
        "    for i, (freq, prob, name) in enumerate(zip(path_freq, path_avg_prob, path_names)):\n",
        "        ax8.annotate(name[:10], (freq, prob), xytext=(5, 5), textcoords='offset points', \n",
        "                    fontsize=8, alpha=0.8)\n",
        "    \n",
        "    ax8.set_xlabel('Fr√©quence de D√©tection (nombre d\\'images)')\n",
        "    ax8.set_ylabel('Probabilit√© Moyenne')\n",
        "    ax8.set_title('Analyse Pathologie: Fr√©quence vs Probabilit√© Moyenne', fontweight='bold')\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle(f'ANALYSE COMPL√àTE DU DATASET - {len(results)} Images', \n",
        "                fontsize=18, fontweight='bold')\n",
        "    plt.show()\n",
        "\n",
        "def create_pathology_correlation_matrix(results, pathologies):\n",
        "    \"\"\"\n",
        "    Cr√©er une matrice de corr√©lation entre pathologies\n",
        "    Create correlation matrix between pathologies\n",
        "    \"\"\"\n",
        "    print(\"\\nüîó ANALYSE DES CORR√âLATIONS ENTRE PATHOLOGIES\")\n",
        "    \n",
        "    # Cr√©er matrice de probabilit√©s\n",
        "    prob_matrix = []\n",
        "    for pathology in pathologies:\n",
        "        probs = [result.get(pathology, 0) for result in results]\n",
        "        prob_matrix.append(probs)\n",
        "    \n",
        "    prob_matrix = np.array(prob_matrix)\n",
        "    correlation_matrix = np.corrcoef(prob_matrix)\n",
        "    \n",
        "    # Visualiser seulement les corr√©lations les plus significatives\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    \n",
        "    # Masquer la diagonale et les corr√©lations faibles\n",
        "    mask = np.zeros_like(correlation_matrix, dtype=bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "    mask[np.abs(correlation_matrix) < 0.3] = True\n",
        "    \n",
        "    sns.heatmap(correlation_matrix, \n",
        "                mask=mask,\n",
        "                annot=True, \n",
        "                fmt='.2f', \n",
        "                cmap='RdBu_r',\n",
        "                vmin=-1, vmax=1,\n",
        "                xticklabels=[p[:12] for p in pathologies],\n",
        "                yticklabels=[p[:12] for p in pathologies],\n",
        "                cbar_kws={'label': 'Coefficient de Corr√©lation'})\n",
        "    \n",
        "    plt.title('Matrice de Corr√©lation entre Pathologies\\n(Seulement corr√©lations |r| > 0.3)', \n",
        "             fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Trouver les corr√©lations les plus fortes\n",
        "    strong_correlations = []\n",
        "    for i in range(len(pathologies)):\n",
        "        for j in range(i+1, len(pathologies)):\n",
        "            corr = correlation_matrix[i, j]\n",
        "            if abs(corr) > 0.5:  # Corr√©lations fortes\n",
        "                strong_correlations.append((pathologies[i], pathologies[j], corr))\n",
        "    \n",
        "    if strong_correlations:\n",
        "        print(\"\\nüîó CORR√âLATIONS FORTES D√âTECT√âES (|r| > 0.5):\")\n",
        "        strong_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "        for path1, path2, corr in strong_correlations[:10]:\n",
        "            direction = \"positive\" if corr > 0 else \"n√©gative\"\n",
        "            print(f\"   ‚Ä¢ {path1} ‚Üî {path2}: r = {corr:.3f} ({direction})\")\n",
        "\n",
        "# Effectuer l'analyse d√©taill√©e si nous avons des r√©sultats\n",
        "if 'batch_results' in locals() and batch_results:\n",
        "    top_pathologies, problematic_images = create_detailed_analysis(batch_results, all_filenames)\n",
        "else:\n",
        "    print(\"‚ùå Aucun r√©sultat disponible pour l'analyse d√©taill√©e\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export_results"
      },
      "source": [
        "## üíæ Export des R√©sultats / Export Results\n",
        "\n",
        "Sauvegardons et exportons tous les r√©sultats pour usage ult√©rieur :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_results_code"
      },
      "outputs": [],
      "source": [
        "def export_dataset_results(batch_results, filenames, processed_images, analysis_summary):\n",
        "    \"\"\"\n",
        "    Exporter tous les r√©sultats dans diff√©rents formats\n",
        "    Export all results in different formats\n",
        "    \"\"\"\n",
        "    print(\"üíæ EXPORT DES R√âSULTATS\")\n",
        "    print(\"üíæ EXPORTING RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # 1. Export en CSV (pour Excel, analyse statistique)\n",
        "    print(\"üìä Export CSV...\")\n",
        "    \n",
        "    for model_name, results in batch_results.items():\n",
        "        df = pd.DataFrame(results)\n",
        "        csv_filename = f'results/dataset_analysis_{model_name}_{timestamp}.csv'\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        print(f\"   ‚úÖ {csv_filename}\")\n",
        "    \n",
        "    # 2. Export du r√©sum√© ex√©cutif\n",
        "    print(\"üìã Export r√©sum√© ex√©cutif...\")\n",
        "    \n",
        "    summary_df = pd.DataFrame(analysis_summary)\n",
        "    summary_filename = f'results/dataset_summary_{timestamp}.csv'\n",
        "    summary_df.to_csv(summary_filename, index=False)\n",
        "    print(f\"   ‚úÖ {summary_filename}\")\n",
        "    \n",
        "    # 3. Export JSON complet (pour int√©gration avec d'autres outils)\n",
        "    print(\"üóÇÔ∏è Export JSON...\")\n",
        "    \n",
        "    json_data = {\n",
        "        'metadata': {\n",
        "            'timestamp': timestamp,\n",
        "            'total_images': len(filenames),\n",
        "            'models_used': list(batch_results.keys()),\n",
        "            'analysis_date': datetime.now().isoformat()\n",
        "        },\n",
        "        'results': batch_results,\n",
        "        'summary': analysis_summary,\n",
        "        'filenames': filenames\n",
        "    }\n",
        "    \n",
        "    json_filename = f'results/complete_analysis_{timestamp}.json'\n",
        "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(json_data, f, indent=2, ensure_ascii=False, default=str)\n",
        "    print(f\"   ‚úÖ {json_filename}\")\n",
        "    \n",
        "    # 4. Rapport textuel d√©taill√©\n",
        "    print(\"üìù G√©n√©ration rapport textuel...\")\n",
        "    \n",
        "    report_filename = f'results/detailed_report_{timestamp}.txt'\n",
        "    generate_detailed_text_report(batch_results, analysis_summary, report_filename, timestamp)\n",
        "    print(f\"   ‚úÖ {report_filename}\")\n",
        "    \n",
        "    # 5. Package de t√©l√©chargement\n",
        "    print(\"üì¶ Cr√©ation package de t√©l√©chargement...\")\n",
        "    \n",
        "    # Zipper tous les r√©sultats\n",
        "    zip_filename = f'dataset_analysis_complete_{timestamp}.zip'\n",
        "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "        # Ajouter tous les fichiers du dossier results\n",
        "        for filename in glob.glob('results/*'):\n",
        "            if timestamp in filename:\n",
        "                zipf.write(filename, os.path.basename(filename))\n",
        "    \n",
        "    print(f\"   ‚úÖ {zip_filename}\")\n",
        "    \n",
        "    # 6. Interface de t√©l√©chargement Colab\n",
        "    print(\"\\nüì§ T√âL√âCHARGEMENT DES R√âSULTATS:\")\n",
        "    print(\"Choisissez quels fichiers t√©l√©charger:\")\n",
        "    \n",
        "    download_options = {\n",
        "        '1': ('Package complet (ZIP)', zip_filename),\n",
        "        '2': ('R√©sum√© ex√©cutif (CSV)', summary_filename),\n",
        "        '3': ('Rapport d√©taill√© (TXT)', report_filename),\n",
        "        '4': ('Donn√©es compl√®tes (JSON)', json_filename)\n",
        "    }\n",
        "    \n",
        "    for key, (desc, filename) in download_options.items():\n",
        "        print(f\"   {key}. {desc}\")\n",
        "    \n",
        "    # Interface de s√©lection\n",
        "    choice = input(\"\\nEntrez le num√©ro de votre choix (ou 'all' pour tout t√©l√©charger): \")\n",
        "    \n",
        "    if choice.lower() == 'all':\n",
        "        for desc, filename in download_options.values():\n",
        "            try:\n",
        "                files.download(filename)\n",
        "                print(f\"‚úÖ T√©l√©charg√©: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Erreur t√©l√©chargement {filename}: {e}\")\n",
        "    elif choice in download_options:\n",
        "        desc, filename = download_options[choice]\n",
        "        try:\n",
        "            files.download(filename)\n",
        "            print(f\"‚úÖ T√©l√©charg√©: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur t√©l√©chargement: {e}\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Aucun t√©l√©chargement s√©lectionn√©\")\n",
        "    \n",
        "    return {\n",
        "        'zip_file': zip_filename,\n",
        "        'summary_file': summary_filename,\n",
        "        'report_file': report_filename,\n",
        "        'json_file': json_filename\n",
        "    }\n",
        "\n",
        "def generate_detailed_text_report(batch_results, analysis_summary, filename, timestamp):\n",
        "    \"\"\"\n",
        "    G√©n√©rer un rapport textuel d√©taill√©\n",
        "    Generate detailed text report\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"=\" * 80 + \"\\n\")\n",
        "        f.write(\"RAPPORT D'ANALYSE DE DATASET PERSONNALIS√â TORCHXRAYVISION\\n\")\n",
        "        f.write(\"CUSTOM DATASET ANALYSIS REPORT - TORCHXRAYVISION\\n\")\n",
        "        f.write(\"=\" * 80 + \"\\n\")\n",
        "        f.write(f\"Date d'analyse: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "        \n",
        "        # R√©sum√© ex√©cutif\n",
        "        f.write(\"R√âSUM√â EX√âCUTIF\\n\")\n",
        "        f.write(\"-\" * 50 + \"\\n\")\n",
        "        \n",
        "        total_images = len(batch_results[list(batch_results.keys())[0]])\n",
        "        f.write(f\"Nombre total d'images analys√©es: {total_images}\\n\")\n",
        "        f.write(f\"Mod√®les utilis√©s: {', '.join(batch_results.keys())}\\n\")\n",
        "        \n",
        "        for summary in analysis_summary:\n",
        "            f.write(f\"\\nMod√®le {summary['model']}:\\n\")\n",
        "            f.write(f\"  - Images avec d√©tections: {summary['images_with_detections']}/{summary['images_analyzed']}\\n\")\n",
        "            f.write(f\"  - D√©tections moyennes par image: {summary['avg_detections_per_image']:.2f}\\n\")\n",
        "            f.write(f\"  - Maximum d√©tections (une image): {summary['max_detections_single_image']}\\n\")\n",
        "        \n",
        "        # D√©tails par mod√®le\n",
        "        for model_name, results in batch_results.items():\n",
        "            f.write(f\"\\n\\n{'='*60}\\n\")\n",
        "            f.write(f\"ANALYSE D√âTAILL√âE - MOD√àLE {model_name}\\n\")\n",
        "            f.write(f\"{'='*60}\\n\")\n",
        "            \n",
        "            # Top pathologies\n",
        "            pathologies = [col for col in results[0].keys() \n",
        "                          if col not in ['image_id', 'filename', 'model', 'total_detections', \n",
        "                                       'max_probability', 'avg_probability'] \n",
        "                          and not col.endswith('_detected')]\n",
        "            \n",
        "            pathology_counts = {}\n",
        "            for pathology in pathologies:\n",
        "                count = sum(1 for result in results if result.get(f'{pathology}_detected', False))\n",
        "                pathology_counts[pathology] = count\n",
        "            \n",
        "            sorted_paths = sorted(pathology_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "            \n",
        "            f.write(\"\\nTOP 15 PATHOLOGIES D√âTECT√âES:\\n\")\n",
        "            for i, (pathology, count) in enumerate(sorted_paths[:15]):\n",
        "                percentage = (count / len(results)) * 100\n",
        "                f.write(f\"  {i+1:2d}. {pathology:30s}: {count:3d} ({percentage:5.1f}%)\\n\")\n",
        "            \n",
        "            # Images les plus probl√©matiques\n",
        "            f.write(\"\\nIMAGES LES PLUS PROBL√âMATIQUES:\\n\")\n",
        "            sorted_images = sorted(results, key=lambda x: x['total_detections'], reverse=True)\n",
        "            \n",
        "            for i, result in enumerate(sorted_images[:10]):\n",
        "                f.write(f\"  {i+1:2d}. {result['filename']:40s}: {result['total_detections']} d√©tections\\n\")\n",
        "        \n",
        "        # Recommandations\n",
        "        f.write(f\"\\n\\n{'='*60}\\n\")\n",
        "        f.write(\"RECOMMANDATIONS ET ACTIONS SUGG√âR√âES\\n\")\n",
        "        f.write(f\"{'='*60}\\n\")\n",
        "        \n",
        "        main_results = batch_results[list(batch_results.keys())[0]]\n",
        "        high_detection_images = [r for r in main_results if r['total_detections'] > 5]\n",
        "        \n",
        "        if high_detection_images:\n",
        "            f.write(f\"\\nATTENTION: {len(high_detection_images)} image(s) avec 6+ pathologies d√©tect√©es.\\n\")\n",
        "            f.write(\"Recommandation: R√©vision clinique prioritaire.\\n\")\n",
        "        \n",
        "        normal_images = [r for r in main_results if r['total_detections'] == 0]\n",
        "        f.write(f\"\\nImages normales: {len(normal_images)}/{len(main_results)} ({len(normal_images)/len(main_results)*100:.1f}%)\\n\")\n",
        "        \n",
        "        f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "        f.write(\"Fin du rapport\\n\")\n",
        "        f.write(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# Effectuer l'export si nous avons des r√©sultats\n",
        "if 'batch_results' in locals() and batch_results:\n",
        "    export_files = export_dataset_results(batch_results, all_filenames, processed_images, batch_summary)\n",
        "    \n",
        "    print(\"\\nüéâ EXPORT TERMIN√â AVEC SUCC√àS!\")\n",
        "    print(\"üéâ EXPORT COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"\\nüìã Fichiers g√©n√©r√©s:\")\n",
        "    for desc, filename in export_files.items():\n",
        "        print(f\"   ‚Ä¢ {desc}: {filename}\")\n",
        "else:\n",
        "    print(\"‚ùå Aucun r√©sultat √† exporter\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tutorial_summary"
      },
      "source": [
        "## üéâ R√©sum√© du Tutorial / Tutorial Summary\n",
        "\n",
        "### Ce que vous avez accompli / What you accomplished:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "# G√©n√©ration du r√©sum√© final du tutorial\n",
        "print(\"üéâ\" * 50)\n",
        "print(\"TUTORIAL 6 TERMIN√â AVEC SUCC√àS!\")\n",
        "print(\"TUTORIAL 6 COMPLETED SUCCESSFULLY!\")\n",
        "print(\"üéâ\" * 50)\n",
        "\n",
        "print(\"\\nüìä R√âSUM√â DE VOS ACCOMPLISSEMENTS:\")\n",
        "print(\"üìä SUMMARY OF YOUR ACHIEVEMENTS:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if 'all_images' in locals() and all_images:\n",
        "    print(f\"\\n‚úÖ CHARGEMENT DE DONN√âES:\")\n",
        "    print(f\"   ‚Ä¢ {len(all_images)} image(s) charg√©e(s) avec succ√®s\")\n",
        "    \n",
        "    # D√©tails par source\n",
        "    source_summary = {}\n",
        "    if 'all_sources' in locals():\n",
        "        for source in set(all_sources):\n",
        "            count = all_sources.count(source)\n",
        "            source_summary[source] = count\n",
        "        \n",
        "        for source, count in source_summary.items():\n",
        "            print(f\"   ‚Ä¢ {source}: {count} image(s)\")\n",
        "\n",
        "if 'processed_images' in locals() and processed_images:\n",
        "    print(f\"\\n‚úÖ PR√âPARATION DES DONN√âES:\")\n",
        "    print(f\"   ‚Ä¢ {len(processed_images)} image(s) pr√©par√©e(s) pour l'analyse\")\n",
        "    print(f\"   ‚Ä¢ Redimensionnement vers 224x224 pixels\")\n",
        "    print(f\"   ‚Ä¢ Normalisation Z-score appliqu√©e\")\n",
        "    print(f\"   ‚Ä¢ Conversion en tenseurs PyTorch r√©ussie\")\n",
        "\n",
        "if 'models' in locals() and models:\n",
        "    print(f\"\\n‚úÖ MOD√àLES UTILIS√âS:\")\n",
        "    for model_name, model_data in model_info.items():\n",
        "        if model_name in models:\n",
        "            print(f\"   ‚Ä¢ {model_data['name']}: {len(model_data['pathologies'])} pathologies\")\n",
        "\n",
        "if 'batch_results' in locals() and batch_results:\n",
        "    print(f\"\\n‚úÖ ANALYSE R√âALIS√âE:\")\n",
        "    \n",
        "    main_model_key = list(batch_results.keys())[0]\n",
        "    main_results = batch_results[main_model_key]\n",
        "    \n",
        "    total_detections = sum(result['total_detections'] for result in main_results)\n",
        "    images_with_findings = sum(1 for result in main_results if result['total_detections'] > 0)\n",
        "    max_detections = max(result['total_detections'] for result in main_results)\n",
        "    \n",
        "    print(f\"   ‚Ä¢ {len(main_results)} image(s) analys√©e(s) avec IA\")\n",
        "    print(f\"   ‚Ä¢ {total_detections} pathologies d√©tect√©es au total\")\n",
        "    print(f\"   ‚Ä¢ {images_with_findings}/{len(main_results)} image(s) avec anomalies\")\n",
        "    print(f\"   ‚Ä¢ Maximum {max_detections} pathologies sur une image\")\n",
        "\n",
        "if 'export_files' in locals() and export_files:\n",
        "    print(f\"\\n‚úÖ R√âSULTATS EXPORT√âS:\")\n",
        "    print(f\"   ‚Ä¢ Rapport CSV pour analyse statistique\")\n",
        "    print(f\"   ‚Ä¢ Fichier JSON pour int√©gration logicielle\")\n",
        "    print(f\"   ‚Ä¢ Rapport textuel d√©taill√©\")\n",
        "    print(f\"   ‚Ä¢ Package ZIP complet\")\n",
        "\n",
        "print(f\"\\nüéì COMP√âTENCES ACQUISES:\")\n",
        "print(f\"   üîπ Chargement facile de datasets personnalis√©s\")\n",
        "print(f\"   üîπ Pr√©paration automatique d'images m√©dicales\")\n",
        "print(f\"   üîπ Analyse en lot avec TorchXRayVision\")\n",
        "print(f\"   üîπ Comparaison multi-mod√®les\")\n",
        "print(f\"   üîπ Visualisation et interpr√©tation des r√©sultats\")\n",
        "print(f\"   üîπ Export pour usage clinique et recherche\")\n",
        "\n",
        "print(f\"\\nüöÄ APPLICATIONS PRATIQUES:\")\n",
        "print(f\"   üè• Analyse de cas cliniques personnels\")\n",
        "print(f\"   üìä √âtudes de recherche m√©dicale\")\n",
        "print(f\"   üìö Projets √©tudiants en m√©decine\")\n",
        "print(f\"   üî¨ Validation d'IA sur populations locales\")\n",
        "print(f\"   üìà Audit qualit√© radiologique\")\n",
        "\n",
        "print(f\"\\nüí° PROCHAINES √âTAPES SUGG√âR√âES:\")\n",
        "print(f\"   üìñ R√©vision des tutoriels 1-5 pour approfondir\")\n",
        "print(f\"   üîÑ Test avec vos propres donn√©es m√©dicales\")\n",
        "print(f\"   üìä Analyse statistique avanc√©e des r√©sultats\")\n",
        "print(f\"   ü§ù Collaboration avec radiologues pour validation\")\n",
        "print(f\"   üìù R√©daction d'articles de recherche\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è RAPPELS IMPORTANTS:\")\n",
        "print(f\"   üîê Respecter la confidentialit√© des donn√©es patients\")\n",
        "print(f\"   üè• Toujours valider avec expertise m√©dicale\")\n",
        "print(f\"   üìã L'IA assiste mais ne remplace pas le m√©decin\")\n",
        "print(f\"   üìä Documenter m√©thodes et limitations\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"üôè F√âLICITATIONS! Vous ma√Ætrisez maintenant l'int√©gration\")\n",
        "print(f\"   de datasets personnalis√©s avec TorchXRayVision!\")\n",
        "print(f\"\")\n",
        "print(f\"üåü CONGRATULATIONS! You now master custom dataset\")\n",
        "print(f\"   integration with TorchXRayVision!\")\n",
        "print(f\"=\"*70)\n",
        "\n",
        "# Suggestion pour continuer\n",
        "print(f\"\\nüîÑ POUR CONTINUER VOTRE APPRENTISSAGE:\")\n",
        "print(f\"   ‚Ä¢ Testez avec diff√©rents types d'images\")\n",
        "print(f\"   ‚Ä¢ Comparez les mod√®les sur vos donn√©es\")\n",
        "print(f\"   ‚Ä¢ Int√©grez dans vos workflows de recherche\")\n",
        "print(f\"   ‚Ä¢ Partagez vos d√©couvertes avec la communaut√©\")\n",
        "\n",
        "print(f\"\\nüéØ Vous √™tes maintenant pr√™t(e) pour des applications\")\n",
        "print(f\"   concr√®tes en recherche m√©dicale et pratique clinique!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}