{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/maclandrol/cours-ia-med/blob/master/10_Clinical_Deployment_Ethics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-info"
   },
   "source": [
    "**Enseignant:** Emmanuel Noutahi, PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title-section"
   },
   "source": [
    "# Tutorial 10: Clinical Deployment Ethics and Responsible AI\n",
    "\n",
    "## Medical Context\n",
    "\n",
    "### For Medical Students\n",
    "Understanding the ethical dimensions of AI in medicine is essential for:\n",
    "- **Future practice**: Preparing for an AI-integrated healthcare environment\n",
    "- **Patient advocacy**: Ensuring AI serves patient interests and wellbeing\n",
    "- **Professional responsibility**: Understanding duties and obligations in AI-assisted care\n",
    "- **Research ethics**: Conducting responsible AI research with proper ethical oversight\n",
    "\n",
    "### For Practitioners\n",
    "- **Clinical implementation**: Deploying AI systems safely and ethically\n",
    "- **Patient trust**: Maintaining transparency and informed consent\n",
    "- **Professional liability**: Understanding legal and ethical responsibilities\n",
    "- **Quality assurance**: Ensuring AI systems meet ethical and safety standards\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "1. **Identify** key ethical principles governing medical AI deployment\n",
    "2. **Assess** bias, fairness, and equity considerations in AI systems\n",
    "3. **Implement** transparency and explainability requirements\n",
    "4. **Navigate** regulatory frameworks and compliance obligations\n",
    "5. **Design** ethical AI governance structures\n",
    "6. **Apply** ethical frameworks to real clinical scenarios\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial integrates concepts from all previous tutorials. You should understand:\n",
    "- Basic AI model concepts and capabilities\n",
    "- Clinical workflow integration\n",
    "- Model performance and limitations\n",
    "- Implementation planning and deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## Setup and Ethical Framework Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-libraries"
   },
   "outputs": [],
   "source": [
    "# Import libraries for ethical analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display configuration\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Ethical Framework Analysis Tools Loaded\")\n",
    "print(\"Ready for responsible AI deployment analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ethical-principles"
   },
   "source": [
    "## Foundational Ethical Principles in Medical AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define-ethical-principles"
   },
   "outputs": [],
   "source": [
    "class MedicalAIEthicsFramework:\n",
    "    \"\"\"\n",
    "    Comprehensive framework for medical AI ethics based on established principles\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.principles = {\n",
    "            'beneficence': {\n",
    "                'definition': 'AI systems should promote patient wellbeing and clinical benefit',\n",
    "                'requirements': [\n",
    "                    'Demonstrable clinical benefit',\n",
    "                    'Evidence-based performance validation',\n",
    "                    'Continuous monitoring for positive outcomes',\n",
    "                    'Optimization for patient care improvement'\n",
    "                ],\n",
    "                'practical_applications': [\n",
    "                    'Rigorous clinical testing before deployment',\n",
    "                    'Outcome measurement and tracking',\n",
    "                    'Regular performance reviews',\n",
    "                    'Patient-centered design principles'\n",
    "                ],\n",
    "                'challenges': [\n",
    "                    'Defining measurable clinical benefit',\n",
    "                    'Balancing innovation with proven benefit',\n",
    "                    'Long-term vs short-term outcomes'\n",
    "                ]\n",
    "            },\n",
    "            'non_maleficence': {\n",
    "                'definition': 'AI systems must not cause harm to patients or healthcare providers',\n",
    "                'requirements': [\n",
    "                    'Comprehensive risk assessment and mitigation',\n",
    "                    'Fail-safe mechanisms and human oversight',\n",
    "                    'Protection against misuse and errors',\n",
    "                    'Ongoing safety monitoring'\n",
    "                ],\n",
    "                'practical_applications': [\n",
    "                    'Multi-layered safety systems',\n",
    "                    'Alert fatigue prevention',\n",
    "                    'Error detection and correction',\n",
    "                    'User training and competency verification'\n",
    "                ],\n",
    "                'challenges': [\n",
    "                    'Identifying all potential harms',\n",
    "                    'Balancing automation with human control',\n",
    "                    'Managing false positives and negatives'\n",
    "                ]\n",
    "            },\n",
    "            'autonomy': {\n",
    "                'definition': 'Patients have the right to make informed decisions about AI-assisted care',\n",
    "                'requirements': [\n",
    "                    'Informed consent for AI use',\n",
    "                    'Transparency in AI decision-making',\n",
    "                    'Right to opt-out or request human review',\n",
    "                    'Clear communication about AI limitations'\n",
    "                ],\n",
    "                'practical_applications': [\n",
    "                    'AI disclosure in consent forms',\n",
    "                    'Patient education materials',\n",
    "                    'Opt-out mechanisms',\n",
    "                    'Second opinion procedures'\n",
    "                ],\n",
    "                'challenges': [\n",
    "                    'Complexity of AI explanations',\n",
    "                    'Emergency situations with limited time',\n",
    "                    'Varying patient understanding levels'\n",
    "                ]\n",
    "            },\n",
    "            'justice': {\n",
    "                'definition': 'AI benefits and risks should be fairly distributed across all patient populations',\n",
    "                'requirements': [\n",
    "                    'Equitable access to AI-enhanced care',\n",
    "                    'Fair representation in training data',\n",
    "                    'Bias detection and mitigation',\n",
    "                    'Protection of vulnerable populations'\n",
    "                ],\n",
    "                'practical_applications': [\n",
    "                    'Diverse training datasets',\n",
    "                    'Bias testing across demographics',\n",
    "                    'Accessible AI deployment',\n",
    "                    'Health equity monitoring'\n",
    "                ],\n",
    "                'challenges': [\n",
    "                    'Data representation gaps',\n",
    "                    'Resource allocation decisions',\n",
    "                    'Defining fairness across contexts'\n",
    "                ]\n",
    "            },\n",
    "            'transparency': {\n",
    "                'definition': 'AI systems should be understandable and accountable to users and patients',\n",
    "                'requirements': [\n",
    "                    'Explainable AI outputs',\n",
    "                    'Clear documentation of capabilities and limitations',\n",
    "                    'Audit trails for decisions',\n",
    "                    'Open validation processes'\n",
    "                ],\n",
    "                'practical_applications': [\n",
    "                    'Visual explanation tools',\n",
    "                    'Model cards and documentation',\n",
    "                    'Decision logging systems',\n",
    "                    'Public performance reporting'\n",
    "                ],\n",
    "                'challenges': [\n",
    "                    'Technical complexity vs understandability',\n",
    "                    'Proprietary algorithm protection',\n",
    "                    'Information overload'\n",
    "                ]\n",
    "            },\n",
    "            'accountability': {\n",
    "                'definition': 'Clear responsibility structures for AI-assisted medical decisions',\n",
    "                'requirements': [\n",
    "                    'Defined roles and responsibilities',\n",
    "                    'Professional oversight mechanisms',\n",
    "                    'Error reporting and learning systems',\n",
    "                    'Liability frameworks'\n",
    "                ],\n",
    "                'practical_applications': [\n",
    "                    'Governance committees',\n",
    "                    'Quality assurance programs',\n",
    "                    'Incident reporting systems',\n",
    "                    'Professional certification requirements'\n",
    "                ],\n",
    "                'challenges': [\n",
    "                    'Complex decision-making chains',\n",
    "                    'Shared responsibility models',\n",
    "                    'Evolving legal frameworks'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def display_principles_overview(self):\n",
    "        \"\"\"Display comprehensive overview of ethical principles\"\"\"\n",
    "        print(\"MEDICAL AI ETHICAL PRINCIPLES FRAMEWORK\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for principle, details in self.principles.items():\n",
    "            print(f\"\\n{principle.upper().replace('_', ' ')}\")\n",
    "            print(\"-\" * len(principle))\n",
    "            print(f\"Definition: {details['definition']}\")\n",
    "            \n",
    "            print(f\"\\nKey Requirements:\")\n",
    "            for req in details['requirements']:\n",
    "                print(f\"   • {req}\")\n",
    "            \n",
    "            print(f\"\\nPractical Applications:\")\n",
    "            for app in details['practical_applications']:\n",
    "                print(f\"   • {app}\")\n",
    "            \n",
    "            print(f\"\\nKey Challenges:\")\n",
    "            for challenge in details['challenges']:\n",
    "                print(f\"   • {challenge}\")\n",
    "            \n",
    "            print()\n",
    "    \n",
    "    def create_principles_visualization(self):\n",
    "        \"\"\"Create interactive visualization of ethical principles\"\"\"\n",
    "        # Prepare data for radar chart\n",
    "        principles_list = list(self.principles.keys())\n",
    "        \n",
    "        # Create metrics for visualization (example complexity scores)\n",
    "        implementation_complexity = [3, 4, 2, 5, 4, 3]  # 1-5 scale\n",
    "        regulatory_importance = [5, 5, 4, 3, 3, 4]      # 1-5 scale\n",
    "        patient_impact = [4, 5, 5, 4, 3, 3]             # 1-5 scale\n",
    "        \n",
    "        # Create comprehensive visualization\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Implementation Complexity', 'Regulatory Importance', \n",
    "                          'Patient Impact', 'Principle Relationships'),\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "                   [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "        )\n",
    "        \n",
    "        # Implementation complexity\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=[p.replace('_', ' ').title() for p in principles_list], \n",
    "                   y=implementation_complexity,\n",
    "                   name=\"Complexity\",\n",
    "                   marker_color='lightblue'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Regulatory importance\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=[p.replace('_', ' ').title() for p in principles_list], \n",
    "                   y=regulatory_importance,\n",
    "                   name=\"Importance\",\n",
    "                   marker_color='lightcoral'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Patient impact\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=[p.replace('_', ' ').title() for p in principles_list], \n",
    "                   y=patient_impact,\n",
    "                   name=\"Impact\",\n",
    "                   marker_color='lightgreen'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Principle relationships scatter\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=implementation_complexity, y=patient_impact,\n",
    "                      mode='markers+text',\n",
    "                      text=[p.replace('_', ' ').title() for p in principles_list],\n",
    "                      textposition=\"top center\",\n",
    "                      marker=dict(size=15, color=regulatory_importance, \n",
    "                                colorscale='Viridis', showscale=True),\n",
    "                      name=\"Relationships\"),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title_text=\"Medical AI Ethics Principles Analysis\",\n",
    "            title_x=0.5,\n",
    "            showlegend=False,\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Summary table\n",
    "        summary_data = {\n",
    "            'Principle': [p.replace('_', ' ').title() for p in principles_list],\n",
    "            'Implementation Complexity (1-5)': implementation_complexity,\n",
    "            'Regulatory Importance (1-5)': regulatory_importance,\n",
    "            'Patient Impact (1-5)': patient_impact,\n",
    "            'Priority Level': ['High' if sum(scores) >= 12 else 'Medium' if sum(scores) >= 9 else 'Standard' \n",
    "                             for scores in zip(implementation_complexity, regulatory_importance, patient_impact)]\n",
    "        }\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        print(\"\\nETHICAL PRINCIPLES PRIORITY MATRIX\")\n",
    "        print(\"=\" * 40)\n",
    "        print(summary_df.to_string(index=False))\n",
    "        \n",
    "        return summary_df\n",
    "\n",
    "# Initialize ethics framework\n",
    "ethics_framework = MedicalAIEthicsFramework()\n",
    "print(\"Medical AI Ethics Framework initialized\")\n",
    "print(f\"Core principles loaded: {len(ethics_framework.principles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "display-principles"
   },
   "source": [
    "## Core Ethical Principles Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-principles"
   },
   "outputs": [],
   "source": [
    "# Display comprehensive principles overview\n",
    "ethics_framework.display_principles_overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-principles"
   },
   "outputs": [],
   "source": [
    "# Create interactive visualization\n",
    "principles_summary = ethics_framework.create_principles_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "regulatory-landscape"
   },
   "source": [
    "## Regulatory Landscape and Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "regulatory-framework"
   },
   "outputs": [],
   "source": [
    "class RegulatoryComplianceFramework:\n",
    "    \"\"\"\n",
    "    Comprehensive framework for navigating regulatory requirements for medical AI\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.regulatory_bodies = {\n",
    "            'FDA': {\n",
    "                'region': 'United States',\n",
    "                'key_regulations': [\n",
    "                    'FDA Software as Medical Device (SaMD) Guidance',\n",
    "                    'AI/ML-Based Software as Medical Device Action Plan',\n",
    "                    'Good Machine Learning Practice (GMLP)',\n",
    "                    'Clinical Evaluation of AI/ML-based devices'\n",
    "                ],\n",
    "                'classification_system': {\n",
    "                    'Class I': 'Low risk, general controls',\n",
    "                    'Class II': 'Moderate risk, special controls + 510(k)',\n",
    "                    'Class III': 'High risk, PMA required'\n",
    "                },\n",
    "                'recent_updates': [\n",
    "                    '2021: AI/ML Action Plan update',\n",
    "                    '2022: Digital Health Software Precertification Program',\n",
    "                    '2023: Real-World Performance monitoring guidance'\n",
    "                ]\n",
    "            },\n",
    "            'EU_MDR': {\n",
    "                'region': 'European Union',\n",
    "                'key_regulations': [\n",
    "                    'Medical Device Regulation (MDR) 2017/745',\n",
    "                    'AI Act (proposed)',\n",
    "                    'GDPR for data protection',\n",
    "                    'Clinical Evaluation and Post-Market Surveillance'\n",
    "                ],\n",
    "                'classification_system': {\n",
    "                    'Class I': 'Low risk, CE marking',\n",
    "                    'Class IIa': 'Low-medium risk, notified body',\n",
    "                    'Class IIb': 'Medium-high risk, notified body',\n",
    "                    'Class III': 'High risk, notified body + clinical data'\n",
    "                },\n",
    "                'recent_updates': [\n",
    "                    '2021: MDR full implementation',\n",
    "                    '2022: AI Act proposal',\n",
    "                    '2023: MDCG guidance on AI in medical devices'\n",
    "                ]\n",
    "            },\n",
    "            'Health_Canada': {\n",
    "                'region': 'Canada',\n",
    "                'key_regulations': [\n",
    "                    'Medical Device License (MDL)',\n",
    "                    'Software as Medical Device guidance',\n",
    "                    'Quality System Regulation',\n",
    "                    'Clinical Evidence requirements'\n",
    "                ],\n",
    "                'classification_system': {\n",
    "                    'Class I': 'Low risk',\n",
    "                    'Class II': 'Medium risk',\n",
    "                    'Class III': 'Medium-high risk',\n",
    "                    'Class IV': 'High risk'\n",
    "                },\n",
    "                'recent_updates': [\n",
    "                    '2021: Digital health technologies pathway',\n",
    "                    '2022: Agile licensing pilot program',\n",
    "                    '2023: AI/ML medical device guidance'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.compliance_requirements = {\n",
    "            'pre_market': {\n",
    "                'clinical_validation': [\n",
    "                    'Clinical performance studies',\n",
    "                    'Analytical performance validation',\n",
    "                    'Usability and human factors studies',\n",
    "                    'Risk management documentation'\n",
    "                ],\n",
    "                'technical_documentation': [\n",
    "                    'Software lifecycle documentation',\n",
    "                    'Algorithm training and validation',\n",
    "                    'Cybersecurity assessment',\n",
    "                    'Quality management system'\n",
    "                ],\n",
    "                'regulatory_submission': [\n",
    "                    'Regulatory pathway determination',\n",
    "                    'Classification and predicate analysis',\n",
    "                    'Clinical evidence compilation',\n",
    "                    'Regulatory application submission'\n",
    "                ]\n",
    "            },\n",
    "            'post_market': {\n",
    "                'surveillance': [\n",
    "                    'Real-world performance monitoring',\n",
    "                    'Adverse event reporting',\n",
    "                    'Periodic safety updates',\n",
    "                    'Field safety corrective actions'\n",
    "                ],\n",
    "                'quality_management': [\n",
    "                    'Quality management system maintenance',\n",
    "                    'Change control procedures',\n",
    "                    'Software updates and versioning',\n",
    "                    'Supplier oversight'\n",
    "                ],\n",
    "                'compliance_monitoring': [\n",
    "                    'Regulatory inspection readiness',\n",
    "                    'Compliance auditing',\n",
    "                    'Documentation maintenance',\n",
    "                    'Training and competency'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def display_regulatory_overview(self):\n",
    "        \"\"\"Display comprehensive regulatory overview\"\"\"\n",
    "        print(\"REGULATORY COMPLIANCE FRAMEWORK\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for body, details in self.regulatory_bodies.items():\n",
    "            print(f\"\\n{body} - {details['region']}\")\n",
    "            print(\"-\" * (len(body) + len(details['region']) + 3))\n",
    "            \n",
    "            print(\"Key Regulations:\")\n",
    "            for reg in details['key_regulations']:\n",
    "                print(f\"   • {reg}\")\n",
    "            \n",
    "            print(\"\\nClassification System:\")\n",
    "            for class_level, description in details['classification_system'].items():\n",
    "                print(f\"   • {class_level}: {description}\")\n",
    "            \n",
    "            print(\"\\nRecent Updates:\")\n",
    "            for update in details['recent_updates']:\n",
    "                print(f\"   • {update}\")\n",
    "            print()\n",
    "    \n",
    "    def create_compliance_checklist(self, regulatory_region='FDA'):\n",
    "        \"\"\"Create customized compliance checklist\"\"\"\n",
    "        print(f\"COMPLIANCE CHECKLIST FOR {regulatory_region}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        checklist = {\n",
    "            'Pre-Market Phase': [\n",
    "                \"☐ Device classification determined\",\n",
    "                \"☐ Regulatory pathway identified\",\n",
    "                \"☐ Predicate device analysis completed\",\n",
    "                \"☐ Clinical validation plan developed\",\n",
    "                \"☐ Risk management file established\",\n",
    "                \"☐ Quality management system implemented\",\n",
    "                \"☐ Software lifecycle processes defined\",\n",
    "                \"☐ Cybersecurity assessment completed\",\n",
    "                \"☐ Usability validation conducted\",\n",
    "                \"☐ Clinical performance studies executed\",\n",
    "                \"☐ Regulatory submission prepared\",\n",
    "                \"☐ Pre-submission meetings conducted\"\n",
    "            ],\n",
    "            'Market Authorization': [\n",
    "                \"☐ Regulatory application submitted\",\n",
    "                \"☐ Response to regulatory questions\",\n",
    "                \"☐ Additional studies if required\",\n",
    "                \"☐ Final regulatory review\",\n",
    "                \"☐ Market authorization received\",\n",
    "                \"☐ Labeling and instructions approved\"\n",
    "            ],\n",
    "            'Post-Market Phase': [\n",
    "                \"☐ Post-market surveillance system active\",\n",
    "                \"☐ Adverse event reporting procedures\",\n",
    "                \"☐ Periodic safety update reports\",\n",
    "                \"☐ Real-world performance monitoring\",\n",
    "                \"☐ Change control procedures implemented\",\n",
    "                \"☐ Software update management\",\n",
    "                \"☐ Quality management system maintenance\",\n",
    "                \"☐ Regulatory inspection readiness\",\n",
    "                \"☐ Continuous compliance monitoring\",\n",
    "                \"☐ Training and competency programs\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for phase, items in checklist.items():\n",
    "            print(f\"\\n{phase}:\")\n",
    "            for item in items:\n",
    "                print(f\"   {item}\")\n",
    "        \n",
    "        return checklist\n",
    "    \n",
    "    def assess_regulatory_risk(self, device_characteristics):\n",
    "        \"\"\"Assess regulatory risk based on device characteristics\"\"\"\n",
    "        risk_factors = {\n",
    "            'clinical_impact': device_characteristics.get('clinical_impact', 'medium'),\n",
    "            'automation_level': device_characteristics.get('automation_level', 'medium'),\n",
    "            'patient_population': device_characteristics.get('patient_population', 'general'),\n",
    "            'data_sensitivity': device_characteristics.get('data_sensitivity', 'medium'),\n",
    "            'algorithm_complexity': device_characteristics.get('algorithm_complexity', 'medium')\n",
    "        }\n",
    "        \n",
    "        risk_scores = {\n",
    "            'low': 1, 'medium': 2, 'high': 3\n",
    "        }\n",
    "        \n",
    "        total_score = sum(risk_scores.get(factor, 2) for factor in risk_factors.values())\n",
    "        max_score = len(risk_factors) * 3\n",
    "        \n",
    "        risk_level = 'Low' if total_score <= max_score * 0.4 else 'Medium' if total_score <= max_score * 0.7 else 'High'\n",
    "        \n",
    "        print(\"REGULATORY RISK ASSESSMENT\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Overall Risk Level: {risk_level}\")\n",
    "        print(f\"Risk Score: {total_score}/{max_score}\")\n",
    "        print(\"\\nRisk Factors:\")\n",
    "        for factor, level in risk_factors.items():\n",
    "            print(f\"   • {factor.replace('_', ' ').title()}: {level}\")\n",
    "        \n",
    "        # Regulatory pathway recommendation\n",
    "        if risk_level == 'Low':\n",
    "            pathway = 'Likely Class I or IIa - General controls or 510(k)'\n",
    "        elif risk_level == 'Medium':\n",
    "            pathway = 'Likely Class II - 510(k) with special controls'\n",
    "        else:\n",
    "            pathway = 'Likely Class III - Pre-market approval (PMA) required'\n",
    "        \n",
    "        print(f\"\\nRecommended Pathway: {pathway}\")\n",
    "        \n",
    "        return {\n",
    "            'risk_level': risk_level,\n",
    "            'risk_score': total_score,\n",
    "            'pathway_recommendation': pathway,\n",
    "            'risk_factors': risk_factors\n",
    "        }\n",
    "\n",
    "# Initialize regulatory framework\n",
    "regulatory_framework = RegulatoryComplianceFramework()\n",
    "print(\"Regulatory Compliance Framework initialized\")\n",
    "print(f\"Regulatory bodies covered: {list(regulatory_framework.regulatory_bodies.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display-regulatory"
   },
   "outputs": [],
   "source": [
    "# Display regulatory overview\n",
    "regulatory_framework.display_regulatory_overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compliance-checklist"
   },
   "outputs": [],
   "source": [
    "# Create compliance checklist\n",
    "compliance_checklist = regulatory_framework.create_compliance_checklist('FDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "risk-assessment"
   },
   "outputs": [],
   "source": [
    "# Example regulatory risk assessment\n",
    "example_device = {\n",
    "    'clinical_impact': 'high',      # High impact on patient care\n",
    "    'automation_level': 'medium',   # Semi-autonomous with human oversight\n",
    "    'patient_population': 'general', # General adult population\n",
    "    'data_sensitivity': 'high',     # Medical imaging data\n",
    "    'algorithm_complexity': 'high'  # Deep learning model\n",
    "}\n",
    "\n",
    "risk_assessment = regulatory_framework.assess_regulatory_risk(example_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bias-fairness"
   },
   "source": [
    "## Bias Detection and Fairness Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bias-framework"
   },
   "outputs": [],
   "source": [
    "class BiasAndFairnessFramework:\n",
    "    \"\"\"\n",
    "    Framework for detecting and mitigating bias in medical AI systems\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bias_types = {\n",
    "            'historical_bias': {\n",
    "                'definition': 'Bias present in training data due to past discriminatory practices',\n",
    "                'examples': [\n",
    "                    'Underrepresentation of women in cardiac studies',\n",
    "                    'Racial disparities in pain assessment data',\n",
    "                    'Age bias in treatment recommendation data'\n",
    "                ],\n",
    "                'detection_methods': [\n",
    "                    'Demographic representation analysis',\n",
    "                    'Historical outcome pattern review',\n",
    "                    'Data source audit'\n",
    "                ],\n",
    "                'mitigation_strategies': [\n",
    "                    'Data augmentation for underrepresented groups',\n",
    "                    'Bias-aware data collection',\n",
    "                    'Synthetic data generation'\n",
    "                ]\n",
    "            },\n",
    "            'representation_bias': {\n",
    "                'definition': 'Inadequate representation of certain groups in training data',\n",
    "                'examples': [\n",
    "                    'Skin tone bias in dermatology AI',\n",
    "                    'Geographic bias in disease patterns',\n",
    "                    'Socioeconomic status underrepresentation'\n",
    "                ],\n",
    "                'detection_methods': [\n",
    "                    'Stratified performance analysis',\n",
    "                    'Demographic distribution visualization',\n",
    "                    'Intersectional bias assessment'\n",
    "                ],\n",
    "                'mitigation_strategies': [\n",
    "                    'Stratified sampling',\n",
    "                    'Multi-site data collection',\n",
    "                    'Balanced dataset curation'\n",
    "                ]\n",
    "            },\n",
    "            'algorithmic_bias': {\n",
    "                'definition': 'Bias introduced by the algorithm design or optimization process',\n",
    "                'examples': [\n",
    "                    'Optimization for majority group performance',\n",
    "                    'Feature selection bias',\n",
    "                    'Threshold optimization disparities'\n",
    "                ],\n",
    "                'detection_methods': [\n",
    "                    'Fairness metric evaluation',\n",
    "                    'Model explanation analysis',\n",
    "                    'Counterfactual testing'\n",
    "                ],\n",
    "                'mitigation_strategies': [\n",
    "                    'Fairness-constrained optimization',\n",
    "                    'Multi-objective learning',\n",
    "                    'Post-processing calibration'\n",
    "                ]\n",
    "            },\n",
    "            'deployment_bias': {\n",
    "                'definition': 'Bias emerging during real-world deployment and use',\n",
    "                'examples': [\n",
    "                    'Differential access to AI-enhanced care',\n",
    "                    'User interpretation bias',\n",
    "                    'Feedback loop amplification'\n",
    "                ],\n",
    "                'detection_methods': [\n",
    "                    'Real-world performance monitoring',\n",
    "                    'Usage pattern analysis',\n",
    "                    'Outcome disparity tracking'\n",
    "                ],\n",
    "                'mitigation_strategies': [\n",
    "                    'Equitable deployment strategies',\n",
    "                    'User training and calibration',\n",
    "                    'Continuous monitoring systems'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.fairness_metrics = {\n",
    "            'demographic_parity': {\n",
    "                'definition': 'Equal positive prediction rates across groups',\n",
    "                'formula': 'P(Y_hat=1|A=a) = P(Y_hat=1|A=b) for all groups a,b',\n",
    "                'use_case': 'Screening applications where equal access is priority'\n",
    "            },\n",
    "            'equalized_odds': {\n",
    "                'definition': 'Equal true positive and false positive rates across groups',\n",
    "                'formula': 'P(Y_hat=1|Y=y,A=a) = P(Y_hat=1|Y=y,A=b) for all y,a,b',\n",
    "                'use_case': 'Diagnostic applications where accuracy consistency matters'\n",
    "            },\n",
    "            'calibration': {\n",
    "                'definition': 'Equal positive predictive values across groups',\n",
    "                'formula': 'P(Y=1|Y_hat=1,A=a) = P(Y=1|Y_hat=1,A=b) for all a,b',\n",
    "                'use_case': 'Risk prediction where probability interpretation is critical'\n",
    "            },\n",
    "            'individual_fairness': {\n",
    "                'definition': 'Similar individuals receive similar predictions',\n",
    "                'formula': 'd(x_i,x_j) ≤ δ → |f(x_i) - f(x_j)| ≤ ε',\n",
    "                'use_case': 'Personalized medicine applications'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_bias_assessment_protocol(self):\n",
    "        \"\"\"Create comprehensive bias assessment protocol\"\"\"\n",
    "        print(\"BIAS ASSESSMENT PROTOCOL\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        assessment_phases = {\n",
    "            'Phase 1: Data Analysis': [\n",
    "                'Demographic representation analysis',\n",
    "                'Historical bias pattern identification',\n",
    "                'Data quality assessment by subgroup',\n",
    "                'Label quality and consistency evaluation',\n",
    "                'Missing data pattern analysis',\n",
    "                'Data source diversity assessment'\n",
    "            ],\n",
    "            'Phase 2: Model Development': [\n",
    "                'Feature importance analysis by subgroup',\n",
    "                'Model performance stratification',\n",
    "                'Fairness metric evaluation',\n",
    "                'Bias mitigation technique application',\n",
    "                'Cross-validation with fairness constraints',\n",
    "                'Intersectional bias assessment'\n",
    "            ],\n",
    "            'Phase 3: Validation Testing': [\n",
    "                'Independent test set bias evaluation',\n",
    "                'Stress testing with edge cases',\n",
    "                'Counterfactual fairness testing',\n",
    "                'Robustness assessment across demographics',\n",
    "                'External validation on diverse populations',\n",
    "                'Clinical expert bias review'\n",
    "            ],\n",
    "            'Phase 4: Deployment Monitoring': [\n",
    "                'Real-world performance tracking by subgroup',\n",
    "                'Usage pattern bias detection',\n",
    "                'Outcome disparity monitoring',\n",
    "                'Feedback loop bias identification',\n",
    "                'Continuous fairness metric evaluation',\n",
    "                'Bias drift detection and alerting'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for phase, tasks in assessment_phases.items():\n",
    "            print(f\"\\n{phase}:\")\n",
    "            for i, task in enumerate(tasks, 1):\n",
    "                print(f\"   {i}. {task}\")\n",
    "        \n",
    "        return assessment_phases\n",
    "    \n",
    "    def simulate_bias_analysis(self):\n",
    "        \"\"\"Simulate bias analysis with example data\"\"\"\n",
    "        print(\"\\n\\nSIMULATED BIAS ANALYSIS EXAMPLE\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Simulate performance data across different demographics\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        demographics = ['White', 'Black', 'Hispanic', 'Asian', 'Other']\n",
    "        age_groups = ['18-30', '31-50', '51-70', '70+']\n",
    "        genders = ['Male', 'Female', 'Non-binary']\n",
    "        \n",
    "        # Simulate biased performance (some groups perform worse)\n",
    "        base_performance = 0.85\n",
    "        bias_factors = {\n",
    "            'White': 0.02, 'Black': -0.08, 'Hispanic': -0.05, \n",
    "            'Asian': 0.01, 'Other': -0.06\n",
    "        }\n",
    "        \n",
    "        performance_data = []\n",
    "        for demo in demographics:\n",
    "            for age in age_groups:\n",
    "                for gender in genders:\n",
    "                    # Add some random variation\n",
    "                    performance = base_performance + bias_factors[demo] + np.random.normal(0, 0.02)\n",
    "                    performance_data.append({\n",
    "                        'demographic': demo,\n",
    "                        'age_group': age,\n",
    "                        'gender': gender,\n",
    "                        'accuracy': max(0.5, min(1.0, performance)),\n",
    "                        'sensitivity': max(0.5, min(1.0, performance + np.random.normal(0, 0.01))),\n",
    "                        'specificity': max(0.5, min(1.0, performance + np.random.normal(0, 0.01)))\n",
    "                    })\n",
    "        \n",
    "        bias_df = pd.DataFrame(performance_data)\n",
    "        \n",
    "        # Analysis by demographic group\n",
    "        demo_analysis = bias_df.groupby('demographic')[['accuracy', 'sensitivity', 'specificity']].agg(['mean', 'std'])\n",
    "        \n",
    "        print(\"Performance by Demographic Group:\")\n",
    "        print(demo_analysis.round(3))\n",
    "        \n",
    "        # Identify potential bias\n",
    "        max_accuracy = demo_analysis[('accuracy', 'mean')].max()\n",
    "        min_accuracy = demo_analysis[('accuracy', 'mean')].min()\n",
    "        bias_gap = max_accuracy - min_accuracy\n",
    "        \n",
    "        print(f\"\\nBias Analysis Results:\")\n",
    "        print(f\"   Maximum accuracy gap: {bias_gap:.3f}\")\n",
    "        print(f\"   Bias severity: {'High' if bias_gap > 0.1 else 'Medium' if bias_gap > 0.05 else 'Low'}\")\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Performance by demographic\n",
    "        demo_means = bias_df.groupby('demographic')['accuracy'].mean()\n",
    "        axes[0].bar(demo_means.index, demo_means.values, alpha=0.7, color='skyblue')\n",
    "        axes[0].set_title('Model Accuracy by Demographic Group')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].set_ylim(0.7, 0.9)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add bias threshold line\n",
    "        axes[0].axhline(y=demo_means.mean(), color='red', linestyle='--', alpha=0.7, label='Overall Mean')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Performance distribution\n",
    "        bias_df.boxplot(column='accuracy', by='demographic', ax=axes[1])\n",
    "        axes[1].set_title('Accuracy Distribution by Demographic')\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        \n",
    "        # Fairness metrics comparison\n",
    "        fairness_scores = {\n",
    "            'Demographic Parity': 0.73,  # Example scores\n",
    "            'Equalized Odds': 0.68,\n",
    "            'Calibration': 0.81,\n",
    "            'Individual Fairness': 0.75\n",
    "        }\n",
    "        \n",
    "        metrics = list(fairness_scores.keys())\n",
    "        scores = list(fairness_scores.values())\n",
    "        colors = ['red' if s < 0.7 else 'orange' if s < 0.8 else 'green' for s in scores]\n",
    "        \n",
    "        axes[2].bar(metrics, scores, color=colors, alpha=0.7)\n",
    "        axes[2].set_title('Fairness Metrics Evaluation')\n",
    "        axes[2].set_ylabel('Fairness Score')\n",
    "        axes[2].set_ylim(0, 1)\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "        axes[2].axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='Good Threshold')\n",
    "        axes[2].axhline(y=0.7, color='orange', linestyle='--', alpha=0.7, label='Acceptable Threshold')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Bias and Fairness Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return bias_df, fairness_scores\n",
    "\n",
    "# Initialize bias and fairness framework\n",
    "bias_framework = BiasAndFairnessFramework()\n",
    "print(\"Bias and Fairness Framework initialized\")\n",
    "print(f\"Bias types covered: {len(bias_framework.bias_types)}\")\n",
    "print(f\"Fairness metrics included: {len(bias_framework.fairness_metrics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bias-assessment"
   },
   "outputs": [],
   "source": [
    "# Create bias assessment protocol\n",
    "bias_protocol = bias_framework.create_bias_assessment_protocol()\n",
    "\n",
    "# Run simulated bias analysis\n",
    "bias_data, fairness_metrics = bias_framework.simulate_bias_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "transparency-explainability"
   },
   "source": [
    "## Transparency and Explainability Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explainability-framework"
   },
   "outputs": [],
   "source": [
    "class ExplainabilityFramework:\n",
    "    \"\"\"\n",
    "    Framework for implementing transparency and explainability in medical AI\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.explanation_types = {\n",
    "            'global_explanations': {\n",
    "                'purpose': 'Understand overall model behavior and patterns',\n",
    "                'methods': [\n",
    "                    'Feature importance analysis',\n",
    "                    'Model architecture documentation',\n",
    "                    'Training data characteristics',\n",
    "                    'Performance statistics by subgroup',\n",
    "                    'Decision boundary visualization'\n",
    "                ],\n",
    "                'clinical_use': [\n",
    "                    'Model validation and verification',\n",
    "                    'Clinical guideline alignment',\n",
    "                    'Training and education materials',\n",
    "                    'Regulatory submission documentation'\n",
    "                ],\n",
    "                'target_audience': ['Clinicians', 'Researchers', 'Regulators', 'Quality assurance']\n",
    "            },\n",
    "            'local_explanations': {\n",
    "                'purpose': 'Explain specific predictions for individual cases',\n",
    "                'methods': [\n",
    "                    'LIME (Local Interpretable Model-agnostic Explanations)',\n",
    "                    'SHAP (SHapley Additive exPlanations)',\n",
    "                    'Grad-CAM for medical imaging',\n",
    "                    'Attention mechanisms visualization',\n",
    "                    'Counterfactual explanations'\n",
    "                ],\n",
    "                'clinical_use': [\n",
    "                    'Individual patient decision support',\n",
    "                    'Case review and quality assurance',\n",
    "                    'Medical education and training',\n",
    "                    'Patient communication and consent'\n",
    "                ],\n",
    "                'target_audience': ['Clinicians', 'Patients', 'Medical students']\n",
    "            },\n",
    "            'example_based': {\n",
    "                'purpose': 'Provide similar cases and prototypical examples',\n",
    "                'methods': [\n",
    "                    'Nearest neighbor examples',\n",
    "                    'Prototypical case identification',\n",
    "                    'Contrastive examples',\n",
    "                    'Case-based reasoning',\n",
    "                    'Atlas of representative cases'\n",
    "                ],\n",
    "                'clinical_use': [\n",
    "                    'Clinical pattern recognition',\n",
    "                    'Differential diagnosis support',\n",
    "                    'Medical education resources',\n",
    "                    'Second opinion consultation'\n",
    "                ],\n",
    "                'target_audience': ['Clinicians', 'Medical students', 'Specialists']\n",
    "            },\n",
    "            'uncertainty_quantification': {\n",
    "                'purpose': 'Communicate model confidence and limitations',\n",
    "                'methods': [\n",
    "                    'Confidence intervals',\n",
    "                    'Prediction uncertainty estimation',\n",
    "                    'Out-of-distribution detection',\n",
    "                    'Ensemble disagreement measures',\n",
    "                    'Bayesian uncertainty quantification'\n",
    "                ],\n",
    "                'clinical_use': [\n",
    "                    'Risk stratification',\n",
    "                    'Escalation to human experts',\n",
    "                    'Quality control mechanisms',\n",
    "                    'Clinical decision confidence'\n",
    "                ],\n",
    "                'target_audience': ['Clinicians', 'Quality assurance', 'Risk management']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.transparency_requirements = {\n",
    "            'algorithmic_transparency': [\n",
    "                'Model architecture description',\n",
    "                'Training methodology documentation',\n",
    "                'Hyperparameter specifications',\n",
    "                'Optimization procedures',\n",
    "                'Version control and reproducibility'\n",
    "            ],\n",
    "            'data_transparency': [\n",
    "                'Training data source description',\n",
    "                'Data preprocessing steps',\n",
    "                'Inclusion and exclusion criteria',\n",
    "                'Data quality assessment',\n",
    "                'Bias analysis and mitigation'\n",
    "            ],\n",
    "            'performance_transparency': [\n",
    "                'Validation methodology',\n",
    "                'Performance metrics definition',\n",
    "                'Confidence intervals and statistics',\n",
    "                'Subgroup performance analysis',\n",
    "                'Failure case analysis'\n",
    "            ],\n",
    "            'operational_transparency': [\n",
    "                'Intended use specification',\n",
    "                'Clinical workflow integration',\n",
    "                'User interface design rationale',\n",
    "                'Monitoring and maintenance procedures',\n",
    "                'Update and versioning policies'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def create_explainability_plan(self, use_case, target_users):\n",
    "        \"\"\"Create customized explainability plan\"\"\"\n",
    "        print(f\"EXPLAINABILITY PLAN FOR {use_case.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Target Users: {', '.join(target_users)}\")\n",
    "        print()\n",
    "        \n",
    "        # Customize explanations based on use case and users\n",
    "        explanation_priorities = {\n",
    "            'diagnosis': ['local_explanations', 'uncertainty_quantification', 'example_based'],\n",
    "            'screening': ['global_explanations', 'uncertainty_quantification', 'local_explanations'],\n",
    "            'treatment': ['local_explanations', 'example_based', 'uncertainty_quantification'],\n",
    "            'prognosis': ['uncertainty_quantification', 'local_explanations', 'global_explanations']\n",
    "        }\n",
    "        \n",
    "        user_priorities = {\n",
    "            'clinicians': ['local_explanations', 'uncertainty_quantification'],\n",
    "            'patients': ['example_based', 'local_explanations'],\n",
    "            'researchers': ['global_explanations', 'uncertainty_quantification'],\n",
    "            'regulators': ['global_explanations', 'performance_transparency']\n",
    "        }\n",
    "        \n",
    "        # Determine priority explanations\n",
    "        priority_explanations = set()\n",
    "        if use_case in explanation_priorities:\n",
    "            priority_explanations.update(explanation_priorities[use_case])\n",
    "        \n",
    "        for user in target_users:\n",
    "            if user.lower() in user_priorities:\n",
    "                priority_explanations.update(user_priorities[user.lower()])\n",
    "        \n",
    "        # Generate plan\n",
    "        plan = {\n",
    "            'high_priority': [],\n",
    "            'medium_priority': [],\n",
    "            'low_priority': []\n",
    "        }\n",
    "        \n",
    "        for exp_type, details in self.explanation_types.items():\n",
    "            if exp_type in priority_explanations:\n",
    "                plan['high_priority'].append(exp_type)\n",
    "            elif any(user.lower() in [aud.lower() for aud in details['target_audience']] for user in target_users):\n",
    "                plan['medium_priority'].append(exp_type)\n",
    "            else:\n",
    "                plan['low_priority'].append(exp_type)\n",
    "        \n",
    "        # Display plan\n",
    "        for priority_level, explanations in plan.items():\n",
    "            if explanations:\n",
    "                print(f\"{priority_level.replace('_', ' ').title()}:\")\n",
    "                for exp_type in explanations:\n",
    "                    details = self.explanation_types[exp_type]\n",
    "                    print(f\"\\n• {exp_type.replace('_', ' ').title()}\")\n",
    "                    print(f\"  Purpose: {details['purpose']}\")\n",
    "                    print(f\"  Methods: {', '.join(details['methods'][:3])}\")\n",
    "                    print(f\"  Clinical Use: {', '.join(details['clinical_use'][:2])}\")\n",
    "                print()\n",
    "        \n",
    "        return plan\n",
    "    \n",
    "    def create_model_card(self, model_info):\n",
    "        \"\"\"Generate comprehensive model card for transparency\"\"\"\n",
    "        print(\"MODEL CARD FOR CLINICAL AI SYSTEM\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        model_card_template = {\n",
    "            'Model Details': {\n",
    "                'Model Name': model_info.get('name', 'Medical AI Model'),\n",
    "                'Version': model_info.get('version', '1.0'),\n",
    "                'Model Type': model_info.get('type', 'Deep Learning Classifier'),\n",
    "                'Architecture': model_info.get('architecture', 'DenseNet121'),\n",
    "                'Training Date': model_info.get('training_date', '2023-01-01'),\n",
    "                'Developers': model_info.get('developers', 'Medical AI Research Team')\n",
    "            },\n",
    "            'Intended Use': {\n",
    "                'Primary Use Case': model_info.get('primary_use', 'Medical image analysis'),\n",
    "                'Target Population': model_info.get('target_population', 'Adult patients'),\n",
    "                'Clinical Setting': model_info.get('clinical_setting', 'Radiology departments'),\n",
    "                'User Expertise Required': model_info.get('expertise_required', 'Licensed radiologist'),\n",
    "                'Out of Scope Use': model_info.get('out_of_scope', 'Pediatric populations, emergency diagnosis')\n",
    "            },\n",
    "            'Training Data': {\n",
    "                'Data Sources': model_info.get('data_sources', 'Multi-institutional dataset'),\n",
    "                'Dataset Size': model_info.get('dataset_size', '100,000 images'),\n",
    "                'Demographics': model_info.get('demographics', 'See diversity report'),\n",
    "                'Data Quality': model_info.get('data_quality', 'Expert-validated labels'),\n",
    "                'Bias Assessment': model_info.get('bias_assessment', 'Comprehensive bias analysis conducted')\n",
    "            },\n",
    "            'Performance': {\n",
    "                'Overall Accuracy': model_info.get('accuracy', '0.89 (95% CI: 0.87-0.91)'),\n",
    "                'Sensitivity': model_info.get('sensitivity', '0.91 (95% CI: 0.89-0.93)'),\n",
    "                'Specificity': model_info.get('specificity', '0.87 (95% CI: 0.85-0.89)'),\n",
    "                'Validation Method': model_info.get('validation', 'External validation on 3 independent sites'),\n",
    "                'Subgroup Performance': model_info.get('subgroup_perf', 'See fairness assessment report')\n",
    "            },\n",
    "            'Limitations': {\n",
    "                'Known Limitations': model_info.get('limitations', [\n",
    "                    'Limited to specific imaging protocols',\n",
    "                    'Performance may vary with equipment changes',\n",
    "                    'Requires expert interpretation'\n",
    "                ]),\n",
    "                'Failure Modes': model_info.get('failure_modes', [\n",
    "                    'Poor image quality',\n",
    "                    'Rare pathology presentations',\n",
    "                    'Technical artifacts'\n",
    "                ]),\n",
    "                'Mitigation Strategies': model_info.get('mitigation', [\n",
    "                    'Quality control checks',\n",
    "                    'Human oversight required',\n",
    "                    'Confidence thresholds'\n",
    "                ])\n",
    "            },\n",
    "            'Ethical Considerations': {\n",
    "                'Bias and Fairness': model_info.get('bias_considerations', 'Regular bias monitoring implemented'),\n",
    "                'Privacy Protection': model_info.get('privacy', 'HIPAA compliant, de-identification verified'),\n",
    "                'Informed Consent': model_info.get('consent', 'Patient consent process established'),\n",
    "                'Accountability': model_info.get('accountability', 'Clear physician oversight required')\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for section, details in model_card_template.items():\n",
    "            print(f\"\\n{section}:\")\n",
    "            print(\"-\" * len(section))\n",
    "            if isinstance(details, dict):\n",
    "                for key, value in details.items():\n",
    "                    print(f\"   {key}: {value}\")\n",
    "            elif isinstance(details, list):\n",
    "                for item in details:\n",
    "                    print(f\"   • {item}\")\n",
    "            print()\n",
    "        \n",
    "        return model_card_template\n",
    "    \n",
    "    def generate_explanation_examples(self):\n",
    "        \"\"\"Generate examples of different explanation types\"\"\"\n",
    "        print(\"\\n\\nEXPLANATION EXAMPLES\")\n",
    "        print(\"=\" * 25)\n",
    "        \n",
    "        examples = {\n",
    "            'Global Explanation Example': {\n",
    "                'context': 'Chest X-ray pneumonia detection model',\n",
    "                'explanation': [\n",
    "                    'Most important features: lung opacity patterns, consolidation areas',\n",
    "                    'Model focuses on lower lobe regions (65% of decisions)',\n",
    "                    'Performance: 89% accuracy across 5 hospitals',\n",
    "                    'Limitation: Reduced accuracy in patients with COPD'\n",
    "                ]\n",
    "            },\n",
    "            'Local Explanation Example': {\n",
    "                'context': 'Individual pneumonia prediction (85% confidence)',\n",
    "                'explanation': [\n",
    "                    'Primary indicators: opacity in right lower lobe (+40% confidence)',\n",
    "                    'Supporting factors: air bronchograms visible (+25% confidence)',\n",
    "                    'Patient age and symptoms (+15% confidence)',\n",
    "                    'Recommendation: Clinical correlation advised'\n",
    "                ]\n",
    "            },\n",
    "            'Uncertainty Communication': {\n",
    "                'context': 'Model prediction with uncertainty bounds',\n",
    "                'explanation': [\n",
    "                    'Prediction: 75% probability of pneumonia',\n",
    "                    'Confidence interval: 65-85% (moderate confidence)',\n",
    "                    'Similar cases: 8/10 had confirmed pneumonia',\n",
    "                    'Recommendation: Consider additional imaging if clinical suspicion high'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for example_type, details in examples.items():\n",
    "            print(f\"\\n{example_type}:\")\n",
    "            print(f\"Context: {details['context']}\")\n",
    "            print(\"Explanation:\")\n",
    "            for item in details['explanation']:\n",
    "                print(f\"   • {item}\")\n",
    "        \n",
    "        return examples\n",
    "\n",
    "# Initialize explainability framework\n",
    "explainability_framework = ExplainabilityFramework()\n",
    "print(\"Explainability Framework initialized\")\n",
    "print(f\"Explanation types covered: {len(explainability_framework.explanation_types)}\")\n",
    "print(f\"Transparency requirements: {len(explainability_framework.transparency_requirements)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explainability-plan"
   },
   "outputs": [],
   "source": [
    "# Create explainability plan for diagnosis use case\n",
    "explainability_plan = explainability_framework.create_explainability_plan(\n",
    "    'diagnosis', ['Clinicians', 'Patients']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-card"
   },
   "outputs": [],
   "source": [
    "# Generate model card example\n",
    "example_model_info = {\n",
    "    'name': 'ChestAI Pneumonia Detector',\n",
    "    'version': '2.1.0',\n",
    "    'type': 'Deep Learning Binary Classifier',\n",
    "    'primary_use': 'Pneumonia detection in chest X-rays',\n",
    "    'accuracy': '0.89 (95% CI: 0.87-0.91)',\n",
    "    'limitations': [\n",
    "        'Limited to PA and AP chest X-rays',\n",
    "        'Reduced accuracy in patients with multiple comorbidities',\n",
    "        'Not validated for pediatric populations'\n",
    "    ]\n",
    "}\n",
    "\n",
    "model_card = explainability_framework.create_model_card(example_model_info)\n",
    "\n",
    "# Generate explanation examples\n",
    "explanation_examples = explainability_framework.generate_explanation_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ethical-governance"
   },
   "source": [
    "## Ethical Governance and Oversight Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "governance-framework"
   },
   "outputs": [],
   "source": [
    "class EthicalGovernanceFramework:\n",
    "    \"\"\"\n",
    "    Framework for establishing ethical governance structures for medical AI\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.governance_committees = {\n",
    "            'AI_Ethics_Committee': {\n",
    "                'purpose': 'Provide ethical oversight for AI development and deployment',\n",
    "                'composition': [\n",
    "                    'Medical ethicist (Chair)',\n",
    "                    'Chief Medical Officer',\n",
    "                    'AI/Data scientist',\n",
    "                    'Clinical department representatives',\n",
    "                    'Patient advocacy representative',\n",
    "                    'Legal counsel',\n",
    "                    'Information security officer'\n",
    "                ],\n",
    "                'responsibilities': [\n",
    "                    'Review AI implementation proposals',\n",
    "                    'Assess ethical implications and risks',\n",
    "                    'Approve deployment decisions',\n",
    "                    'Monitor ongoing ethical compliance',\n",
    "                    'Investigate ethical concerns',\n",
    "                    'Develop institutional AI policies'\n",
    "                ],\n",
    "                'meeting_frequency': 'Monthly or as needed for urgent reviews'\n",
    "            },\n",
    "            'Clinical_AI_Oversight_Board': {\n",
    "                'purpose': 'Ensure clinical safety and quality of AI systems',\n",
    "                'composition': [\n",
    "                    'Chief Quality Officer (Chair)',\n",
    "                    'Clinical department heads',\n",
    "                    'Biomedical informaticist',\n",
    "                    'Clinical research coordinator',\n",
    "                    'Nursing representative',\n",
    "                    'Pharmacy representative',\n",
    "                    'Risk management officer'\n",
    "                ],\n",
    "                'responsibilities': [\n",
    "                    'Clinical validation review',\n",
    "                    'Performance monitoring oversight',\n",
    "                    'Safety signal investigation',\n",
    "                    'Clinical workflow integration',\n",
    "                    'Quality improvement initiatives',\n",
    "                    'Incident response coordination'\n",
    "                ],\n",
    "                'meeting_frequency': 'Bi-weekly performance reviews'\n",
    "            },\n",
    "            'Data_Governance_Committee': {\n",
    "                'purpose': 'Oversee data management, privacy, and security for AI systems',\n",
    "                'composition': [\n",
    "                    'Chief Data Officer (Chair)',\n",
    "                    'Privacy officer',\n",
    "                    'Security officer',\n",
    "                    'Clinical informatics lead',\n",
    "                    'Research compliance officer',\n",
    "                    'IT systems administrator',\n",
    "                    'External privacy consultant'\n",
    "                ],\n",
    "                'responsibilities': [\n",
    "                    'Data governance policy development',\n",
    "                    'Privacy impact assessments',\n",
    "                    'Data sharing agreement oversight',\n",
    "                    'Breach response coordination',\n",
    "                    'Audit and compliance monitoring',\n",
    "                    'Data quality standards'\n",
    "                ],\n",
    "                'meeting_frequency': 'Monthly'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.oversight_processes = {\n",
    "            'pre_implementation': {\n",
    "                'ethical_review': [\n",
    "                    'Ethical impact assessment',\n",
    "                    'Stakeholder consultation',\n",
    "                    'Risk-benefit analysis',\n",
    "                    'Alternative consideration',\n",
    "                    'Implementation plan review'\n",
    "                ],\n",
    "                'clinical_validation': [\n",
    "                    'Clinical evidence review',\n",
    "                    'Performance validation',\n",
    "                    'Safety assessment',\n",
    "                    'Usability testing',\n",
    "                    'Workflow integration analysis'\n",
    "                ],\n",
    "                'regulatory_compliance': [\n",
    "                    'Regulatory pathway assessment',\n",
    "                    'Documentation review',\n",
    "                    'Quality system verification',\n",
    "                    'Approval status confirmation',\n",
    "                    'Post-market requirements'\n",
    "                ]\n",
    "            },\n",
    "            'ongoing_monitoring': {\n",
    "                'performance_oversight': [\n",
    "                    'Real-world performance tracking',\n",
    "                    'Quality metric monitoring',\n",
    "                    'User feedback collection',\n",
    "                    'Incident tracking and analysis',\n",
    "                    'Comparative effectiveness assessment'\n",
    "                ],\n",
    "                'ethical_compliance': [\n",
    "                    'Bias monitoring and assessment',\n",
    "                    'Fairness metric evaluation',\n",
    "                    'Patient consent compliance',\n",
    "                    'Transparency requirement adherence',\n",
    "                    'Ethical guideline compliance'\n",
    "                ],\n",
    "                'safety_surveillance': [\n",
    "                    'Adverse event monitoring',\n",
    "                    'Safety signal detection',\n",
    "                    'Risk assessment updates',\n",
    "                    'Corrective action implementation',\n",
    "                    'Safety communication'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_governance_charter(self, organization_name):\n",
    "        \"\"\"Create AI ethics governance charter\"\"\"\n",
    "        print(f\"AI ETHICS GOVERNANCE CHARTER\")\n",
    "        print(f\"{organization_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        charter = {\n",
    "            'Mission Statement': [\n",
    "                f'To ensure the ethical, safe, and responsible development, deployment, and use of artificial intelligence technologies at {organization_name}.',\n",
    "                'To uphold the highest standards of patient care, safety, and trust while advancing medical practice through AI innovation.',\n",
    "                'To protect patient rights, promote equity, and maintain transparency in all AI-related activities.'\n",
    "            ],\n",
    "            'Governing Principles': [\n",
    "                'Patient-centered care and benefit maximization',\n",
    "                'Do no harm and risk mitigation',\n",
    "                'Respect for patient autonomy and informed consent',\n",
    "                'Justice, fairness, and equity in AI applications',\n",
    "                'Transparency and accountability in AI systems',\n",
    "                'Professional competence and continuous learning',\n",
    "                'Privacy protection and data stewardship',\n",
    "                'Scientific rigor and evidence-based practice'\n",
    "            ],\n",
    "            'Scope of Authority': [\n",
    "                'Review and approval of all AI system implementations',\n",
    "                'Ethical oversight of AI research and development',\n",
    "                'Policy development for AI governance',\n",
    "                'Investigation of ethical concerns and violations',\n",
    "                'Monitoring and evaluation of AI system performance',\n",
    "                'Education and training program oversight',\n",
    "                'External collaboration and partnership guidance'\n",
    "            ],\n",
    "            'Decision-Making Process': [\n",
    "                'Consensus-based decision making preferred',\n",
    "                'Majority vote required for approval decisions',\n",
    "                'Chair has tie-breaking authority',\n",
    "                'Minority opinions documented and considered',\n",
    "                'External expert consultation when needed',\n",
    "                'Appeals process for disputed decisions',\n",
    "                'Documentation of all decisions and rationale'\n",
    "            ],\n",
    "            'Reporting and Accountability': [\n",
    "                'Quarterly reports to executive leadership',\n",
    "                'Annual public transparency report',\n",
    "                'Board of directors annual presentation',\n",
    "                'Regulatory reporting as required',\n",
    "                'Incident response and communication',\n",
    "                'Continuous improvement initiatives',\n",
    "                'External audit and assessment cooperation'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for section, items in charter.items():\n",
    "            print(f\"\\n{section}:\")\n",
    "            print(\"-\" * len(section))\n",
    "            for item in items:\n",
    "                if section == 'Mission Statement':\n",
    "                    print(f\"   {item}\")\n",
    "                else:\n",
    "                    print(f\"   • {item}\")\n",
    "        \n",
    "        print(f\"\\n\\nAdopted: {datetime.now().strftime('%B %d, %Y')}\")\n",
    "        print(f\"Effective Date: {datetime.now().strftime('%B %d, %Y')}\")\n",
    "        print(f\"Next Review: {datetime.now().strftime('%B %d, %Y')} (Annual)\")\n",
    "        \n",
    "        return charter\n",
    "    \n",
    "    def create_ethical_review_checklist(self):\n",
    "        \"\"\"Create comprehensive ethical review checklist\"\"\"\n",
    "        print(\"\\n\\nETHICAL REVIEW CHECKLIST FOR AI SYSTEMS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        checklist_sections = {\n",
    "            'Beneficence Assessment': [\n",
    "                '☐ Clinical benefit clearly demonstrated',\n",
    "                '☐ Evidence-based performance validation',\n",
    "                '☐ Outcome improvements quantified',\n",
    "                '☐ Patient wellbeing prioritized',\n",
    "                '☐ Benefits outweigh risks'\n",
    "            ],\n",
    "            'Non-maleficence Verification': [\n",
    "                '☐ Comprehensive risk assessment completed',\n",
    "                '☐ Safety mechanisms implemented',\n",
    "                '☐ Failure mode analysis conducted',\n",
    "                '☐ Human oversight mechanisms in place',\n",
    "                '☐ Error detection and correction systems active'\n",
    "            ],\n",
    "            'Autonomy Protection': [\n",
    "                '☐ Informed consent process established',\n",
    "                '☐ Patient education materials prepared',\n",
    "                '☐ Opt-out mechanisms available',\n",
    "                '☐ Decision transparency maintained',\n",
    "                '☐ Human review options provided'\n",
    "            ],\n",
    "            'Justice and Fairness': [\n",
    "                '☐ Bias assessment completed',\n",
    "                '☐ Fairness metrics evaluated',\n",
    "                '☐ Equitable access ensured',\n",
    "                '☐ Vulnerable populations protected',\n",
    "                '☐ Health equity considerations addressed'\n",
    "            ],\n",
    "            'Transparency Requirements': [\n",
    "                '☐ Model documentation complete',\n",
    "                '☐ Performance metrics disclosed',\n",
    "                '☐ Limitations clearly communicated',\n",
    "                '☐ Explanation mechanisms implemented',\n",
    "                '☐ Audit trails established'\n",
    "            ],\n",
    "            'Accountability Structures': [\n",
    "                '☐ Responsibility chains defined',\n",
    "                '☐ Governance oversight established',\n",
    "                '☐ Monitoring systems active',\n",
    "                '☐ Incident response procedures ready',\n",
    "                '☐ Professional standards maintained'\n",
    "            ],\n",
    "            'Privacy and Security': [\n",
    "                '☐ Privacy impact assessment completed',\n",
    "                '☐ Data protection measures implemented',\n",
    "                '☐ Security controls verified',\n",
    "                '☐ Consent for data use obtained',\n",
    "                '☐ Compliance with regulations confirmed'\n",
    "            ],\n",
    "            'Quality Assurance': [\n",
    "                '☐ Quality management system in place',\n",
    "                '☐ Performance monitoring active',\n",
    "                '☐ Continuous improvement processes established',\n",
    "                '☐ User training completed',\n",
    "                '☐ Support systems operational'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for section, items in checklist_sections.items():\n",
    "            print(f\"\\n{section}:\")\n",
    "            for item in items:\n",
    "                print(f\"   {item}\")\n",
    "        \n",
    "        print(f\"\\n\\nReview Date: _____________\")\n",
    "        print(f\"Reviewer: _____________\")\n",
    "        print(f\"Approval Status: _____________\")\n",
    "        print(f\"Next Review Date: _____________\")\n",
    "        \n",
    "        return checklist_sections\n",
    "\n",
    "# Initialize governance framework\n",
    "governance_framework = EthicalGovernanceFramework()\n",
    "print(\"Ethical Governance Framework initialized\")\n",
    "print(f\"Governance committees defined: {len(governance_framework.governance_committees)}\")\n",
    "print(f\"Oversight processes established: {len(governance_framework.oversight_processes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "governance-charter"
   },
   "outputs": [],
   "source": [
    "# Create governance charter\n",
    "charter = governance_framework.create_governance_charter(\"Academic Medical Center\")\n",
    "\n",
    "# Create ethical review checklist\n",
    "ethical_checklist = governance_framework.create_ethical_review_checklist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "case-studies"
   },
   "source": [
    "## Ethical Case Studies and Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "case-studies-framework"
   },
   "outputs": [],
   "source": [
    "class EthicalCaseStudies:\n",
    "    \"\"\"\n",
    "    Collection of ethical case studies and scenarios for medical AI\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.case_studies = {\n",
    "            'case_1_bias_detection': {\n",
    "                'title': 'Racial Bias in Dermatology AI',\n",
    "                'scenario': [\n",
    "                    'A dermatology AI system shows excellent performance on light skin',\n",
    "                    'but significantly reduced accuracy for darker skin tones.',\n",
    "                    'The training dataset was primarily from patients with light skin.',\n",
    "                    'Clinical deployment could worsen health disparities.'\n",
    "                ],\n",
    "                'ethical_issues': [\n",
    "                    'Justice: Unequal benefits across racial groups',\n",
    "                    'Non-maleficence: Potential for misdiagnosis in minority populations',\n",
    "                    'Beneficence: Benefits not distributed fairly'\n",
    "                ],\n",
    "                'stakeholders': [\n",
    "                    'Patients from underrepresented communities',\n",
    "                    'Dermatologists using the system',\n",
    "                    'Healthcare institution',\n",
    "                    'AI development company',\n",
    "                    'Regulatory authorities'\n",
    "                ],\n",
    "                'potential_solutions': [\n",
    "                    'Delay deployment until bias is addressed',\n",
    "                    'Collect additional diverse training data',\n",
    "                    'Implement bias-aware algorithms',\n",
    "                    'Deploy with clear limitations and warnings',\n",
    "                    'Establish monitoring for disparate outcomes'\n",
    "                ],\n",
    "                'recommended_approach': [\n",
    "                    'Immediate halt of deployment plans',\n",
    "                    'Comprehensive bias audit and mitigation',\n",
    "                    'Diverse stakeholder consultation',\n",
    "                    'Re-validation with representative data',\n",
    "                    'Transparent communication about limitations'\n",
    "                ]\n",
    "            },\n",
    "            'case_2_transparency_conflict': {\n",
    "                'title': 'Proprietary Algorithm vs. Transparency',\n",
    "                'scenario': [\n",
    "                    'A commercial AI company provides highly accurate diagnostic AI',\n",
    "                    'but refuses to share algorithmic details citing trade secrets.',\n",
    "                    'Physicians want to understand how decisions are made',\n",
    "                    'but the \"black box\" nature limits clinical acceptance.'\n",
    "                ],\n",
    "                'ethical_issues': [\n",
    "                    'Transparency: Lack of algorithmic explainability',\n",
    "                    'Autonomy: Physicians unable to fully inform patients',\n",
    "                    'Accountability: Unclear responsibility for decisions'\n",
    "                ],\n",
    "                'stakeholders': [\n",
    "                    'Patients receiving AI-assisted care',\n",
    "                    'Physicians using the system',\n",
    "                    'Hospital administration',\n",
    "                    'AI company and shareholders',\n",
    "                    'Medical malpractice insurers'\n",
    "                ],\n",
    "                'potential_solutions': [\n",
    "                    'Require minimum transparency standards',\n",
    "                    'Develop explanation-focused interfaces',\n",
    "                    'Implement third-party algorithmic audits',\n",
    "                    'Create liability-sharing agreements',\n",
    "                    'Establish professional interpretation guidelines'\n",
    "                ],\n",
    "                'recommended_approach': [\n",
    "                    'Negotiate transparency requirements in contracts',\n",
    "                    'Require explanation capabilities for deployment',\n",
    "                    'Establish clear accountability frameworks',\n",
    "                    'Implement physician training on AI limitations',\n",
    "                    'Develop patient communication protocols'\n",
    "                ]\n",
    "            },\n",
    "            'case_3_consent_emergency': {\n",
    "                'title': 'AI Use in Emergency Situations',\n",
    "                'scenario': [\n",
    "                    'An AI system for emergency stroke detection can rapidly identify cases',\n",
    "                    'but unconscious patients cannot provide informed consent.',\n",
    "                    'Time-critical decisions must be made quickly',\n",
    "                    'and family members may not be immediately available.'\n",
    "                ],\n",
    "                'ethical_issues': [\n",
    "                    'Autonomy: Inability to obtain informed consent',\n",
    "                    'Beneficence: AI could save lives through rapid detection',\n",
    "                    'Justice: Emergency care should be available to all'\n",
    "                ],\n",
    "                'stakeholders': [\n",
    "                    'Emergency patients and their families',\n",
    "                    'Emergency physicians and nurses',\n",
    "                    'Hospital emergency department',\n",
    "                    'Healthcare system administrators',\n",
    "                    'Medical ethics committees'\n",
    "                ],\n",
    "                'potential_solutions': [\n",
    "                    'Implement presumed consent with opt-out options',\n",
    "                    'Use AI as decision support only, not autonomous decisions',\n",
    "                    'Require post-emergency disclosure and consent',\n",
    "                    'Establish clear clinical override procedures',\n",
    "                    'Create emergency ethics consultation protocols'\n",
    "                ],\n",
    "                'recommended_approach': [\n",
    "                    'Develop emergency AI use protocols',\n",
    "                    'Implement layered consent procedures',\n",
    "                    'Ensure physician oversight for all decisions',\n",
    "                    'Establish post-emergency disclosure procedures',\n",
    "                    'Create opt-out mechanisms for future care'\n",
    "                ]\n",
    "            },\n",
    "            'case_4_resource_allocation': {\n",
    "                'title': 'AI-Assisted Resource Allocation During Crisis',\n",
    "                'scenario': [\n",
    "                    'During a pandemic, an AI system helps allocate scarce ICU beds',\n",
    "                    'based on predicted survival probability and treatment success.',\n",
    "                    'The algorithm may inadvertently discriminate against elderly',\n",
    "                    'or chronically ill patients due to lower predicted outcomes.'\n",
    "                ],\n",
    "                'ethical_issues': [\n",
    "                    'Justice: Fair distribution of scarce resources',\n",
    "                    'Non-maleficence: Avoiding systematic discrimination',\n",
    "                    'Autonomy: Respecting individual treatment preferences'\n",
    "                ],\n",
    "                'stakeholders': [\n",
    "                    'Critically ill patients and families',\n",
    "                    'ICU physicians and staff',\n",
    "                    'Hospital administrators',\n",
    "                    'Ethics committees',\n",
    "                    'Public health authorities',\n",
    "                    'Society at large'\n",
    "                ],\n",
    "                'potential_solutions': [\n",
    "                    'Develop fair allocation criteria beyond AI predictions',\n",
    "                    'Include ethical review in allocation decisions',\n",
    "                    'Implement human oversight for all allocation decisions',\n",
    "                    'Create appeal and review processes',\n",
    "                    'Ensure transparent criteria and decision-making'\n",
    "                ],\n",
    "                'recommended_approach': [\n",
    "                    'Use AI as one factor among multiple criteria',\n",
    "                    'Establish ethics committee oversight',\n",
    "                    'Implement age and disability bias protections',\n",
    "                    'Create transparent allocation protocols',\n",
    "                    'Ensure regular review and appeals process'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def present_case_study(self, case_key):\n",
    "        \"\"\"Present detailed case study analysis\"\"\"\n",
    "        if case_key not in self.case_studies:\n",
    "            print(f\"Case study '{case_key}' not found.\")\n",
    "            return\n",
    "        \n",
    "        case = self.case_studies[case_key]\n",
    "        \n",
    "        print(f\"ETHICAL CASE STUDY: {case['title']}\")\n",
    "        print(\"=\" * (len(case['title']) + 22))\n",
    "        \n",
    "        print(\"\\nSCENARIO:\")\n",
    "        for line in case['scenario']:\n",
    "            print(f\"   {line}\")\n",
    "        \n",
    "        print(\"\\nETHICAL ISSUES:\")\n",
    "        for issue in case['ethical_issues']:\n",
    "            print(f\"   • {issue}\")\n",
    "        \n",
    "        print(\"\\nSTAKEHOLDERS:\")\n",
    "        for stakeholder in case['stakeholders']:\n",
    "            print(f\"   • {stakeholder}\")\n",
    "        \n",
    "        print(\"\\nPOTENTIAL SOLUTIONS:\")\n",
    "        for solution in case['potential_solutions']:\n",
    "            print(f\"   • {solution}\")\n",
    "        \n",
    "        print(\"\\nRECOMMENDED APPROACH:\")\n",
    "        for step in case['recommended_approach']:\n",
    "            print(f\"   1. {step}\" if step == case['recommended_approach'][0] else f\"   2. {step}\" if step == case['recommended_approach'][1] else f\"   3. {step}\" if step == case['recommended_approach'][2] else f\"   4. {step}\" if step == case['recommended_approach'][3] else f\"   5. {step}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        return case\n",
    "    \n",
    "    def create_ethical_decision_matrix(self):\n",
    "        \"\"\"Create decision matrix for ethical evaluation\"\"\"\n",
    "        print(\"ETHICAL DECISION MATRIX\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        # Create matrix data\n",
    "        cases = list(self.case_studies.keys())\n",
    "        principles = ['Beneficence', 'Non-maleficence', 'Autonomy', 'Justice', 'Transparency', 'Accountability']\n",
    "        \n",
    "        # Simulate ethical impact scores (1-5 scale)\n",
    "        np.random.seed(42)\n",
    "        matrix_data = []\n",
    "        \n",
    "        impact_patterns = {\n",
    "            'case_1_bias_detection': [3, 2, 3, 1, 3, 3],  # High justice impact\n",
    "            'case_2_transparency_conflict': [3, 3, 2, 3, 1, 2],  # High transparency impact\n",
    "            'case_3_consent_emergency': [4, 4, 1, 4, 3, 3],  # High autonomy impact\n",
    "            'case_4_resource_allocation': [3, 2, 2, 1, 3, 3]  # High justice impact\n",
    "        }\n",
    "        \n",
    "        for case in cases:\n",
    "            case_name = self.case_studies[case]['title'][:20] + '...' if len(self.case_studies[case]['title']) > 20 else self.case_studies[case]['title']\n",
    "            row = [case_name] + impact_patterns[case]\n",
    "            matrix_data.append(row)\n",
    "        \n",
    "        # Create DataFrame for visualization\n",
    "        columns = ['Case Study'] + principles\n",
    "        matrix_df = pd.DataFrame(matrix_data, columns=columns)\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Prepare data for heatmap (exclude case names)\n",
    "        heatmap_data = matrix_df.iloc[:, 1:].astype(float)\n",
    "        \n",
    "        sns.heatmap(heatmap_data, \n",
    "                   annot=True, \n",
    "                   fmt='d', \n",
    "                   cmap='RdYlBu_r',\n",
    "                   xticklabels=principles,\n",
    "                   yticklabels=[self.case_studies[case]['title'][:25] + '...' if len(self.case_studies[case]['title']) > 25 else self.case_studies[case]['title'] for case in cases],\n",
    "                   cbar_kws={'label': 'Ethical Impact (1=Low, 5=High)'})\n",
    "        \n",
    "        plt.title('Ethical Impact Matrix for Case Studies', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Ethical Principles')\n",
    "        plt.ylabel('Case Studies')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nETHICAL IMPACT SCORES (1=Low Impact, 5=High Impact)\")\n",
    "        print(matrix_df.to_string(index=False))\n",
    "        \n",
    "        return matrix_df\n",
    "    \n",
    "    def generate_discussion_questions(self, case_key):\n",
    "        \"\"\"Generate discussion questions for case study\"\"\"\n",
    "        if case_key not in self.case_studies:\n",
    "            return []\n",
    "        \n",
    "        case = self.case_studies[case_key]\n",
    "        \n",
    "        questions = {\n",
    "            'case_1_bias_detection': [\n",
    "                'How should healthcare institutions balance innovation with equity?',\n",
    "                'What are the responsibilities of AI companies to ensure fairness?',\n",
    "                'Should biased AI systems ever be deployed with warnings?',\n",
    "                'How can we prevent bias from being perpetuated in medical AI?',\n",
    "                'What role should affected communities play in AI development?'\n",
    "            ],\n",
    "            'case_2_transparency_conflict': [\n",
    "                'How much algorithmic transparency is necessary for ethical use?',\n",
    "                'Can proprietary algorithms be ethically justified in healthcare?',\n",
    "                'How should liability be allocated between AI companies and physicians?',\n",
    "                'What are the minimum explanation requirements for clinical AI?',\n",
    "                'How can we balance innovation incentives with transparency needs?'\n",
    "            ],\n",
    "            'case_3_consent_emergency': [\n",
    "                'When is presumed consent acceptable for AI use in medicine?',\n",
    "                'How do we respect autonomy in emergency situations?',\n",
    "                'Should patients be able to opt out of AI assistance in advance?',\n",
    "                'What are the duties to disclose AI use after emergency treatment?',\n",
    "                'How do we balance life-saving potential with consent requirements?'\n",
    "            ],\n",
    "            'case_4_resource_allocation': [\n",
    "                'Should AI predictions influence life-and-death allocation decisions?',\n",
    "                'How do we ensure fairness in crisis resource allocation?',\n",
    "                'What safeguards are needed against discriminatory algorithms?',\n",
    "                'Who should have final authority over AI-assisted allocation decisions?',\n",
    "                'How do we balance individual rights with population-level optimization?'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nDISCUSSION QUESTIONS FOR: {case['title']}\")\n",
    "        print(\"-\" * (len(case['title']) + 25))\n",
    "        \n",
    "        for i, question in enumerate(questions.get(case_key, []), 1):\n",
    "            print(f\"{i}. {question}\")\n",
    "        \n",
    "        return questions.get(case_key, [])\n",
    "\n",
    "# Initialize case studies\n",
    "case_studies = EthicalCaseStudies()\n",
    "print(\"Ethical Case Studies Framework initialized\")\n",
    "print(f\"Case studies available: {len(case_studies.case_studies)}\")\n",
    "print(f\"Cases: {list(case_studies.case_studies.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "present-case-studies"
   },
   "outputs": [],
   "source": [
    "# Present case study 1: Bias Detection\n",
    "case1 = case_studies.present_case_study('case_1_bias_detection')\n",
    "\n",
    "# Generate discussion questions\n",
    "questions1 = case_studies.generate_discussion_questions('case_1_bias_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "case-study-matrix"
   },
   "outputs": [],
   "source": [
    "# Create ethical decision matrix\n",
    "ethical_matrix = case_studies.create_ethical_decision_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial-summary"
   },
   "source": [
    "## Tutorial Summary and Action Plan\n",
    "\n",
    "### What You Have Learned:\n",
    "\n",
    "**Ethical Foundations:**\n",
    "- Core ethical principles for medical AI: beneficence, non-maleficence, autonomy, justice, transparency, accountability\n",
    "- Practical applications and implementation challenges for each principle\n",
    "- Systematic framework for ethical evaluation and decision-making\n",
    "\n",
    "**Regulatory Compliance:**\n",
    "- Comprehensive overview of global regulatory landscapes (FDA, EU MDR, Health Canada)\n",
    "- Step-by-step compliance processes for pre-market and post-market phases\n",
    "- Risk assessment methodologies and regulatory pathway determination\n",
    "\n",
    "**Bias and Fairness:**\n",
    "- Types of bias in medical AI systems and their detection methods\n",
    "- Fairness metrics and evaluation frameworks\n",
    "- Mitigation strategies and monitoring approaches\n",
    "\n",
    "**Transparency and Explainability:**\n",
    "- Different types of explanations and their clinical applications\n",
    "- Model documentation and transparency requirements\n",
    "- Communication strategies for different stakeholder groups\n",
    "\n",
    "**Governance Structures:**\n",
    "- Ethical oversight committee compositions and responsibilities\n",
    "- Governance charters and decision-making processes\n",
    "- Monitoring and accountability mechanisms\n",
    "\n",
    "### Skills Acquired:\n",
    "\n",
    "- **Ethical Analysis**: Ability to systematically evaluate AI systems using established ethical frameworks\n",
    "- **Risk Assessment**: Skills to identify, assess, and mitigate ethical and regulatory risks\n",
    "- **Bias Detection**: Competency in recognizing and addressing bias and fairness issues\n",
    "- **Stakeholder Engagement**: Understanding of how to involve different stakeholders in ethical decision-making\n",
    "- **Policy Development**: Capability to develop institutional policies and governance structures\n",
    "\n",
    "### Implementation Priorities:\n",
    "\n",
    "**Immediate Actions (0-3 months):**\n",
    "1. Establish ethical review processes for AI initiatives\n",
    "2. Conduct bias audits of existing AI systems\n",
    "3. Develop informed consent procedures for AI use\n",
    "4. Create transparency documentation for deployed systems\n",
    "5. Form multidisciplinary ethics committee\n",
    "\n",
    "**Medium-term Goals (3-12 months):**\n",
    "1. Implement comprehensive governance structures\n",
    "2. Develop institutional AI ethics policies\n",
    "3. Establish monitoring and auditing systems\n",
    "4. Create training programs for staff\n",
    "5. Build stakeholder engagement processes\n",
    "\n",
    "**Long-term Objectives (1-3 years):**\n",
    "1. Achieve full regulatory compliance for all AI systems\n",
    "2. Demonstrate measurable improvements in fairness and equity\n",
    "3. Establish organization as a leader in ethical AI deployment\n",
    "4. Contribute to professional standards and best practices\n",
    "5. Build sustainable culture of responsible AI innovation\n",
    "\n",
    "### Key Success Metrics:\n",
    "\n",
    "**Ethical Compliance:**\n",
    "- 100% of AI systems undergo ethical review\n",
    "- Zero tolerance for biased or unfair AI deployment\n",
    "- Complete transparency documentation for all systems\n",
    "- Patient satisfaction with AI transparency and consent processes\n",
    "\n",
    "**Regulatory Adherence:**\n",
    "- Full compliance with applicable regulatory requirements\n",
    "- Successful regulatory approvals for new AI systems\n",
    "- Zero regulatory violations or warnings\n",
    "- Proactive engagement with regulatory bodies\n",
    "\n",
    "**Organizational Culture:**\n",
    "- High awareness of AI ethics among staff\n",
    "- Active participation in ethics committees and processes\n",
    "- Regular reporting and discussion of ethical issues\n",
    "- Continuous improvement in ethical practices\n",
    "\n",
    "### Professional Responsibility:\n",
    "\n",
    "As healthcare professionals deploying AI systems, you have responsibilities to:\n",
    "\n",
    "- **Patients**: Ensure AI serves their best interests and respects their rights\n",
    "- **Profession**: Uphold medical ethics and professional standards\n",
    "- **Society**: Promote equitable and beneficial use of AI in healthcare\n",
    "- **Future generations**: Establish responsible precedents for AI development\n",
    "\n",
    "### Continuous Learning:\n",
    "\n",
    "The field of AI ethics evolves rapidly. Stay current through:\n",
    "- Professional development in AI ethics and law\n",
    "- Participation in professional societies and working groups\n",
    "- Regular review of regulatory updates and guidance\n",
    "- Engagement with patient advocacy and community groups\n",
    "- Collaboration with ethicists, lawyers, and social scientists\n",
    "\n",
    "You now have the knowledge, tools, and frameworks necessary to deploy AI systems responsibly and ethically in clinical practice while maintaining the highest standards of patient care and professional integrity."
   ]
  }
 ],\n "metadata": {\n  \"accelerator\": \"GPU\",\n  \"colab\": {\n   \"gpuType\": \"T4\",\n   \"provenance\": []\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 0\n}