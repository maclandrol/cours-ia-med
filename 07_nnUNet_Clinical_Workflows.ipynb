{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/maclandrol/cours-ia-med/blob/master/07_nnUNet_Clinical_Workflows.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 07. nnU-Net - Guide Complet et Workflows Cliniques\n",
        "\n",
        "**Enseignant:** Emmanuel Noutahi, PhD\n",
        "\n",
        "---\n",
        "\n",
        "**Objectif:** Maîtriser nnU-Net de l'installation au déploiement clinique.\n",
        "\n",
        "**Applications pratiques :**\n",
        "- Configuration complète de l'environnement nnU-Net\n",
        "- Entraînement sur datasets médicaux (Medical Decathlon)\n",
        "- Adaptation aux données personnalisées\n",
        "- Inférence et post-traitement\n",
        "- Intégration dans workflows hospitaliers (DICOM, PACS)\n",
        "- Déploiement en production clinique\n",
        "\n",
        "**Important:** Ce cours couvre l'implémentation complète de nnU-Net pour usage clinique réel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation et Configuration\n",
        "\n",
        "nnU-Net est un framework de segmentation automatique qui s'adapte automatiquement aux données d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation complète de nnU-Net et dépendances\n",
        "!pip install nnunet torch torchvision -q\n",
        "!pip install nibabel SimpleITK pandas numpy matplotlib seaborn -q\n",
        "!pip install scikit-learn scipy tqdm -q\n",
        "!pip install pydicom monai -q\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration système\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Dispositif utilisé: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Mémoire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(\"Configuration nnU-Net prête.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration de l'Environnement nnU-Net\n",
        "\n",
        "nnU-Net utilise une structure de dossiers spécifique pour organiser les données d'entraînement et les résultats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration de l'environnement nnU-Net\n",
        "print(\"=== CONFIGURATION DE L'ENVIRONNEMENT NNUNET ===\")\n",
        "\n",
        "# Configuration des chemins nnU-Net\n",
        "base_dir = Path(\"/content/nnUNet_data\") if 'google.colab' in sys.modules else Path(\"./nnUNet_data\")\n",
        "\n",
        "# Structure des dossiers nnU-Net\n",
        "folders = {\n",
        "    'nnUNet_raw': base_dir / \"nnUNet_raw\",\n",
        "    'nnUNet_preprocessed': base_dir / \"nnUNet_preprocessed\", \n",
        "    'nnUNet_results': base_dir / \"nnUNet_results\",\n",
        "    'nnUNet_trained_models': base_dir / \"nnUNet_trained_models\"\n",
        "}\n",
        "\n",
        "# Création des dossiers\n",
        "for folder_name, folder_path in folders.items():\n",
        "    folder_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Dossier créé: {folder_path}\")\n",
        "\n",
        "# Configuration des variables d'environnement\n",
        "os.environ['nnUNet_raw'] = str(folders['nnUNet_raw'])\n",
        "os.environ['nnUNet_preprocessed'] = str(folders['nnUNet_preprocessed'])\n",
        "os.environ['nnUNet_results'] = str(folders['nnUNet_results'])\n",
        "\n",
        "print(f\"\\nVariables d'environnement configurées:\")\n",
        "print(f\"nnUNet_raw: {os.environ.get('nnUNet_raw')}\")\n",
        "print(f\"nnUNet_preprocessed: {os.environ.get('nnUNet_preprocessed')}\")\n",
        "print(f\"nnUNet_results: {os.environ.get('nnUNet_results')}\")\n",
        "\n",
        "# Fonction utilitaire pour créer la structure dataset\n",
        "def create_dataset_structure(dataset_id, dataset_name, base_folder):\n",
        "    \"\"\"\n",
        "    Crée la structure de dossiers pour un dataset nnU-Net\n",
        "    \"\"\"\n",
        "    dataset_folder = base_folder / f\"Dataset{dataset_id:03d}_{dataset_name}\"\n",
        "    \n",
        "    subfolders = ['imagesTr', 'labelsTr', 'imagesTs', 'labelsTs']\n",
        "    created_paths = {}\n",
        "    \n",
        "    for subfolder in subfolders:\n",
        "        path = dataset_folder / subfolder\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "        created_paths[subfolder] = path\n",
        "    \n",
        "    # Fichier dataset.json template\n",
        "    dataset_json = {\n",
        "        \"name\": dataset_name,\n",
        "        \"description\": f\"Dataset {dataset_id} - {dataset_name}\",\n",
        "        \"tensorImageSize\": \"3D\",\n",
        "        \"reference\": \"Custom dataset\",\n",
        "        \"licence\": \"Educational use\",\n",
        "        \"release\": \"1.0\",\n",
        "        \"modality\": {\n",
        "            \"0\": \"MRI\"\n",
        "        },\n",
        "        \"labels\": {\n",
        "            \"0\": \"Background\",\n",
        "            \"1\": \"Target\"\n",
        "        },\n",
        "        \"numTraining\": 0,\n",
        "        \"numTest\": 0,\n",
        "        \"training\": [],\n",
        "        \"test\": []\n",
        "    }\n",
        "    \n",
        "    json_path = dataset_folder / \"dataset.json\"\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(dataset_json, f, indent=2)\n",
        "    \n",
        "    created_paths['dataset_json'] = json_path\n",
        "    created_paths['dataset_folder'] = dataset_folder\n",
        "    \n",
        "    return created_paths\n",
        "\n",
        "print(\"\\nFonctions utilitaires créées.\")\n",
        "print(\"Environnement nnU-Net configuré avec succès.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Préparation des Données Médicales\n",
        "\n",
        "Préparons des données d'exemple au format nnU-Net en utilisant le Medical Decathlon comme référence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Préparation des données médicales pour nnU-Net\n",
        "print(\"=== PRÉPARATION DES DONNÉES MÉDICALES ===\")\n",
        "\n",
        "import nibabel as nib\n",
        "import SimpleITK as sitk\n",
        "\n",
        "def create_synthetic_medical_data(task_name=\"Hippocampus\", task_id=4, num_samples=5):\n",
        "    \"\"\"\n",
        "    Crée des données médicales synthétiques au format nnU-Net\n",
        "    Simule le dataset Medical Decathlon Task004 - Hippocampus\n",
        "    \"\"\"\n",
        "    \n",
        "    # Création de la structure\n",
        "    dataset_paths = create_dataset_structure(task_id, task_name, folders['nnUNet_raw'])\n",
        "    \n",
        "    print(f\"Création de {num_samples} échantillons synthétiques...\")\n",
        "    \n",
        "    training_cases = []\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        case_id = f\"hippocampus_{i+1:03d}\"\n",
        "        \n",
        "        # Création d'une image cérébrale synthétique (64x64x32)\n",
        "        image_shape = (64, 64, 32)\n",
        "        \n",
        "        # Image de base (matière grise/blanche)\n",
        "        brain_image = np.random.normal(100, 20, image_shape).astype(np.float32)\n",
        "        \n",
        "        # Ajout du crâne\n",
        "        x, y, z = np.meshgrid(\n",
        "            np.linspace(-1, 1, image_shape[0]),\n",
        "            np.linspace(-1, 1, image_shape[1]), \n",
        "            np.linspace(-1, 1, image_shape[2]),\n",
        "            indexing='ij'\n",
        "        )\n",
        "        \n",
        "        # Masque cérébral (sphère)\n",
        "        brain_mask = (x**2 + y**2 + z**2) < 0.8\n",
        "        brain_image[~brain_mask] = 0\n",
        "        \n",
        "        # Création de l'hippocampe synthétique\n",
        "        hippo_mask = np.zeros(image_shape, dtype=np.uint8)\n",
        "        \n",
        "        # Hippocampe gauche\n",
        "        hippo_left = (\n",
        "            (x + 0.3)**2 * 4 + (y - 0.1)**2 * 8 + (z)**2 * 12 < 0.15\n",
        "        ) & brain_mask\n",
        "        \n",
        "        # Hippocampe droit \n",
        "        hippo_right = (\n",
        "            (x - 0.3)**2 * 4 + (y - 0.1)**2 * 8 + (z)**2 * 12 < 0.15\n",
        "        ) & brain_mask\n",
        "        \n",
        "        # Assignation des labels\n",
        "        hippo_mask[hippo_left] = 1  # Hippocampe gauche\n",
        "        hippo_mask[hippo_right] = 2  # Hippocampe droit\n",
        "        \n",
        "        # Intensité plus élevée dans l'hippocampe\n",
        "        brain_image[hippo_mask > 0] = 150 + np.random.normal(0, 10, np.sum(hippo_mask > 0))\n",
        "        \n",
        "        # Sauvegarde au format NIfTI\n",
        "        # Image principale\n",
        "        img_filename = f\"{case_id}_0000.nii.gz\"\n",
        "        img_path = dataset_paths['imagesTr'] / img_filename\n",
        "        \n",
        "        # Création du NIfTI avec métadonnées réalistes\n",
        "        affine = np.eye(4)\n",
        "        affine[0, 0] = affine[1, 1] = affine[2, 2] = 1.0  # Résolution 1mm isotrope\n",
        "        \n",
        "        nifti_img = nib.Nifti1Image(brain_image, affine)\n",
        "        nib.save(nifti_img, img_path)\n",
        "        \n",
        "        # Label/segmentation\n",
        "        label_filename = f\"{case_id}.nii.gz\"\n",
        "        label_path = dataset_paths['labelsTr'] / label_filename\n",
        "        \n",
        "        nifti_label = nib.Nifti1Image(hippo_mask.astype(np.uint8), affine)\n",
        "        nib.save(nifti_label, label_path)\n",
        "        \n",
        "        # Ajout à la liste d'entraînement\n",
        "        training_cases.append({\n",
        "            \"image\": f\"./imagesTr/{img_filename}\",\n",
        "            \"label\": f\"./labelsTr/{label_filename}\"\n",
        "        })\n",
        "        \n",
        "        print(f\"  Cas {i+1}/{num_samples} créé: {case_id}\")\n",
        "    \n",
        "    # Mise à jour du dataset.json\n",
        "    with open(dataset_paths['dataset_json'], 'r') as f:\n",
        "        dataset_json = json.load(f)\n",
        "    \n",
        "    dataset_json.update({\n",
        "        \"description\": \"Synthetic Hippocampus segmentation from brain MRI\",\n",
        "        \"modality\": {\n",
        "            \"0\": \"T1\"\n",
        "        },\n",
        "        \"labels\": {\n",
        "            \"0\": \"Background\",\n",
        "            \"1\": \"Hippocampus_left\", \n",
        "            \"2\": \"Hippocampus_right\"\n",
        "        },\n",
        "        \"numTraining\": len(training_cases),\n",
        "        \"training\": training_cases\n",
        "    })\n",
        "    \n",
        "    with open(dataset_paths['dataset_json'], 'w') as f:\n",
        "        json.dump(dataset_json, f, indent=2)\n",
        "    \n",
        "    return dataset_paths, training_cases\n",
        "\n",
        "# Création du dataset d'exemple\n",
        "dataset_paths, training_data = create_synthetic_medical_data(\n",
        "    task_name=\"Hippocampus\", \n",
        "    task_id=4, \n",
        "    num_samples=8\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset créé avec succès:\")\n",
        "print(f\"Dossier: {dataset_paths['dataset_folder']}\")\n",
        "print(f\"Images d'entraînement: {len(training_data)}\")\n",
        "\n",
        "# Vérification de la structure\n",
        "def verify_dataset_structure(dataset_folder):\n",
        "    \"\"\"\n",
        "    Vérifie la structure du dataset nnU-Net\n",
        "    \"\"\"\n",
        "    print(f\"\\nVérification de la structure du dataset:\")\n",
        "    \n",
        "    required_folders = ['imagesTr', 'labelsTr']\n",
        "    for folder in required_folders:\n",
        "        folder_path = dataset_folder / folder\n",
        "        if folder_path.exists():\n",
        "            file_count = len(list(folder_path.glob('*.nii.gz')))\n",
        "            print(f\"✓ {folder}: {file_count} fichiers\")\n",
        "        else:\n",
        "            print(f\"✗ {folder}: Dossier manquant\")\n",
        "    \n",
        "    # Vérification du dataset.json\n",
        "    json_path = dataset_folder / \"dataset.json\"\n",
        "    if json_path.exists():\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"✓ dataset.json: {data.get('numTraining', 0)} cas d'entraînement\")\n",
        "        print(f\"  Modalités: {list(data.get('modality', {}).values())}\")\n",
        "        print(f\"  Labels: {list(data.get('labels', {}).values())}\")\n",
        "    else:\n",
        "        print(f\"✗ dataset.json: Fichier manquant\")\n",
        "\n",
        "verify_dataset_structure(dataset_paths['dataset_folder'])\n",
        "\n",
        "print(\"\\nDonnées préparées pour nnU-Net.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Préprocessing et Configuration Automatique\n",
        "\n",
        "nnU-Net analyse automatiquement les données et configure les paramètres optimaux d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Préprocessing automatique nnU-Net\n",
        "print(\"=== PRÉPROCESSING ET CONFIGURATION AUTOMATIQUE ===\")\n",
        "\n",
        "def run_nnunet_preprocessing(dataset_id, dataset_folder):\n",
        "    \"\"\"\n",
        "    Simule le préprocessing nnU-Net\n",
        "    En production, ceci utiliserait: nnUNetv2_plan_and_preprocess\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"Analyse du dataset {dataset_id}...\")\n",
        "    \n",
        "    # Simulation de l'analyse des données\n",
        "    imagesTr_folder = dataset_folder / \"imagesTr\"\n",
        "    labelsTr_folder = dataset_folder / \"labelsTr\"\n",
        "    \n",
        "    image_files = list(imagesTr_folder.glob('*.nii.gz'))\n",
        "    label_files = list(labelsTr_folder.glob('*.nii.gz'))\n",
        "    \n",
        "    print(f\"Images trouvées: {len(image_files)}\")\n",
        "    print(f\"Labels trouvés: {len(label_files)}\")\n",
        "    \n",
        "    # Analyse des propriétés des images\n",
        "    image_properties = []\n",
        "    \n",
        "    for img_file in image_files[:3]:  # Analyser quelques échantillons\n",
        "        try:\n",
        "            nifti_img = nib.load(img_file)\n",
        "            data = nifti_img.get_fdata()\n",
        "            \n",
        "            properties = {\n",
        "                'shape': data.shape,\n",
        "                'spacing': nifti_img.header.get_zooms(),\n",
        "                'intensity_mean': float(np.mean(data[data > 0])),\n",
        "                'intensity_std': float(np.std(data[data > 0])),\n",
        "                'intensity_min': float(np.min(data)),\n",
        "                'intensity_max': float(np.max(data))\n",
        "            }\n",
        "            \n",
        "            image_properties.append(properties)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors de l'analyse de {img_file}: {e}\")\n",
        "    \n",
        "    # Calcul des statistiques globales\n",
        "    if image_properties:\n",
        "        avg_shape = tuple(int(np.mean([p['shape'][i] for p in image_properties])) \n",
        "                         for i in range(len(image_properties[0]['shape'])))\n",
        "        avg_spacing = tuple(np.mean([p['spacing'][i] for p in image_properties]) \n",
        "                           for i in range(len(image_properties[0]['spacing'])))\n",
        "        avg_intensity = np.mean([p['intensity_mean'] for p in image_properties])\n",
        "        \n",
        "        print(f\"\\nPropriétés moyennes du dataset:\")\n",
        "        print(f\"  Forme moyenne: {avg_shape}\")\n",
        "        print(f\"  Espacement moyen: {[f'{s:.2f}mm' for s in avg_spacing]}\")\n",
        "        print(f\"  Intensité moyenne: {avg_intensity:.1f}\")\n",
        "    \n",
        "    # Configuration automatique simulée\n",
        "    configurations = {\n",
        "        '2d': {\n",
        "            'description': 'Configuration 2D - slice par slice',\n",
        "            'patch_size': [256, 256],\n",
        "            'batch_size': 12,\n",
        "            'recommended_for': 'Images avec peu de contexte 3D'\n",
        "        },\n",
        "        '3d_fullres': {\n",
        "            'description': 'Configuration 3D pleine résolution',\n",
        "            'patch_size': [128, 128, 64] if avg_shape[2] < 100 else [96, 96, 96],\n",
        "            'batch_size': 2,\n",
        "            'recommended_for': 'Images 3D de taille modérée'\n",
        "        },\n",
        "        '3d_lowres': {\n",
        "            'description': 'Configuration 3D basse résolution',\n",
        "            'patch_size': [64, 64, 64],\n",
        "            'batch_size': 4,\n",
        "            'recommended_for': 'Images 3D de grande taille'\n",
        "        },\n",
        "        '3d_cascade': {\n",
        "            'description': 'Configuration cascade (lowres → fullres)',\n",
        "            'patch_size': 'Variable selon étape',\n",
        "            'batch_size': 'Variable selon étape', \n",
        "            'recommended_for': 'Images très grandes, meilleure précision'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nConfigurations automatiques générées:\")\n",
        "    for config_name, config in configurations.items():\n",
        "        print(f\"\\n  {config_name.upper()}:\")\n",
        "        print(f\"    Description: {config['description']}\")\n",
        "        print(f\"    Taille de patch: {config['patch_size']}\")\n",
        "        print(f\"    Taille de batch: {config['batch_size']}\")\n",
        "        print(f\"    Recommandé pour: {config['recommended_for']}\")\n",
        "    \n",
        "    # Création du dossier preprocessed\n",
        "    preprocessed_folder = folders['nnUNet_preprocessed'] / f\"Dataset{dataset_id:03d}_Hippocampus\"\n",
        "    preprocessed_folder.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Sauvegarde des configurations\n",
        "    config_file = preprocessed_folder / \"nnUNetPlans.json\"\n",
        "    \n",
        "    plans = {\n",
        "        \"dataset_name\": f\"Dataset{dataset_id:03d}_Hippocampus\",\n",
        "        \"plans_name\": \"nnUNetPlans\",\n",
        "        \"original_median_spacing_after_transp\": list(avg_spacing),\n",
        "        \"original_median_shape_after_transp\": list(avg_shape),\n",
        "        \"image_reader_writer\": \"SimpleITKIO\",\n",
        "        \"transpose_forward\": [0, 1, 2],\n",
        "        \"transpose_backward\": [0, 1, 2],\n",
        "        \"configurations\": configurations\n",
        "    }\n",
        "    \n",
        "    with open(config_file, 'w') as f:\n",
        "        json.dump(plans, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nFichier de configuration sauvegardé: {config_file}\")\n",
        "    \n",
        "    return plans, preprocessed_folder\n",
        "\n",
        "# Exécution du preprocessing\n",
        "dataset_id = 4\n",
        "plans, preprocessed_folder = run_nnunet_preprocessing(dataset_id, dataset_paths['dataset_folder'])\n",
        "\n",
        "print(f\"\\nPréprocessing terminé.\")\n",
        "print(f\"Configuration recommandée pour ce dataset: 3d_fullres\")\n",
        "print(f\"Raison: Images de taille modérée ({plans['original_median_shape_after_transp']})\")\n",
        "\n",
        "# Simulation du préprocessing des données\n",
        "def simulate_data_preprocessing(source_folder, target_folder, configuration='3d_fullres'):\n",
        "    \"\"\"\n",
        "    Simule le préprocessing des données (normalization, resampling, etc.)\n",
        "    \"\"\"\n",
        "    print(f\"\\nSimulation du préprocessing des données ({configuration})...\")\n",
        "    \n",
        "    # Création des dossiers de sortie\n",
        "    config_folder = target_folder / configuration\n",
        "    config_folder.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Simulation des fichiers préprocessés\n",
        "    image_files = list((source_folder / \"imagesTr\").glob('*.nii.gz'))\n",
        "    \n",
        "    preprocessed_info = {\n",
        "        'configuration': configuration,\n",
        "        'num_files_processed': len(image_files),\n",
        "        'normalization': 'Z-score normalization',\n",
        "        'resampling': 'Isotropic 1mm spacing',\n",
        "        'cropping': 'Non-zero region extraction',\n",
        "        'output_format': 'NPZ compressed arrays'\n",
        "    }\n",
        "    \n",
        "    # Sauvegarde des informations de preprocessing\n",
        "    info_file = config_folder / \"preprocessing_info.json\"\n",
        "    with open(info_file, 'w') as f:\n",
        "        json.dump(preprocessed_info, f, indent=2)\n",
        "    \n",
        "    print(f\"  Fichiers traités: {preprocessed_info['num_files_processed']}\")\n",
        "    print(f\"  Normalisation: {preprocessed_info['normalization']}\")\n",
        "    print(f\"  Rééchantillonnage: {preprocessed_info['resampling']}\")\n",
        "    print(f\"  Format de sortie: {preprocessed_info['output_format']}\")\n",
        "    \n",
        "    return preprocessed_info\n",
        "\n",
        "# Simulation du preprocessing\n",
        "preprocessing_info = simulate_data_preprocessing(\n",
        "    dataset_paths['dataset_folder'], \n",
        "    preprocessed_folder,\n",
        "    '3d_fullres'\n",
        ")\n",
        "\n",
        "print(\"\\nPhase de préprocessing complétée.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Entraînement du Modèle nnU-Net\n",
        "\n",
        "Simulons l'entraînement nnU-Net avec monitoring des performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entraînement du modèle nnU-Net\n",
        "print(\"=== ENTRAÎNEMENT DU MODÈLE NNUNET ===\")\n",
        "\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "class nnUNetTrainingSimulator:\n",
        "    \"\"\"\n",
        "    Simule l'entraînement nnU-Net avec monitoring réaliste\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_id, configuration='3d_fullres', fold=0):\n",
        "        self.dataset_id = dataset_id\n",
        "        self.configuration = configuration\n",
        "        self.fold = fold\n",
        "        self.trainer_name = \"nnUNetTrainer\"\n",
        "        \n",
        "        # Paramètres d'entraînement simulés\n",
        "        self.max_epochs = 1000\n",
        "        self.initial_lr = 0.01\n",
        "        self.current_epoch = 0\n",
        "        \n",
        "        # Métriques d'entraînement\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.val_dice_scores = []\n",
        "        self.learning_rates = []\n",
        "        \n",
        "        # Simulation de l'architecture\n",
        "        self.network_architecture = {\n",
        "            'name': 'nnU-Net',\n",
        "            'encoder': 'ResNet encoder with instance normalization',\n",
        "            'decoder': 'U-Net decoder with skip connections',\n",
        "            'deep_supervision': True,\n",
        "            'loss_function': 'Dice + Cross-entropy',\n",
        "            'optimizer': 'SGD with momentum 0.99',\n",
        "            'data_augmentation': 'Rotation, scaling, gamma, noise'\n",
        "        }\n",
        "    \n",
        "    def simulate_training_epoch(self, epoch):\n",
        "        \"\"\"\n",
        "        Simule une epoch d'entraînement\n",
        "        \"\"\"\n",
        "        # Simulation réaliste des courbes d'apprentissage\n",
        "        \n",
        "        # Loss décroissante avec fluctuations\n",
        "        base_train_loss = 0.8 * np.exp(-epoch / 100) + 0.1\n",
        "        train_loss = base_train_loss + np.random.normal(0, 0.05)\n",
        "        \n",
        "        base_val_loss = 0.85 * np.exp(-epoch / 120) + 0.15\n",
        "        val_loss = base_val_loss + np.random.normal(0, 0.08)\n",
        "        \n",
        "        # Dice score croissant\n",
        "        base_dice = 0.95 * (1 - np.exp(-epoch / 80))\n",
        "        val_dice = base_dice + np.random.normal(0, 0.05)\n",
        "        val_dice = np.clip(val_dice, 0, 1)\n",
        "        \n",
        "        # Learning rate avec decay\n",
        "        if epoch < 100:\n",
        "            lr = self.initial_lr\n",
        "        elif epoch < 250:\n",
        "            lr = self.initial_lr * 0.1\n",
        "        else:\n",
        "            lr = self.initial_lr * 0.01\n",
        "        \n",
        "        return {\n",
        "            'epoch': epoch,\n",
        "            'train_loss': max(0.05, train_loss),\n",
        "            'val_loss': max(0.1, val_loss),\n",
        "            'val_dice': val_dice,\n",
        "            'learning_rate': lr,\n",
        "            'time_per_epoch': 2.5 + np.random.normal(0, 0.3)  # minutes\n",
        "        }\n",
        "    \n",
        "    def run_training_simulation(self, num_epochs=300, print_frequency=50):\n",
        "        \"\"\"\n",
        "        Lance la simulation d'entraînement\n",
        "        \"\"\"\n",
        "        print(f\"Démarrage entraînement nnU-Net:\")\n",
        "        print(f\"Dataset: {self.dataset_id}, Configuration: {self.configuration}, Fold: {self.fold}\")\n",
        "        print(f\"Architecture: {self.network_architecture['name']}\")\n",
        "        print(f\"Optimiseur: {self.network_architecture['optimizer']}\")\n",
        "        print(f\"Fonction de perte: {self.network_architecture['loss_function']}\")\n",
        "        print(f\"Epochs prévues: {num_epochs}\")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        best_val_dice = 0\n",
        "        best_epoch = 0\n",
        "        patience_counter = 0\n",
        "        \n",
        "        print(f\"\\n{'Epoch':<6} {'Train Loss':<12} {'Val Loss':<10} {'Val Dice':<10} {'LR':<10} {'Time/Epoch':<12}\")\n",
        "        print(\"-\" * 70)\n",
        "        \n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            # Simulation d'une epoch\n",
        "            epoch_results = self.simulate_training_epoch(epoch)\n",
        "            \n",
        "            # Stockage des métriques\n",
        "            self.train_losses.append(epoch_results['train_loss'])\n",
        "            self.val_losses.append(epoch_results['val_loss'])\n",
        "            self.val_dice_scores.append(epoch_results['val_dice'])\n",
        "            self.learning_rates.append(epoch_results['learning_rate'])\n",
        "            \n",
        "            # Vérification du meilleur modèle\n",
        "            if epoch_results['val_dice'] > best_val_dice:\n",
        "                best_val_dice = epoch_results['val_dice']\n",
        "                best_epoch = epoch\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            \n",
        "            # Affichage périodique\n",
        "            if epoch % print_frequency == 0 or epoch == 1:\n",
        "                print(f\"{epoch:<6} {epoch_results['train_loss']:<12.4f} {epoch_results['val_loss']:<10.4f} \"\n",
        "                      f\"{epoch_results['val_dice']:<10.4f} {epoch_results['learning_rate']:<10.6f} \"\n",
        "                      f\"{epoch_results['time_per_epoch']:<12.1f} min\")\n",
        "            \n",
        "            # Early stopping simulation\n",
        "            if patience_counter > 100 and epoch > 200:  # Patience de 100 epochs\n",
        "                print(f\"\\nEarly stopping à l'epoch {epoch} (patience dépassée)\")\n",
        "                break\n",
        "        \n",
        "        total_time = time.time() - start_time\n",
        "        \n",
        "        # Résumé de l'entraînement\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ENTRAÎNEMENT TERMINÉ\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Epochs complétées: {len(self.train_losses)}\")\n",
        "        print(f\"Meilleur Dice score: {best_val_dice:.4f} (epoch {best_epoch})\")\n",
        "        print(f\"Temps total (simulé): {total_time/60:.1f} minutes\")\n",
        "        print(f\"Loss finale train: {self.train_losses[-1]:.4f}\")\n",
        "        print(f\"Loss finale validation: {self.val_losses[-1]:.4f}\")\n",
        "        \n",
        "        # Sauvegarde des résultats\n",
        "        results_folder = folders['nnUNet_results'] / f\"Dataset{self.dataset_id:03d}_Hippocampus\" / \\\n",
        "                        f\"{self.trainer_name}__{self.configuration}__fold_{self.fold}\"\n",
        "        results_folder.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        training_log = {\n",
        "            'dataset_id': self.dataset_id,\n",
        "            'configuration': self.configuration,\n",
        "            'fold': self.fold,\n",
        "            'trainer': self.trainer_name,\n",
        "            'architecture': self.network_architecture,\n",
        "            'total_epochs': len(self.train_losses),\n",
        "            'best_epoch': best_epoch,\n",
        "            'best_val_dice': float(best_val_dice),\n",
        "            'final_train_loss': float(self.train_losses[-1]),\n",
        "            'final_val_loss': float(self.val_losses[-1]),\n",
        "            'train_losses': [float(x) for x in self.train_losses],\n",
        "            'val_losses': [float(x) for x in self.val_losses],\n",
        "            'val_dice_scores': [float(x) for x in self.val_dice_scores],\n",
        "            'learning_rates': [float(x) for x in self.learning_rates]\n",
        "        }\n",
        "        \n",
        "        log_file = results_folder / \"training_log.json\"\n",
        "        with open(log_file, 'w') as f:\n",
        "            json.dump(training_log, f, indent=2)\n",
        "        \n",
        "        print(f\"Log d'entraînement sauvegardé: {log_file}\")\n",
        "        \n",
        "        return training_log, results_folder\n",
        "\n",
        "# Lancement de l'entraînement simulé\n",
        "trainer = nnUNetTrainingSimulator(dataset_id=4, configuration='3d_fullres', fold=0)\n",
        "training_log, results_folder = trainer.run_training_simulation(num_epochs=250, print_frequency=50)\n",
        "\n",
        "print(f\"\\nEntraînement nnU-Net simulé terminé.\")\n",
        "print(f\"Modèle entraîné disponible dans: {results_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualisation des Performances d'Entraînement\n",
        "\n",
        "Analysons les courbes d'apprentissage et les métriques de performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisation des performances d'entraînement\n",
        "print(\"=== VISUALISATION DES PERFORMANCES ===\")\n",
        "\n",
        "def plot_training_metrics(training_log):\n",
        "    \"\"\"\n",
        "    Crée des graphiques de performance d'entraînement\n",
        "    \"\"\"\n",
        "    epochs = list(range(1, len(training_log['train_losses']) + 1))\n",
        "    \n",
        "    # Configuration de la figure\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle(f\"Performances d'Entraînement nnU-Net - Dataset {training_log['dataset_id']}\", \n",
        "                fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Courbes de loss\n",
        "    axes[0, 0].plot(epochs, training_log['train_losses'], 'b-', label='Training Loss', alpha=0.8)\n",
        "    axes[0, 0].plot(epochs, training_log['val_losses'], 'r-', label='Validation Loss', alpha=0.8)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Évolution des Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    axes[0, 0].set_yscale('log')\n",
        "    \n",
        "    # 2. Score Dice de validation\n",
        "    axes[0, 1].plot(epochs, training_log['val_dice_scores'], 'g-', label='Validation Dice', linewidth=2)\n",
        "    axes[0, 1].axhline(y=training_log['best_val_dice'], color='r', linestyle='--', \n",
        "                      label=f'Meilleur: {training_log[\"best_val_dice\"]:.4f}')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Dice Score')\n",
        "    axes[0, 1].set_title('Score Dice de Validation')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    axes[0, 1].set_ylim(0, 1)\n",
        "    \n",
        "    # 3. Learning Rate\n",
        "    axes[1, 0].plot(epochs, training_log['learning_rates'], 'orange', linewidth=2)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Learning Rate')\n",
        "    axes[1, 0].set_title('Évolution du Learning Rate')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    axes[1, 0].set_yscale('log')\n",
        "    \n",
        "    # 4. Comparaison Train vs Validation Loss\n",
        "    loss_diff = np.array(training_log['val_losses']) - np.array(training_log['train_losses'])\n",
        "    axes[1, 1].plot(epochs, loss_diff, 'purple', linewidth=2)\n",
        "    axes[1, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Val Loss - Train Loss')\n",
        "    axes[1, 1].set_title('Écart Train/Validation (Overfitting)')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Coloration selon overfitting\n",
        "    axes[1, 1].fill_between(epochs, loss_diff, 0, \n",
        "                           where=(loss_diff > 0), color='red', alpha=0.3, label='Overfitting')\n",
        "    axes[1, 1].fill_between(epochs, loss_diff, 0, \n",
        "                           where=(loss_diff <= 0), color='green', alpha=0.3, label='Bon apprentissage')\n",
        "    axes[1, 1].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Analyse des performances\n",
        "    print(f\"\\nAnalyse des performances:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Stabilité de l'entraînement\n",
        "    final_epochs = training_log['val_dice_scores'][-50:]  # Dernières 50 epochs\n",
        "    stability = np.std(final_epochs) if len(final_epochs) >= 10 else np.std(training_log['val_dice_scores'])\n",
        "    \n",
        "    print(f\"Stabilité finale (std): {stability:.4f}\")\n",
        "    if stability < 0.01:\n",
        "        print(\"✓ Entraînement stable\")\n",
        "    elif stability < 0.03:\n",
        "        print(\"⚠ Entraînement moyennement stable\")\n",
        "    else:\n",
        "        print(\"✗ Entraînement instable\")\n",
        "    \n",
        "    # Convergence\n",
        "    improvement_rate = (training_log['val_dice_scores'][-1] - training_log['val_dice_scores'][0]) / len(epochs)\n",
        "    print(f\"\\nTaux d'amélioration: {improvement_rate:.6f} Dice/epoch\")\n",
        "    \n",
        "    if training_log['val_dice_scores'][-1] > 0.85:\n",
        "        print(\"✓ Performance excellente (Dice > 0.85)\")\n",
        "    elif training_log['val_dice_scores'][-1] > 0.75:\n",
        "        print(\"✓ Performance bonne (Dice > 0.75)\")\n",
        "    elif training_log['val_dice_scores'][-1] > 0.60:\n",
        "        print(\"⚠ Performance modérée (Dice > 0.60)\")\n",
        "    else:\n",
        "        print(\"✗ Performance faible (Dice < 0.60)\")\n",
        "    \n",
        "    # Overfitting\n",
        "    final_gap = training_log['val_losses'][-1] - training_log['train_losses'][-1]\n",
        "    print(f\"\\nÉcart final train/val: {final_gap:.4f}\")\n",
        "    if final_gap < 0.05:\n",
        "        print(\"✓ Pas d'overfitting détecté\")\n",
        "    elif final_gap < 0.15:\n",
        "        print(\"⚠ Léger overfitting\")\n",
        "    else:\n",
        "        print(\"✗ Overfitting significatif\")\n",
        "\n",
        "# Création des visualisations\n",
        "plot_training_metrics(training_log)\n",
        "\n",
        "# Tableau récapitulatif des configurations\n",
        "def create_performance_summary(training_log):\n",
        "    \"\"\"\n",
        "    Crée un résumé des performances\n",
        "    \"\"\"\n",
        "    summary_data = {\n",
        "        'Configuration': training_log['configuration'],\n",
        "        'Dataset': f\"Dataset{training_log['dataset_id']:03d}\",\n",
        "        'Fold': training_log['fold'],\n",
        "        'Epochs': training_log['total_epochs'],\n",
        "        'Meilleur Dice': f\"{training_log['best_val_dice']:.4f}\",\n",
        "        'Epoch optimal': training_log['best_epoch'],\n",
        "        'Loss finale (train)': f\"{training_log['final_train_loss']:.4f}\",\n",
        "        'Loss finale (val)': f\"{training_log['final_val_loss']:.4f}\",\n",
        "        'Architecture': training_log['architecture']['name']\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nRésumé de Performance:\")\n",
        "    print(f\"{'='*40}\")\n",
        "    for key, value in summary_data.items():\n",
        "        print(f\"{key:<20}: {value}\")\n",
        "    \n",
        "    return summary_data\n",
        "\n",
        "performance_summary = create_performance_summary(training_log)\n",
        "\n",
        "print(\"\\nVisualisation des performances terminée.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inférence et Post-traitement\n",
        "\n",
        "Utilisons le modèle entraîné pour faire des prédictions sur de nouvelles données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inférence et post-traitement nnU-Net\n",
        "print(\"=== INFÉRENCE ET POST-TRAITEMENT ===\")\n",
        "\n",
        "class nnUNetPredictor:\n",
        "    \"\"\"\n",
        "    Simulateur de prédiction nnU-Net\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_folder, configuration='3d_fullres'):\n",
        "        self.model_folder = model_folder\n",
        "        self.configuration = configuration\n",
        "        self.postprocessing_enabled = True\n",
        "        \n",
        "    def predict_single_case(self, input_image_path, output_folder):\n",
        "        \"\"\"\n",
        "        Prédiction sur un cas individuel\n",
        "        \"\"\"\n",
        "        print(f\"Prédiction pour: {input_image_path.name}\")\n",
        "        \n",
        "        try:\n",
        "            # Chargement de l'image\n",
        "            nifti_img = nib.load(input_image_path)\n",
        "            image_data = nifti_img.get_fdata()\n",
        "            \n",
        "            print(f\"  Image chargée: {image_data.shape}\")\n",
        "            \n",
        "            # Simulation du preprocessing\n",
        "            print(f\"  Preprocessing...\")\n",
        "            # Normalisation Z-score\n",
        "            mean_val = np.mean(image_data[image_data > 0])\n",
        "            std_val = np.std(image_data[image_data > 0])\n",
        "            normalized_image = (image_data - mean_val) / (std_val + 1e-8)\n",
        "            \n",
        "            # Simulation de la prédiction\n",
        "            print(f\"  Inférence du modèle...\")\n",
        "            \n",
        "            # Création d'une segmentation simulée réaliste\n",
        "            prediction = self._simulate_hippocampus_prediction(image_data)\n",
        "            \n",
        "            # Post-traitement\n",
        "            if self.postprocessing_enabled:\n",
        "                print(f\"  Post-traitement...\")\n",
        "                prediction = self._apply_postprocessing(prediction)\n",
        "            \n",
        "            # Sauvegarde\n",
        "            output_path = output_folder / f\"{input_image_path.stem.replace('_0000', '')}_predicted.nii.gz\"\n",
        "            output_folder.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            predicted_nifti = nib.Nifti1Image(prediction.astype(np.uint8), nifti_img.affine)\n",
        "            nib.save(predicted_nifti, output_path)\n",
        "            \n",
        "            # Calcul de métriques de qualité\n",
        "            metrics = self._calculate_prediction_metrics(prediction)\n",
        "            \n",
        "            print(f\"  ✓ Prédiction sauvegardée: {output_path}\")\n",
        "            print(f\"  Volume hippocampe gauche: {metrics['left_volume']} voxels\")\n",
        "            print(f\"  Volume hippocampe droit: {metrics['right_volume']} voxels\")\n",
        "            print(f\"  Score de confiance: {metrics['confidence']:.3f}\")\n",
        "            \n",
        "            return {\n",
        "                'input_path': input_image_path,\n",
        "                'output_path': output_path,\n",
        "                'prediction_shape': prediction.shape,\n",
        "                'metrics': metrics\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Erreur lors de la prédiction: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def _simulate_hippocampus_prediction(self, image_data):\n",
        "        \"\"\"\n",
        "        Simule une prédiction d'hippocampe réaliste\n",
        "        \"\"\"\n",
        "        shape = image_data.shape\n",
        "        prediction = np.zeros(shape, dtype=np.uint8)\n",
        "        \n",
        "        # Coordonnées pour simulation réaliste\n",
        "        x, y, z = np.meshgrid(\n",
        "            np.linspace(-1, 1, shape[0]),\n",
        "            np.linspace(-1, 1, shape[1]),\n",
        "            np.linspace(-1, 1, shape[2]),\n",
        "            indexing='ij'\n",
        "        )\n",
        "        \n",
        "        # Hippocampe gauche avec forme réaliste\n",
        "        left_hippo = (\n",
        "            (x + 0.3)**2 * 4 + (y - 0.1)**2 * 8 + z**2 * 12 < 0.15\n",
        "        ) & (image_data > np.percentile(image_data[image_data > 0], 20))\n",
        "        \n",
        "        # Hippocampe droit\n",
        "        right_hippo = (\n",
        "            (x - 0.3)**2 * 4 + (y - 0.1)**2 * 8 + z**2 * 12 < 0.15\n",
        "        ) & (image_data > np.percentile(image_data[image_data > 0], 20))\n",
        "        \n",
        "        # Ajout de bruit réaliste (erreurs de segmentation)\n",
        "        noise_mask = np.random.random(shape) < 0.02  # 2% de pixels bruités\n",
        "        left_hippo = left_hippo & ~noise_mask\n",
        "        right_hippo = right_hippo & ~noise_mask\n",
        "        \n",
        "        prediction[left_hippo] = 1\n",
        "        prediction[right_hippo] = 2\n",
        "        \n",
        "        return prediction\n",
        "    \n",
        "    def _apply_postprocessing(self, prediction):\n",
        "        \"\"\"\n",
        "        Applique le post-traitement (nettoyage, connexité)\n",
        "        \"\"\"\n",
        "        from scipy import ndimage\n",
        "        from skimage import morphology, measure\n",
        "        \n",
        "        processed = prediction.copy()\n",
        "        \n",
        "        # Post-traitement par label\n",
        "        for label in [1, 2]:  # Hippocampe gauche et droit\n",
        "            mask = (prediction == label)\n",
        "            \n",
        "            if np.any(mask):\n",
        "                # Fermeture morphologique\n",
        "                mask = ndimage.binary_closing(mask, structure=np.ones((3, 3, 3)))\n",
        "                \n",
        "                # Suppression des petites composantes\n",
        "                labeled_mask = measure.label(mask)\n",
        "                props = measure.regionprops(labeled_mask)\n",
        "                \n",
        "                if props:\n",
        "                    # Garder seulement la plus grande composante\n",
        "                    largest_component = max(props, key=lambda x: x.area)\n",
        "                    mask = (labeled_mask == largest_component.label)\n",
        "                \n",
        "                processed[prediction == label] = 0\n",
        "                processed[mask] = label\n",
        "        \n",
        "        return processed\n",
        "    \n",
        "    def _calculate_prediction_metrics(self, prediction):\n",
        "        \"\"\"\n",
        "        Calcule des métriques de qualité de prédiction\n",
        "        \"\"\"\n",
        "        left_volume = np.sum(prediction == 1)\n",
        "        right_volume = np.sum(prediction == 2)\n",
        "        total_volume = left_volume + right_volume\n",
        "        \n",
        "        # Score de confiance basé sur la régularité\n",
        "        if total_volume > 0:\n",
        "            # Compacité des régions\n",
        "            props_left = measure.regionprops((prediction == 1).astype(int))\n",
        "            props_right = measure.regionprops((prediction == 2).astype(int))\n",
        "            \n",
        "            compactness = 0\n",
        "            if props_left:\n",
        "                compactness += props_left[0].solidity\n",
        "            if props_right:\n",
        "                compactness += props_right[0].solidity\n",
        "            \n",
        "            confidence = compactness / (2 if props_left and props_right else 1)\n",
        "        else:\n",
        "            confidence = 0\n",
        "        \n",
        "        return {\n",
        "            'left_volume': left_volume,\n",
        "            'right_volume': right_volume,\n",
        "            'total_volume': total_volume,\n",
        "            'volume_ratio': left_volume / right_volume if right_volume > 0 else 0,\n",
        "            'confidence': confidence\n",
        "        }\n",
        "    \n",
        "    def predict_folder(self, input_folder, output_folder):\n",
        "        \"\"\"\n",
        "        Prédiction sur un dossier d'images\n",
        "        \"\"\"\n",
        "        input_files = list(input_folder.glob('*_0000.nii.gz'))\n",
        "        \n",
        "        if not input_files:\n",
        "            print(f\"Aucun fichier d'entrée trouvé dans {input_folder}\")\n",
        "            return []\n",
        "        \n",
        "        print(f\"Prédiction sur {len(input_files)} fichiers...\")\n",
        "        \n",
        "        results = []\n",
        "        for img_file in input_files:\n",
        "            result = self.predict_single_case(img_file, output_folder)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Initialisation du prédicteur\n",
        "predictor = nnUNetPredictor(results_folder, configuration='3d_fullres')\n",
        "\n",
        "# Test de prédiction sur les données d'entraînement\n",
        "images_folder = dataset_paths['dataset_folder'] / 'imagesTr'\n",
        "predictions_folder = results_folder / 'predictions'\n",
        "\n",
        "print(f\"Test de prédiction sur le dataset d'entraînement...\")\n",
        "prediction_results = predictor.predict_folder(images_folder, predictions_folder)\n",
        "\n",
        "print(f\"\\nPrédictions terminées: {len(prediction_results)} cas traités\")\n",
        "\n",
        "# Analyse des résultats de prédiction\n",
        "if prediction_results:\n",
        "    print(f\"\\nAnalyse des résultats:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    volumes_left = [r['metrics']['left_volume'] for r in prediction_results]\n",
        "    volumes_right = [r['metrics']['right_volume'] for r in prediction_results]\n",
        "    confidences = [r['metrics']['confidence'] for r in prediction_results]\n",
        "    \n",
        "    print(f\"Volume hippocampe gauche:\")\n",
        "    print(f\"  Moyenne: {np.mean(volumes_left):.0f} ± {np.std(volumes_left):.0f} voxels\")\n",
        "    print(f\"  Min/Max: {min(volumes_left)}/{max(volumes_left)} voxels\")\n",
        "    \n",
        "    print(f\"\\nVolume hippocampe droit:\")\n",
        "    print(f\"  Moyenne: {np.mean(volumes_right):.0f} ± {np.std(volumes_right):.0f} voxels\")\n",
        "    print(f\"  Min/Max: {min(volumes_right)}/{max(volumes_right)} voxels\")\n",
        "    \n",
        "    print(f\"\\nConfiance moyenne: {np.mean(confidences):.3f}\")\n",
        "    \n",
        "    # Détection d'anomalies volumétriques\n",
        "    mean_vol_left = np.mean(volumes_left)\n",
        "    mean_vol_right = np.mean(volumes_right)\n",
        "    \n",
        "    asymmetry_ratio = abs(mean_vol_left - mean_vol_right) / max(mean_vol_left, mean_vol_right)\n",
        "    print(f\"\\nAsymétrie hippocampique: {asymmetry_ratio:.3f}\")\n",
        "    \n",
        "    if asymmetry_ratio < 0.1:\n",
        "        print(\"✓ Symétrie normale\")\n",
        "    elif asymmetry_ratio < 0.3:\n",
        "        print(\"⚠ Asymétrie légère\")\n",
        "    else:\n",
        "        print(\"⚠ Asymétrie significative - investigation recommandée\")\n",
        "\n",
        "print(f\"\\nInférence nnU-Net terminée.\")\n",
        "print(f\"Prédictions disponibles dans: {predictions_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Intégration DICOM et Workflows Cliniques\n",
        "\n",
        "Intégrons nnU-Net dans des workflows hospitaliers réels avec gestion DICOM et connexion PACS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Intégration DICOM et workflows cliniques\n",
        "print(\"=== INTÉGRATION DICOM ET WORKFLOWS CLINIQUES ===\")\n",
        "\n",
        "import pydicom\n",
        "from datetime import datetime\n",
        "\n",
        "class ClinicalWorkflowIntegrator:\n",
        "    \"\"\"\n",
        "    Intègre nnU-Net dans les workflows hospitaliers\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, nnunet_predictor, pacs_config=None):\n",
        "        self.predictor = nnunet_predictor\n",
        "        self.pacs_config = pacs_config or {\n",
        "            'host': '192.168.1.100',\n",
        "            'port': 104,\n",
        "            'aec': 'HOSPITAL_PACS',\n",
        "            'aet': 'NNUNET_AI'\n",
        "        }\n",
        "        \n",
        "        # Configuration clinique\n",
        "        self.clinical_config = {\n",
        "            'auto_processing': True,\n",
        "            'quality_checks': True,\n",
        "            'radiologist_review': True,\n",
        "            'report_generation': True\n",
        "        }\n",
        "    \n",
        "    def create_dicom_metadata(self, patient_info, study_info, series_info):\n",
        "        \"\"\"\n",
        "        Crée les métadonnées DICOM pour les segmentations nnU-Net\n",
        "        \"\"\"\n",
        "        metadata = {\n",
        "            # Informations patient\n",
        "            'PatientName': patient_info.get('name', 'ANONYMOUS'),\n",
        "            'PatientID': patient_info.get('id', 'AI_PATIENT_001'),\n",
        "            'PatientBirthDate': patient_info.get('birth_date', '19800101'),\n",
        "            'PatientSex': patient_info.get('sex', 'O'),\n",
        "            \n",
        "            # Informations étude\n",
        "            'StudyInstanceUID': study_info.get('study_uid', '1.2.3.4.5.6.7.8.9.1'),\n",
        "            'StudyDate': study_info.get('study_date', datetime.now().strftime('%Y%m%d')),\n",
        "            'StudyTime': study_info.get('study_time', datetime.now().strftime('%H%M%S')),\n",
        "            'StudyDescription': study_info.get('description', 'Brain MRI with nnU-Net Segmentation'),\n",
        "            'AccessionNumber': study_info.get('accession', 'AI001'),\n",
        "            \n",
        "            # Informations série\n",
        "            'SeriesInstanceUID': series_info.get('series_uid', '1.2.3.4.5.6.7.8.9.2'),\n",
        "            'SeriesNumber': series_info.get('series_number', 999),\n",
        "            'SeriesDescription': series_info.get('description', 'nnU-Net Hippocampus Segmentation'),\n",
        "            'Modality': 'SEG',  # Segmentation Object\n",
        "            \n",
        "            # Informations IA\n",
        "            'ManufacturerModelName': 'nnU-Net v2.0',\n",
        "            'SoftwareVersions': 'nnU-Net 2.0 + Custom Clinical Integration',\n",
        "            'DeviceSerialNumber': 'NNUNET_CLINICAL_001'\n",
        "        }\n",
        "        \n",
        "        return metadata\n",
        "    \n",
        "    def process_dicom_study(self, dicom_folder, output_folder):\n",
        "        \"\"\"\n",
        "        Traite une étude DICOM complète\n",
        "        \"\"\"\n",
        "        print(f\"Traitement de l'étude DICOM: {dicom_folder}\")\n",
        "        \n",
        "        # Simulation de lecture DICOM\n",
        "        dicom_files = list(Path(dicom_folder).glob('*.dcm')) if Path(dicom_folder).exists() else []\n",
        "        \n",
        "        if not dicom_files:\n",
        "            print(\"  Aucun fichier DICOM trouvé, utilisation de données simulées\")\n",
        "            # Simulation d'une étude\n",
        "            study_results = self._simulate_dicom_study()\n",
        "        else:\n",
        "            study_results = self._process_real_dicom_study(dicom_files, output_folder)\n",
        "        \n",
        "        return study_results\n",
        "    \n",
        "    def _simulate_dicom_study(self):\n",
        "        \"\"\"\n",
        "        Simule le traitement d'une étude DICOM\n",
        "        \"\"\"\n",
        "        print(\"  Simulation d'une étude DICOM...\")\n",
        "        \n",
        "        # Informations patient simulées\n",
        "        patient_info = {\n",
        "            'name': 'DOE^JOHN',\n",
        "            'id': 'PAT001',\n",
        "            'birth_date': '19750315',\n",
        "            'sex': 'M',\n",
        "            'age': 48\n",
        "        }\n",
        "        \n",
        "        study_info = {\n",
        "            'study_uid': f\"1.2.826.0.1.3680043.8.498.{int(time.time())}\",\n",
        "            'study_date': datetime.now().strftime('%Y%m%d'),\n",
        "            'study_time': datetime.now().strftime('%H%M%S'),\n",
        "            'description': 'IRM cérébrale - Suspicion atrophie hippocampique',\n",
        "            'accession': f\"ACC{datetime.now().strftime('%Y%m%d%H%M')}\"\n",
        "        }\n",
        "        \n",
        "        # Simulation du processus clinique\n",
        "        workflow_steps = [\n",
        "            'Réception de l\\'étude depuis PACS',\n",
        "            'Conversion DICOM vers NIfTI',\n",
        "            'Préprocessing nnU-Net', \n",
        "            'Inférence de segmentation',\n",
        "            'Post-traitement et validation qualité',\n",
        "            'Génération du rapport IA',\n",
        "            'Intégration dans le PACS'\n",
        "        ]\n",
        "        \n",
        "        print(f\"  Patient: {patient_info['name']} (ID: {patient_info['id']})\")\n",
        "        print(f\"  Étude: {study_info['description']}\")\n",
        "        print(f\"  Numéro d'accession: {study_info['accession']}\")\n",
        "        \n",
        "        for i, step in enumerate(workflow_steps, 1):\n",
        "            print(f\"  {i}. {step}\")\n",
        "            time.sleep(0.1)  # Simulation du temps de traitement\n",
        "        \n",
        "        # Simulation des résultats\n",
        "        segmentation_results = {\n",
        "            'hippocampus_left_volume': 2850.5,  # mm³\n",
        "            'hippocampus_right_volume': 3020.2,  # mm³\n",
        "            'total_volume': 5870.7,\n",
        "            'asymmetry_index': 0.058,\n",
        "            'confidence_score': 0.94,\n",
        "            'processing_time': 45.2  # secondes\n",
        "        }\n",
        "        \n",
        "        # Évaluation clinique automatique\n",
        "        clinical_assessment = self._generate_clinical_assessment(segmentation_results, patient_info)\n",
        "        \n",
        "        return {\n",
        "            'patient_info': patient_info,\n",
        "            'study_info': study_info,\n",
        "            'segmentation_results': segmentation_results,\n",
        "            'clinical_assessment': clinical_assessment,\n",
        "            'workflow_status': 'completed',\n",
        "            'processing_timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "    \n",
        "    def _generate_clinical_assessment(self, results, patient_info):\n",
        "        \"\"\"\n",
        "        Génère une évaluation clinique automatique\n",
        "        \"\"\"\n",
        "        assessment = {\n",
        "            'volumetric_analysis': {},\n",
        "            'normative_comparison': {},\n",
        "            'clinical_interpretation': {},\n",
        "            'recommendations': []\n",
        "        }\n",
        "        \n",
        "        # Analyse volumétrique\n",
        "        left_vol = results['hippocampus_left_volume']\n",
        "        right_vol = results['hippocampus_right_volume']\n",
        "        asymmetry = results['asymmetry_index']\n",
        "        \n",
        "        assessment['volumetric_analysis'] = {\n",
        "            'left_hippocampus': f\"{left_vol:.1f} mm³\",\n",
        "            'right_hippocampus': f\"{right_vol:.1f} mm³\",\n",
        "            'total_volume': f\"{results['total_volume']:.1f} mm³\",\n",
        "            'asymmetry_index': f\"{asymmetry:.3f}\"\n",
        "        }\n",
        "        \n",
        "        # Comparaison normative (valeurs de référence)\n",
        "        age = patient_info['age']\n",
        "        sex = patient_info['sex']\n",
        "        \n",
        "        # Valeurs normatives approximatives\n",
        "        if sex == 'M':\n",
        "            expected_volume = 3200 - (age - 30) * 15  # Déclin avec l'âge\n",
        "        else:\n",
        "            expected_volume = 2900 - (age - 30) * 12\n",
        "        \n",
        "        volume_percentile = ((left_vol + right_vol) / 2) / expected_volume * 100\n",
        "        \n",
        "        assessment['normative_comparison'] = {\n",
        "            'expected_volume_range': f\"{expected_volume-200:.0f}-{expected_volume+200:.0f} mm³\",\n",
        "            'patient_percentile': f\"{volume_percentile:.0f}%\",\n",
        "            'age_sex_adjusted': 'Appliqué'\n",
        "        }\n",
        "        \n",
        "        # Interprétation clinique\n",
        "        if volume_percentile < 10:\n",
        "            severity = 'Atrophie sévère'\n",
        "            priority = 'Élevée'\n",
        "        elif volume_percentile < 25:\n",
        "            severity = 'Atrophie modérée'\n",
        "            priority = 'Modérée'\n",
        "        elif volume_percentile < 75:\n",
        "            severity = 'Normal'\n",
        "            priority = 'Routine'\n",
        "        else:\n",
        "            severity = 'Au-dessus de la normale'\n",
        "            priority = 'Routine'\n",
        "        \n",
        "        if asymmetry > 0.15:\n",
        "            asymmetry_status = 'Asymétrie significative'\n",
        "        elif asymmetry > 0.10:\n",
        "            asymmetry_status = 'Asymétrie légère'\n",
        "        else:\n",
        "            asymmetry_status = 'Symétrie normale'\n",
        "        \n",
        "        assessment['clinical_interpretation'] = {\n",
        "            'volume_assessment': severity,\n",
        "            'asymmetry_assessment': asymmetry_status,\n",
        "            'clinical_priority': priority,\n",
        "            'confidence_level': 'Élevée' if results['confidence_score'] > 0.9 else 'Modérée'\n",
        "        }\n",
        "        \n",
        "        # Recommandations\n",
        "        if severity in ['Atrophie sévère', 'Atrophie modérée']:\n",
        "            assessment['recommendations'].extend([\n",
        "                'Corrélation avec évaluation neuropsychologique',\n",
        "                'Consultation neurologie/gériatrie', \n",
        "                'Suivi volumétrique à 6-12 mois'\n",
        "            ])\n",
        "        \n",
        "        if asymmetry > 0.15:\n",
        "            assessment['recommendations'].extend([\n",
        "                'Investigation étiologique de l\\'asymétrie',\n",
        "                'Recherche de lésions focales'\n",
        "            ])\n",
        "        \n",
        "        assessment['recommendations'].append('Validation radiologique requise')\n",
        "        \n",
        "        return assessment\n",
        "    \n",
        "    def generate_clinical_report(self, study_data):\n",
        "        \"\"\"\n",
        "        Génère un rapport clinique structuré\n",
        "        \"\"\"\n",
        "        patient = study_data['patient_info']\n",
        "        study = study_data['study_info']\n",
        "        results = study_data['segmentation_results']\n",
        "        assessment = study_data['clinical_assessment']\n",
        "        \n",
        "        report = f\"\"\"\n",
        "RAPPORT D'ANALYSE VOLUMÉTRIQUE IA - HIPPOCAMPE\n",
        "{'='*60}\n",
        "\n",
        "INFORMATIONS PATIENT:\n",
        "Nom: {patient['name']}\n",
        "ID: {patient['id']}\n",
        "Âge: {patient['age']} ans\n",
        "Sexe: {patient['sex']}\n",
        "\n",
        "INFORMATIONS ÉTUDE:\n",
        "Date: {study['study_date']}\n",
        "Description: {study['description']}\n",
        "N° Accession: {study['accession']}\n",
        "\n",
        "RÉSULTATS DE SEGMENTATION IA:\n",
        "Système: nnU-Net v2.0 + Intégration Clinique\n",
        "Temps de traitement: {results['processing_time']:.1f} secondes\n",
        "Score de confiance: {results['confidence_score']:.3f}\n",
        "\n",
        "ANALYSE VOLUMÉTRIQUE:\n",
        "• Hippocampe gauche: {assessment['volumetric_analysis']['left_hippocampus']}\n",
        "• Hippocampe droit: {assessment['volumetric_analysis']['right_hippocampus']}\n",
        "• Volume total: {assessment['volumetric_analysis']['total_volume']}\n",
        "• Index d'asymétrie: {assessment['volumetric_analysis']['asymmetry_index']}\n",
        "\n",
        "COMPARAISON NORMATIVE:\n",
        "• Plage attendue (âge/sexe): {assessment['normative_comparison']['expected_volume_range']}\n",
        "• Percentile patient: {assessment['normative_comparison']['patient_percentile']}\n",
        "• Ajustement: {assessment['normative_comparison']['age_sex_adjusted']}\n",
        "\n",
        "INTERPRÉTATION CLINIQUE:\n",
        "• Évaluation volumétrique: {assessment['clinical_interpretation']['volume_assessment']}\n",
        "• Évaluation asymétrie: {assessment['clinical_interpretation']['asymmetry_assessment']}\n",
        "• Priorité clinique: {assessment['clinical_interpretation']['clinical_priority']}\n",
        "• Niveau de confiance: {assessment['clinical_interpretation']['confidence_level']}\n",
        "\n",
        "RECOMMANDATIONS:\n",
        "\"\"\"\n",
        "        \n",
        "        for i, recommendation in enumerate(assessment['recommendations'], 1):\n",
        "            report += f\"{i}. {recommendation}\\n\"\n",
        "        \n",
        "        report += f\"\"\"\n",
        "\n",
        "AVERTISSEMENTS:\n",
        "• Cette analyse est générée automatiquement par IA\n",
        "• Validation par radiologue senior requise\n",
        "• Corrélation avec examen clinique indispensable\n",
        "• L'IA est un outil d'aide au diagnostic, non substitutif\n",
        "\n",
        "{'='*60}\n",
        "Rapport généré le: {datetime.now().strftime('%d/%m/%Y à %H:%M:%S')}\n",
        "Système: nnU-Net Clinical Integration v1.0\n",
        "{'='*60}\n",
        "\"\"\"\n",
        "        \n",
        "        return report\n",
        "\n",
        "# Initialisation de l'intégrateur clinique\n",
        "clinical_integrator = ClinicalWorkflowIntegrator(predictor)\n",
        "\n",
        "# Simulation d'un workflow clinique complet\n",
        "print(\"Test du workflow clinique intégré...\")\n",
        "study_data = clinical_integrator.process_dicom_study(\"./dicom_input\", \"./dicom_output\")\n",
        "\n",
        "# Génération du rapport clinique\n",
        "clinical_report = clinical_integrator.generate_clinical_report(study_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RAPPORT CLINIQUE GÉNÉRÉ:\")\n",
        "print(\"=\"*60)\n",
        "print(clinical_report)\n",
        "\n",
        "# Sauvegarde du rapport\n",
        "report_file = results_folder / f\"clinical_report_{study_data['study_info']['accession']}.txt\"\n",
        "with open(report_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(clinical_report)\n",
        "\n",
        "print(f\"Rapport sauvegardé: {report_file}\")\n",
        "print(\"\\nIntégration clinique nnU-Net complétée.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Déploiement et Monitoring en Production\n",
        "\n",
        "Configurons le déploiement en production avec monitoring des performances et gestion des erreurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Déploiement et monitoring en production\n",
        "print(\"=== DÉPLOIEMENT ET MONITORING EN PRODUCTION ===\")\n",
        "\n",
        "class ProductionDeploymentManager:\n",
        "    \"\"\"\n",
        "    Gestionnaire de déploiement en production pour nnU-Net\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_path, deployment_config=None):\n",
        "        self.model_path = model_path\n",
        "        self.deployment_config = deployment_config or self._default_config()\n",
        "        self.performance_metrics = {\n",
        "            'total_processed': 0,\n",
        "            'successful_predictions': 0,\n",
        "            'failed_predictions': 0,\n",
        "            'average_processing_time': 0,\n",
        "            'uptime_start': datetime.now(),\n",
        "            'last_error': None\n",
        "        }\n",
        "        self.quality_metrics = []\n",
        "        \n",
        "    def _default_config(self):\n",
        "        return {\n",
        "            'max_concurrent_jobs': 4,\n",
        "            'max_processing_time': 300,  # 5 minutes\n",
        "            'auto_restart_on_error': True,\n",
        "            'backup_frequency': 24,  # heures\n",
        "            'monitoring_interval': 60,  # secondes\n",
        "            'alert_thresholds': {\n",
        "                'error_rate': 0.05,  # 5%\n",
        "                'processing_time': 180,  # 3 minutes\n",
        "                'disk_space': 0.9  # 90%\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def health_check(self):\n",
        "        \"\"\"\n",
        "        Vérifie l'état du système\n",
        "        \"\"\"\n",
        "        checks = {\n",
        "            'model_loaded': self.model_path.exists(),\n",
        "            'gpu_available': torch.cuda.is_available(),\n",
        "            'disk_space': self._check_disk_space(),\n",
        "            'memory_usage': self._check_memory_usage(),\n",
        "            'error_rate': self._calculate_error_rate(),\n",
        "            'system_uptime': self._get_uptime()\n",
        "        }\n",
        "        \n",
        "        # Évaluation globale\n",
        "        critical_checks = ['model_loaded', 'disk_space']\n",
        "        warning_checks = ['gpu_available', 'memory_usage', 'error_rate']\n",
        "        \n",
        "        status = 'healthy'\n",
        "        issues = []\n",
        "        \n",
        "        for check in critical_checks:\n",
        "            if not checks[check]:\n",
        "                status = 'critical'\n",
        "                issues.append(f\"CRITIQUE: {check} échoué\")\n",
        "        \n",
        "        for check in warning_checks:\n",
        "            if not checks[check] and status == 'healthy':\n",
        "                status = 'warning'\n",
        "                issues.append(f\"ATTENTION: {check} problématique\")\n",
        "        \n",
        "        return {\n",
        "            'status': status,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'checks': checks,\n",
        "            'issues': issues\n",
        "        }\n",
        "    \n",
        "    def _check_disk_space(self):\n",
        "        \"\"\"Vérifie l'espace disque disponible\"\"\"\n",
        "        # Simulation\n",
        "        available_space = 0.75  # 75% disponible\n",
        "        return available_space > (1 - self.deployment_config['alert_thresholds']['disk_space'])\n",
        "    \n",
        "    def _check_memory_usage(self):\n",
        "        \"\"\"Vérifie l'usage mémoire\"\"\"\n",
        "        # Simulation\n",
        "        memory_usage = 0.65  # 65% utilisé\n",
        "        return memory_usage < 0.85\n",
        "    \n",
        "    def _calculate_error_rate(self):\n",
        "        \"\"\"Calcule le taux d'erreur\"\"\"\n",
        "        if self.performance_metrics['total_processed'] == 0:\n",
        "            return True\n",
        "        \n",
        "        error_rate = (self.performance_metrics['failed_predictions'] / \n",
        "                     self.performance_metrics['total_processed'])\n",
        "        return error_rate < self.deployment_config['alert_thresholds']['error_rate']\n",
        "    \n",
        "    def _get_uptime(self):\n",
        "        \"\"\"Calcule le temps de fonctionnement\"\"\"\n",
        "        uptime = datetime.now() - self.performance_metrics['uptime_start']\n",
        "        return str(uptime)\n",
        "    \n",
        "    def process_with_monitoring(self, input_data, case_id):\n",
        "        \"\"\"\n",
        "        Traite une prédiction avec monitoring complet\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Mise à jour des métriques\n",
        "            self.performance_metrics['total_processed'] += 1\n",
        "            \n",
        "            # Simulation du traitement\n",
        "            print(f\"Traitement du cas: {case_id}\")\n",
        "            \n",
        "            # Vérifications préalables\n",
        "            if not self._validate_input(input_data):\n",
        "                raise ValueError(\"Données d'entrée invalides\")\n",
        "            \n",
        "            # Simulation de la prédiction\n",
        "            time.sleep(np.random.uniform(1, 3))  # Temps de traitement variable\n",
        "            \n",
        "            # Génération des résultats\n",
        "            results = {\n",
        "                'case_id': case_id,\n",
        "                'predictions': {\n",
        "                    'hippocampus_left': np.random.uniform(2500, 3500),\n",
        "                    'hippocampus_right': np.random.uniform(2500, 3500)\n",
        "                },\n",
        "                'confidence': np.random.uniform(0.85, 0.99),\n",
        "                'processing_time': time.time() - start_time,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            # Validation qualité\n",
        "            quality_score = self._assess_prediction_quality(results)\n",
        "            results['quality_score'] = quality_score\n",
        "            \n",
        "            # Mise à jour des métriques de succès\n",
        "            self.performance_metrics['successful_predictions'] += 1\n",
        "            self._update_processing_time(results['processing_time'])\n",
        "            self.quality_metrics.append(quality_score)\n",
        "            \n",
        "            print(f\"  ✓ Traitement réussi - Temps: {results['processing_time']:.1f}s\")\n",
        "            print(f\"  ✓ Qualité: {quality_score:.3f}, Confiance: {results['confidence']:.3f}\")\n",
        "            \n",
        "            return results\n",
        "            \n",
        "        except Exception as e:\n",
        "            # Gestion des erreurs\n",
        "            self.performance_metrics['failed_predictions'] += 1\n",
        "            self.performance_metrics['last_error'] = {\n",
        "                'error': str(e),\n",
        "                'case_id': case_id,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            print(f\"  ✗ Erreur lors du traitement: {e}\")\n",
        "            \n",
        "            return {\n",
        "                'case_id': case_id,\n",
        "                'error': str(e),\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'processing_time': time.time() - start_time\n",
        "            }\n",
        "    \n",
        "    def _validate_input(self, input_data):\n",
        "        \"\"\"Valide les données d'entrée\"\"\"\n",
        "        # Simulation de validation\n",
        "        return np.random.random() > 0.05  # 95% de succès\n",
        "    \n",
        "    def _assess_prediction_quality(self, results):\n",
        "        \"\"\"Évalue la qualité de la prédiction\"\"\"\n",
        "        # Facteurs de qualité simulés\n",
        "        confidence_factor = results['confidence']\n",
        "        volume_consistency = 1 - abs(results['predictions']['hippocampus_left'] - \n",
        "                                   results['predictions']['hippocampus_right']) / 3000\n",
        "        processing_speed = min(1.0, 60 / results['processing_time'])  # Optimal < 60s\n",
        "        \n",
        "        quality_score = (confidence_factor * 0.5 + volume_consistency * 0.3 + \n",
        "                        processing_speed * 0.2)\n",
        "        \n",
        "        return np.clip(quality_score, 0, 1)\n",
        "    \n",
        "    def _update_processing_time(self, new_time):\n",
        "        \"\"\"Met à jour la moyenne des temps de traitement\"\"\"\n",
        "        current_avg = self.performance_metrics['average_processing_time']\n",
        "        total_success = self.performance_metrics['successful_predictions']\n",
        "        \n",
        "        if total_success == 1:\n",
        "            self.performance_metrics['average_processing_time'] = new_time\n",
        "        else:\n",
        "            # Moyenne mobile\n",
        "            self.performance_metrics['average_processing_time'] = (\n",
        "                (current_avg * (total_success - 1) + new_time) / total_success\n",
        "            )\n",
        "    \n",
        "    def generate_monitoring_report(self):\n",
        "        \"\"\"\n",
        "        Génère un rapport de monitoring détaillé\n",
        "        \"\"\"\n",
        "        metrics = self.performance_metrics\n",
        "        health = self.health_check()\n",
        "        \n",
        "        success_rate = (metrics['successful_predictions'] / \n",
        "                       metrics['total_processed'] * 100) if metrics['total_processed'] > 0 else 0\n",
        "        \n",
        "        avg_quality = np.mean(self.quality_metrics) if self.quality_metrics else 0\n",
        "        \n",
        "        report = f\"\"\"\n",
        "RAPPORT DE MONITORING - nnU-Net PRODUCTION\n",
        "{'='*60}\n",
        "\n",
        "ÉTAT DU SYSTÈME:\n",
        "Statut global: {health['status'].upper()}\n",
        "Dernière vérification: {health['timestamp']}\n",
        "\n",
        "MÉTRIQUES DE PERFORMANCE:\n",
        "• Cas traités: {metrics['total_processed']}\n",
        "• Succès: {metrics['successful_predictions']} ({success_rate:.1f}%)\n",
        "• Échecs: {metrics['failed_predictions']}\n",
        "• Temps moyen de traitement: {metrics['average_processing_time']:.1f} secondes\n",
        "• Score qualité moyen: {avg_quality:.3f}\n",
        "• Temps de fonctionnement: {health['checks']['system_uptime']}\n",
        "\n",
        "VÉRIFICATIONS SYSTÈME:\n",
        "• Modèle chargé: {'✓' if health['checks']['model_loaded'] else '✗'}\n",
        "• GPU disponible: {'✓' if health['checks']['gpu_available'] else '✗'}\n",
        "• Espace disque: {'✓' if health['checks']['disk_space'] else '✗'}\n",
        "• Utilisation mémoire: {'✓' if health['checks']['memory_usage'] else '✗'}\n",
        "• Taux d'erreur: {'✓' if health['checks']['error_rate'] else '✗'}\n",
        "\"\"\"\n",
        "        \n",
        "        if health['issues']:\n",
        "            report += f\"\\nPROBLÈMES DÉTECTÉS:\\n\"\n",
        "            for issue in health['issues']:\n",
        "                report += f\"• {issue}\\n\"\n",
        "        \n",
        "        if metrics['last_error']:\n",
        "            error = metrics['last_error']\n",
        "            report += f\"\"\"\n",
        "\n",
        "DERNIÈRE ERREUR:\n",
        "• Cas: {error['case_id']}\n",
        "• Erreur: {error['error']}\n",
        "• Timestamp: {error['timestamp']}\n",
        "\"\"\"\n",
        "        \n",
        "        report += f\"\"\"\n",
        "\n",
        "RECOMMANDATIONS:\n",
        "{'='*30}\n",
        "\"\"\"\n",
        "        \n",
        "        recommendations = self._generate_recommendations(health, metrics)\n",
        "        for rec in recommendations:\n",
        "            report += f\"• {rec}\\n\"\n",
        "        \n",
        "        report += f\"\"\"\n",
        "\n",
        "{'='*60}\n",
        "Rapport généré: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n",
        "{'='*60}\n",
        "\"\"\"\n",
        "        \n",
        "        return report\n",
        "    \n",
        "    def _generate_recommendations(self, health, metrics):\n",
        "        \"\"\"Génère des recommandations basées sur l'état du système\"\"\"\n",
        "        recommendations = []\n",
        "        \n",
        "        # Recommandations basées sur l'état\n",
        "        if health['status'] == 'critical':\n",
        "            recommendations.append(\"URGENT: Intervention technique requise\")\n",
        "            recommendations.append(\"Arrêt temporaire du service recommandé\")\n",
        "        \n",
        "        # Recommandations basées sur les performances\n",
        "        if metrics['total_processed'] > 0:\n",
        "            error_rate = metrics['failed_predictions'] / metrics['total_processed']\n",
        "            if error_rate > 0.1:\n",
        "                recommendations.append(\"Taux d'erreur élevé - Vérifier la qualité des données d'entrée\")\n",
        "        \n",
        "        if metrics['average_processing_time'] > 120:\n",
        "            recommendations.append(\"Temps de traitement élevé - Optimisation nécessaire\")\n",
        "        \n",
        "        if len(self.quality_metrics) > 10 and np.mean(self.quality_metrics[-10:]) < 0.8:\n",
        "            recommendations.append(\"Baisse de qualité récente - Vérifier le modèle\")\n",
        "        \n",
        "        if not recommendations:\n",
        "            recommendations.append(\"Système fonctionnel - Surveillance continue\")\n",
        "            recommendations.append(\"Sauvegarde programmée des logs\")\n",
        "            recommendations.append(\"Mise à jour des métriques de performance\")\n",
        "        \n",
        "        return recommendations\n",
        "\n",
        "# Initialisation du gestionnaire de déploiement\n",
        "deployment_manager = ProductionDeploymentManager(results_folder)\n",
        "\n",
        "print(\"Simulation d'un déploiement en production...\")\n",
        "\n",
        "# Simulation de traitement de plusieurs cas\n",
        "test_cases = [\n",
        "    {'case_id': 'PROD_001', 'patient_id': 'PAT_123'},\n",
        "    {'case_id': 'PROD_002', 'patient_id': 'PAT_124'},\n",
        "    {'case_id': 'PROD_003', 'patient_id': 'PAT_125'},\n",
        "    {'case_id': 'PROD_004', 'patient_id': 'PAT_126'},\n",
        "    {'case_id': 'PROD_005', 'patient_id': 'PAT_127'}\n",
        "]\n",
        "\n",
        "print(f\"\\nTraitement de {len(test_cases)} cas en production:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "production_results = []\n",
        "for test_case in test_cases:\n",
        "    result = deployment_manager.process_with_monitoring(\n",
        "        input_data=test_case, \n",
        "        case_id=test_case['case_id']\n",
        "    )\n",
        "    production_results.append(result)\n",
        "    print()\n",
        "\n",
        "# Génération du rapport de monitoring\n",
        "monitoring_report = deployment_manager.generate_monitoring_report()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RAPPORT DE MONITORING:\")\n",
        "print(\"=\"*60)\n",
        "print(monitoring_report)\n",
        "\n",
        "# Sauvegarde du rapport\n",
        "monitoring_file = results_folder / f\"monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "with open(monitoring_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(monitoring_report)\n",
        "\n",
        "print(f\"Rapport de monitoring sauvegardé: {monitoring_file}\")\n",
        "print(\"\\nDéploiement en production simulé avec succès.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Résumé et Guide de Déploiement\n",
        "\n",
        "### Compétences Acquises\n",
        "\n",
        "Dans ce notebook complet, vous avez maîtrisé:\n",
        "\n",
        "1. **Configuration Complète de nnU-Net**\n",
        "   - Structure de dossiers standardisée\n",
        "   - Variables d'environnement et configuration\n",
        "   - Préparation de datasets au format Medical Decathlon\n",
        "\n",
        "2. **Préprocessing et Planification Automatique**\n",
        "   - Analyse automatique des propriétés d'images\n",
        "   - Génération de configurations optimisées (2D, 3D_fullres, 3D_lowres, 3D_cascade)\n",
        "   - Plans d'entraînement adaptatifs\n",
        "\n",
        "3. **Entraînement et Monitoring**\n",
        "   - Simulation réaliste d'entraînement nnU-Net\n",
        "   - Courbes d'apprentissage et métriques de performance\n",
        "   - Early stopping et validation croisée\n",
        "\n",
        "4. **Inférence et Post-traitement**\n",
        "   - Pipeline de prédiction automatisé\n",
        "   - Post-traitement morphologique\n",
        "   - Métriques de qualité et validation\n",
        "\n",
        "5. **Intégration Clinique DICOM**\n",
        "   - Workflows hospitaliers complets\n",
        "   - Gestion des métadonnées DICOM\n",
        "   - Génération de rapports cliniques automatisés\n",
        "   - Intégration PACS\n",
        "\n",
        "6. **Déploiement en Production**\n",
        "   - Monitoring des performances en temps réel\n",
        "   - Gestion des erreurs et recovery\n",
        "   - Health checks automatisés\n",
        "   - Alertes et recommandations\n",
        "\n",
        "### Guide de Déploiement Réel\n",
        "\n",
        "#### Phase 1: Installation et Configuration\n",
        "```bash\n",
        "# Installation nnU-Net\n",
        "pip install nnunet\n",
        "\n",
        "# Configuration des variables d'environnement\n",
        "export nnUNet_raw=/data/nnUNet_raw\n",
        "export nnUNet_preprocessed=/data/nnUNet_preprocessed  \n",
        "export nnUNet_results=/data/nnUNet_results\n",
        "```\n",
        "\n",
        "#### Phase 2: Préparation des Données\n",
        "```bash\n",
        "# Préprocessing automatique\n",
        "nnUNetv2_plan_and_preprocess -d DATASET_ID\n",
        "\n",
        "# Vérification de la configuration\n",
        "nnUNetv2_print_plans -d DATASET_ID\n",
        "```\n",
        "\n",
        "#### Phase 3: Entraînement\n",
        "```bash\n",
        "# Entraînement 5-fold cross-validation\n",
        "nnUNetv2_train DATASET_ID 3d_fullres 0\n",
        "nnUNetv2_train DATASET_ID 3d_fullres 1\n",
        "# ... jusqu'au fold 4\n",
        "\n",
        "# Recherche de la meilleure configuration\n",
        "nnUNetv2_find_best_configuration DATASET_ID\n",
        "```\n",
        "\n",
        "#### Phase 4: Inférence\n",
        "```bash\n",
        "# Prédiction sur nouvelles données\n",
        "nnUNetv2_predict -i INPUT_FOLDER -o OUTPUT_FOLDER \\\n",
        "    -d DATASET_ID -c 3d_fullres -f 0 1 2 3 4\n",
        "```\n",
        "\n",
        "### Recommandations de Production\n",
        "\n",
        "1. **Infrastructure**:\n",
        "   - Serveurs GPU dédiés (minimum 16 GB VRAM)\n",
        "   - Stockage SSD rapide pour les données\n",
        "   - Redondance et sauvegarde automatique\n",
        "\n",
        "2. **Sécurité**:\n",
        "   - Chiffrement des données patients\n",
        "   - Logs d'audit complets\n",
        "   - Accès contrôlé et authentification\n",
        "\n",
        "3. **Validation**:\n",
        "   - Tests de régression automatisés\n",
        "   - Validation continue sur données de référence\n",
        "   - Approval workflow radiologique\n",
        "\n",
        "4. **Performance**:\n",
        "   - Monitoring 24/7 des métriques\n",
        "   - Alertes automatiques en cas de problème\n",
        "   - Optimisation continue des modèles\n",
        "\n",
        "### Applications Cliniques Spécialisées\n",
        "\n",
        "- **Neurologie**: Segmentation hippocampe, tumeurs cérébrales\n",
        "- **Cardiologie**: Segmentation cardiaque, calcul de fractions d'éjection\n",
        "- **Radiothérapie**: Délineation d'organes à risque, volumes cibles\n",
        "- **Chirurgie**: Planification préopératoire, guidage per-opératoire\n",
        "\n",
        "### Prochaine Étape\n",
        "\n",
        "Le prochain notebook vous enseignera les **frameworks de sélection et comparaison de modèles IA** pour choisir la meilleure approche selon votre contexte clinique spécifique."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}